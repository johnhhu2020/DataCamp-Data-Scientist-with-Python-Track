{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2264f1d-222b-4a79-b8a3-8b5f1fd0115b",
   "metadata": {},
   "source": [
    "## Joining Data with pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b0a85-3480-4c2b-8f3c-180fe6f5b194",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "Being able to combine and work with multiple datasets is an essential skill for any aspiring Data Scientist. Pandas is a crucial cornerstone of the Python data science ecosystem, with Stack Overflow recording 5 million views for pandas questions. Learn to handle multiple DataFrames by combining, organizing, joining, and reshaping them using pandas. You'll work with datasets from the World Bank and the City Of Chicago. You will finish the course with a solid skillset for data-joining in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac21d26-0b93-4127-ba38-242cb56f3dd2",
   "metadata": {},
   "source": [
    "##  Data Merging Basics\n",
    "Free\n",
    "0%\n",
    "\n",
    "Learn how you can merge disparate data using inner joins. By combining information from multiple sources you’ll uncover compelling insights that may have previously been hidden. You’ll also learn how the relationship between those sources, such as one-to-one or one-to-many, can affect your result.\n",
    "\n",
    "    Inner join    50 xp\n",
    "    What column to merge on?    50 xp\n",
    "    Your first inner join    100 xp\n",
    "    Inner joins and number of rows returned    100 xp\n",
    "    One-to-many relationships    50 xp\n",
    "    One-to-many classification    100 xp\n",
    "    One-to-many merge    100 xp\n",
    "    Merging multiple DataFrames    50 xp\n",
    "    Total riders in a month    100 xp\n",
    "    Three table merge    100 xp\n",
    "    One-to-many merge with multiple tables    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7890ade1-e66a-46c5-a8ef-f4c96f8cff86",
   "metadata": {},
   "source": [
    "##  Merging Tables With Different Join Types\n",
    "0%\n",
    "\n",
    "Take your knowledge of joins to the next level. In this chapter, you’ll work with TMDb movie data as you learn about left, right, and outer joins. You’ll also discover how to merge a table to itself and merge on a DataFrame index.\n",
    "\n",
    "    Left join    50 xp\n",
    "    Counting missing rows with left join    100 xp\n",
    "    Enriching a dataset    100 xp\n",
    "    How many rows with a left join?    50 xp\n",
    "    Other joins    50 xp\n",
    "    Right join to find unique movies    100 xp\n",
    "    Popular genres with right join    100 xp\n",
    "    Using outer join to select actors    100 xp\n",
    "    Merging a table to itself    50 xp\n",
    "    Self join    100 xp\n",
    "    How does pandas handle self joins?    50 xp\n",
    "    Merging on indexes    50 xp\n",
    "    Index merge for movie ratings    100 xp\n",
    "    Do sequels earn more?    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a2a0c-02ec-48ba-9e77-74d4cdba100c",
   "metadata": {},
   "source": [
    "##  Advanced Merging and Concatenating\n",
    "0%\n",
    "\n",
    "In this chapter, you’ll leverage powerful filtering techniques, including semi-joins and anti-joins. You’ll also learn how to glue DataFrames by vertically combining and using the pandas.concat function to create new datasets. Finally, because data is rarely clean, you’ll also learn how to validate your newly combined data structures.\n",
    "\n",
    "    Filtering joins    50 xp\n",
    "    Steps of a semi-join    100 xp\n",
    "    Performing an anti-join    100 xp\n",
    "    Performing a semi-join    100 xp\n",
    "    Concatenate DataFrames together vertically    50 xp\n",
    "    Concatenation basics    100 xp\n",
    "    Concatenating with keys    100 xp\n",
    "    Using the append method    100 xp\n",
    "    Verifying integrity    50 xp\n",
    "    Validating a merge    50 xp\n",
    "    Concatenate and merge to find common songs    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5d283-f26c-4a58-966c-bf76786dc601",
   "metadata": {},
   "source": [
    "##  Merging Ordered and Time-Series Data\n",
    "0%\n",
    "\n",
    "In this final chapter, you’ll step up a gear and learn to apply pandas' specialized methods for merging time-series and ordered data together with real-world financial and economic data from the city of Chicago. You’ll also learn how to query resulting tables using a SQL-style format, and unpivot data using the melt method.\n",
    "\n",
    "    Using merge_ordered()    50 xp\n",
    "    Correlation between GDP and S&P500    100 xp\n",
    "    Phillips curve using merge_ordered()    100 xp\n",
    "    merge_ordered() caution, multiple columns    100 xp\n",
    "    Using merge_asof()    50 xp\n",
    "    Using merge_asof() to study stocks    100 xp\n",
    "    Using merge_asof() to create dataset    100 xp\n",
    "    merge_asof() and merge_ordered() differences    100 xp\n",
    "    Selecting data with .query()    50 xp\n",
    "    Explore financials with .query()    50 xp\n",
    "    Subsetting rows with .query()    100 xp\n",
    "    Reshaping data with .melt()    50 xp\n",
    "    Select the right .melt() arguments    50 xp\n",
    "    Using .melt() to reshape government data    100 xp\n",
    "    Using .melt() for stocks vs bond performance    100 xp\n",
    "    Course wrap-up    50 xp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3548aae-3caa-4992-86c3-e09699472764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "232283a8-955a-4843-aea5-1453bbd38fe6",
   "metadata": {},
   "source": [
    "## Inner join\n",
    "\n",
    "\n",
    "\n",
    "**The Pandas package is a powerful tool for manipulating and transforming data in Python.  However, when working on an analysis, the data needed could be in multiple tables.  This course will focus on the vital skill of merging tables together.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# As we start, two quick clarifications.  \n",
    "\n",
    "First, through other courses on DataCamp, you may have learned how to import tabular data as DataFrame.  In this course, you may hear the words table and DataFrame, but they are equivalent here.  \n",
    "\n",
    "Second, we will refer to combingning different tables together as merging tales, but note that some refer to this same process as joining.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To help us learn about merging tables, we will use data from the city of Chicago data portal.  The city of Chicago is divided into fifty local neighborhoods called wards.  We have a table with data about the local goverment offices in each ward.  \n",
    "\n",
    "In this example, we want to merge the local government data with census data about the population of each ward.  If we look at the wards table, we have information about the local goverment of each ward, such as the goverment office address.  The census table contains the population of each ward in 2000 and 2010, and that change as percentage.  Additionally, it includes the address for the center of each ward.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The two tables are related by their 'ward' column.  We can merge them together, matching the ward number from each row of the ward table to the ward number from the census table.  \n",
    "\n",
    "**The Pandas package has an excellent DataFrame method for performing this type of merge called .merge() method.  The .merge() method takes the first DF and merges it with the second DF.  We use on= argument to tell it we want to merge the 2 DFs on the ward column.  Since we listed the ward table first, its columns will appear first in the output, followed by the columns from the second census table.  By default, it returns the rows have matching values for the on=column in both tables.  This is called an inner join.  \n",
    "\n",
    "An inner join will only return rows that have matching values in both tables.  \n",
    "\n",
    "**You may have noticed that the merged table has columns with suffixes of underscore x or y.  This is because both wards and census tables contained address and zip columns.  To avoid multiple columns with the same name, they are automatically given a suffix by the merge method.  We can use the suffix= argument of the .merge() to control this behavior.  We provide a tuple where all of the overlapping columns in the left table are given the suffix '_ward', and those of the right table will be given the suffix '_cen'.  This makes it easier for us to tell the difference between the columns.   \n",
    "\n",
    "\n",
    "\n",
    "wards = pd.read_csv('Ward_Offices.csv')\n",
    "print(wards.head())\n",
    "print(wards.shape)\n",
    "\n",
    "--------------------------------------------------------------------\n",
    "  ward            alderman                          address    zip\n",
    "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647\n",
    "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622\n",
    "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609\n",
    "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616\n",
    "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649\n",
    "\n",
    "(50, 4)\n",
    "\n",
    "\n",
    "census = pd.read_csv('Ward_Census.csv')\n",
    "print(census.head())\n",
    "print(census.shape)\n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "  ward  pop_2000  pop_2010 change                                  address    zip  \n",
    "0    1     52951     56149     6%              2765 WEST SAINT MARY STREET  60647  \n",
    "1    2     54361     55805     3%                 WM WASTE MANAGEMENT 1500  60622  \n",
    "2    3     40385     53039    31%                      17 EAST 38TH STREET  60653  \n",
    "3    4     51953     54589     5%  31ST ST HARBOR BUILDING LAKEFRONT TRAIL  60653  \n",
    "4    5     55302     51455    -7%  JACKSON PARK LAGOON SOUTH CORNELL DRIVE  60637\n",
    "\n",
    "(50,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0ea7a9-46df-45cc-a459-7429e5d57145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ward            alderman                          address    zip\n",
      "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647\n",
      "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622\n",
      "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609\n",
      "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616\n",
      "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649\n",
      "(50, 4)\n",
      "  ward  pop_2000  pop_2010 change                                  address  \\\n",
      "0    1     52951     56149     6%              2765 WEST SAINT MARY STREET   \n",
      "1    2     54361     55805     3%                 WM WASTE MANAGEMENT 1500   \n",
      "2    3     40385     53039    31%                      17 EAST 38TH STREET   \n",
      "3    4     51953     54589     5%  31ST ST HARBOR BUILDING LAKEFRONT TRAIL   \n",
      "4    5     55302     51455    -7%  JACKSON PARK LAGOON SOUTH CORNELL DRIVE   \n",
      "\n",
      "     zip  \n",
      "0  60647  \n",
      "1  60622  \n",
      "2  60653  \n",
      "3  60653  \n",
      "4  60637  \n",
      "(50, 6)\n",
      "  ward            alderman                        address_x  zip_x  pop_2000  \\\n",
      "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647     52951   \n",
      "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622     54361   \n",
      "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609     40385   \n",
      "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616     51953   \n",
      "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649     55302   \n",
      "\n",
      "   pop_2010 change                                address_y  zip_y  \n",
      "0     56149     6%              2765 WEST SAINT MARY STREET  60647  \n",
      "1     55805     3%                 WM WASTE MANAGEMENT 1500  60622  \n",
      "2     53039    31%                      17 EAST 38TH STREET  60653  \n",
      "3     54589     5%  31ST ST HARBOR BUILDING LAKEFRONT TRAIL  60653  \n",
      "4     51455    -7%  JACKSON PARK LAGOON SOUTH CORNELL DRIVE  60637  \n",
      "50\n",
      "  ward            alderman                     address_ward zip_ward  \\\n",
      "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE    60647   \n",
      "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE    60622   \n",
      "2    3          Pat Dowell          5046 SOUTH STATE STREET    60609   \n",
      "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR    60616   \n",
      "4    5  Leslie A. Hairston            2325 EAST 71ST STREET    60649   \n",
      "\n",
      "   pop_2000  pop_2010 change                              address_cen zip_cen  \n",
      "0     52951     56149     6%              2765 WEST SAINT MARY STREET   60647  \n",
      "1     54361     55805     3%                 WM WASTE MANAGEMENT 1500   60622  \n",
      "2     40385     53039    31%                      17 EAST 38TH STREET   60653  \n",
      "3     51953     54589     5%  31ST ST HARBOR BUILDING LAKEFRONT TRAIL   60653  \n",
      "4     55302     51455    -7%  JACKSON PARK LAGOON SOUTH CORNELL DRIVE   60637  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "ward = pd.read_pickle('ward.p')\n",
    "census = pd.read_pickle('census.p')\n",
    "\n",
    "\n",
    "print(ward.head())\n",
    "print(ward.shape)\n",
    "\n",
    "print(census.head())\n",
    "print(census.shape)\n",
    "\n",
    "\n",
    "\n",
    "merged_df = ward.merge(census, how='inner', on='ward')\n",
    "#print(help(pd.merge))\n",
    "print(merged_df.head())\n",
    "print(len(merged_df))\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "merged_df2 = ward.merge(census, how='inner', on='ward', suffixes=('_ward', '_cen'))\n",
    "print(merged_df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296a90a-2510-4b6c-bb3c-8b233ff517c1",
   "metadata": {},
   "source": [
    "## What column to merge on?\n",
    "\n",
    "Chicago provides a list of taxicab owners and vehicles licensed to operate within the city, for public safety. Your goal is to merge two tables together. One table is called 'taxi_owners', with info about the taxi cab company owners, and one is called 'taxi_veh', with info about each taxi cab vehicle. Both the taxi_owners and taxi_veh tables have been loaded for you and you can explore them in the IPython shell.\n",
    "\n",
    "Choose the column you would use to merge the two tables on using the .merge() method.\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    on='rid'\n",
    "#    on='vid'\n",
    "    on='year'\n",
    "    on='zip'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0796c-b9bd-434d-bc85-a9c7ac0c27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]:\n",
    "print(taxi_owners.head())\n",
    "     rid   vid           owner                 address    zip\n",
    "0  T6285  6285  AGEAN TAXI LLC     4536 N. ELSTON AVE.  60630\n",
    "1  T4862  4862    MANGIB CORP.  5717 N. WASHTENAW AVE.  60659\n",
    "2  T1495  1495   FUNRIDE, INC.     3351 W. ADDISON ST.  60618\n",
    "3  T4231  4231    ALQUSH CORP.   6611 N. CAMPBELL AVE.  60645\n",
    "4  T5971  5971  EUNIFFORD INC.     3351 W. ADDISON ST.  60618\n",
    "In [2]:\n",
    "print(taxi_veh.head())\n",
    "    vid    make   model  year fuel_type                owner\n",
    "0  2767  TOYOTA   CAMRY  2013    HYBRID       SEYED M. BADRI\n",
    "1  1411  TOYOTA    RAV4  2017    HYBRID          DESZY CORP.\n",
    "2  6500  NISSAN  SENTRA  2019  GASOLINE       AGAPH CAB CORP\n",
    "3  2746  TOYOTA   CAMRY  2013    HYBRID  MIDWEST CAB CO, INC\n",
    "4  5922  TOYOTA   CAMRY  2013    HYBRID       SUMETTI CAB CO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad795c-7f13-4c62-ba35-9d31af6ace87",
   "metadata": {},
   "source": [
    "## Your first inner join\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# You have been tasked with figuring out what the most popular types of fuel used in Chicago taxis are. \n",
    "To complete the analysis, you need to merge the taxi_owners and taxi_veh tables together on the vid column. You can then use the merged table along with the .value_counts() method to find the most common fuel_type.\n",
    "\n",
    "Since you'll be working with pandas throughout the course, the package will be preloaded for you as pd in each exercise in this course. Also the taxi_owners and taxi_veh DataFrames are loaded for you.\n",
    "Instructions 1/3\n",
    "100 XP\n",
    "\n",
    "    Question 1\n",
    "    Merge taxi_owners with taxi_veh on the column vid, and save the result to taxi_own_veh.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Set the left and right table suffixes for overlapping columns of the merge to _own and _veh, respectively.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Select the fuel_type column from taxi_own_veh and print the value_counts() to find the most popular fuel_types used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84a580ff-d068-49de-90ca-35da92053b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    vid    make   model  year fuel_type                owner\n",
      "0  2767  TOYOTA   CAMRY  2013    HYBRID       SEYED M. BADRI\n",
      "1  1411  TOYOTA    RAV4  2017    HYBRID          DESZY CORP.\n",
      "2  6500  NISSAN  SENTRA  2019  GASOLINE       AGAPH CAB CORP\n",
      "3  2746  TOYOTA   CAMRY  2013    HYBRID  MIDWEST CAB CO, INC\n",
      "4  5922  TOYOTA   CAMRY  2013    HYBRID       SUMETTI CAB CO\n",
      "     rid   vid           owner                 address    zip\n",
      "0  T6285  6285  AGEAN TAXI LLC     4536 N. ELSTON AVE.  60630\n",
      "1  T4862  4862    MANGIB CORP.  5717 N. WASHTENAW AVE.  60659\n",
      "2  T1495  1495   FUNRIDE, INC.     3351 W. ADDISON ST.  60618\n",
      "3  T4231  4231    ALQUSH CORP.   6611 N. CAMPBELL AVE.  60645\n",
      "4  T5971  5971  EUNIFFORD INC.     3351 W. ADDISON ST.  60618\n",
      "     rid   vid       owner_own                 address    zip    make   model  \\\n",
      "0  T6285  6285  AGEAN TAXI LLC     4536 N. ELSTON AVE.  60630  NISSAN  ALTIMA   \n",
      "1  T4862  4862    MANGIB CORP.  5717 N. WASHTENAW AVE.  60659   HONDA     CRV   \n",
      "2  T1495  1495   FUNRIDE, INC.     3351 W. ADDISON ST.  60618  TOYOTA  SIENNA   \n",
      "3  T4231  4231    ALQUSH CORP.   6611 N. CAMPBELL AVE.  60645  TOYOTA   CAMRY   \n",
      "4  T5971  5971  EUNIFFORD INC.     3351 W. ADDISON ST.  60618  TOYOTA  SIENNA   \n",
      "\n",
      "   year fuel_type       owner_veh  \n",
      "0  2011    HYBRID  AGEAN TAXI LLC  \n",
      "1  2014  GASOLINE    MANGIB CORP.  \n",
      "2  2015  GASOLINE   FUNRIDE, INC.  \n",
      "3  2014    HYBRID    ALQUSH CORP.  \n",
      "4  2015  GASOLINE  EUNIFFORD INC.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuel_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COMPRESSED NATURAL GAS</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLEX FUEL</th>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GASOLINE</th>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HYBRID</th>\n",
       "      <td>2792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         vid\n",
       "fuel_type                   \n",
       "COMPRESSED NATURAL GAS    27\n",
       "FLEX FUEL                 89\n",
       "GASOLINE                 611\n",
       "HYBRID                  2792"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "taxi_veh = pd.read_pickle('taxi_vehicles.p')\n",
    "taxi_owners = pd.read_pickle('taxi_owners.p')\n",
    "\n",
    "\n",
    "print(taxi_veh.head())\n",
    "print(taxi_owners.head())\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', how='inner', suffixes=('_own', '_veh'))\n",
    "print(taxi_own_veh.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "taxi_own_veh.pivot_table(values='vid', index='fuel_type', aggfunc=lambda x: len(x.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6cf34e-0dc9-4863-92c3-784431260e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2792\n"
     ]
    }
   ],
   "source": [
    "hyb_group = taxi_own_veh[taxi_own_veh['fuel_type']=='HYBRID'].copy()\n",
    "\n",
    "print(len(hyb_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "254698cb-20b6-4f46-8dd4-1d7224f1d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID                    2792\n",
      "GASOLINE                   611\n",
      "FLEX FUEL                   89\n",
      "COMPRESSED NATURAL GAS      27\n",
      "Name: fuel_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh'))\n",
    "\n",
    "\n",
    "\n",
    "# Print the value_counts to find the most popular fuel_type\n",
    "print(taxi_own_veh['fuel_type'].value_counts())\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "# This approach is much simple than my code above with pivot_table and lambda expression\n",
    "# ***************************************************************************************************************** #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb91716-ca4c-4875-a32a-91458370be84",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************\n",
    "\n",
    "To count the dogs of each breed, we'll subset the breed column and use the .value_counts() method.  We can also use the sort= argument to get the breeds with biggest counts on top.  Also the normalize= argument can be used to turn the counts into proportions of the total.\n",
    "\n",
    "\n",
    "# We learned that in DataCamp Data Manipulation with Pandas course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0cca2-a59c-4ffa-863b-35f4be73d0b6",
   "metadata": {},
   "source": [
    "## Inner joins and number of rows returned\n",
    "\n",
    "All of the merges you have studied to this point are called inner joins.  It is necessary to understand that inner joins only return the rows with matching values in both tables.  You will explore this further by reviewing the merge between the 'wards' and 'census' tables, then comparing it to merges of copies of these tables that are slightly altered, named 'wards_altered', and 'census_altered'.  The first row of the wards column has been changed in the altered tables.  You will examine how this affects the merge between them. The tables have been loaded for you.\n",
    "\n",
    "For this exercise, it is important to know that the wards and census tables start with 50 rows.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Merge wards and census on the 'ward' column and save the result to wards_census.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Merge the 'wards_altered' and 'census' tables on the ward column, and notice the difference in returned rows.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Merge the wards and 'census_altered' tables on the 'ward' column, and notice the difference in returned rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588e6168-6403-412e-a306-87503fcced26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ward            alderman                          address    zip\n",
      "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647\n",
      "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622\n",
      "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609\n",
      "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616\n",
      "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649\n",
      "  ward  pop_2000  pop_2010 change                                  address  \\\n",
      "0    1     52951     56149     6%              2765 WEST SAINT MARY STREET   \n",
      "1    2     54361     55805     3%                 WM WASTE MANAGEMENT 1500   \n",
      "2    3     40385     53039    31%                      17 EAST 38TH STREET   \n",
      "3    4     51953     54589     5%  31ST ST HARBOR BUILDING LAKEFRONT TRAIL   \n",
      "4    5     55302     51455    -7%  JACKSON PARK LAGOON SOUTH CORNELL DRIVE   \n",
      "\n",
      "     zip  \n",
      "0  60647  \n",
      "1  60622  \n",
      "2  60653  \n",
      "3  60653  \n",
      "4  60637  \n",
      "  ward            alderman                        address_x  zip_x  pop_2000  \\\n",
      "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647     52951   \n",
      "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622     54361   \n",
      "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609     40385   \n",
      "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616     51953   \n",
      "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649     55302   \n",
      "\n",
      "   pop_2010 change                                address_y  zip_y  \n",
      "0     56149     6%              2765 WEST SAINT MARY STREET  60647  \n",
      "1     55805     3%                 WM WASTE MANAGEMENT 1500  60622  \n",
      "2     53039    31%                      17 EAST 38TH STREET  60653  \n",
      "3     54589     5%  31ST ST HARBOR BUILDING LAKEFRONT TRAIL  60653  \n",
      "4     51455    -7%  JACKSON PARK LAGOON SOUTH CORNELL DRIVE  60637  \n"
     ]
    }
   ],
   "source": [
    "print(ward.head())\n",
    "print(census.head())\n",
    "\n",
    "\n",
    "\n",
    "wards_census = ward.merge(census, on='ward')  # , on='ward'    Without , on='ward', code not work\n",
    "print(wards_census.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609b67b-bc38-4dbb-9f6e-1a46e9bca369",
   "metadata": {},
   "outputs": [],
   "source": [
    "  ward            alderman                          address    zip\n",
    "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647\n",
    "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622\n",
    "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609\n",
    "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616\n",
    "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649\n",
    "\n",
    "\n",
    "wards_altered\n",
    "\n",
    "   ward                   alderman                            address    zip\n",
    "0    61         Proco \"Joe\" Moreno          2058 NORTH WESTERN AVENUE  60647\n",
    "1     2              Brian Hopkins         1400 NORTH  ASHLAND AVENUE  60622\n",
    "2     3                 Pat Dowell            5046 SOUTH STATE STREET  60609\n",
    "3     4           William D. Burns    435 EAST 35TH STREET, 1ST FLOOR  60616\n",
    "4     5         Leslie A. Hairston              2325 EAST 71ST STREET  60649\n",
    "5     6         Roderick T. Sawyer   8001 S. MARTIN LUTHER KING DRIVE  60619\n",
    "6     7        Gregory I. Mitchell              2249 EAST 95TH STREET  60617\n",
    "7     8         Michelle A. Harris    8539 SOUTH COTTAGE GROVE AVENUE  60619\n",
    "8     9           Anthony A. Beale                34 EAST 112TH PLACE  60628\n",
    "9    10      Susan Sadlowski Garza           10500 SOUTH EWING AVENUE  60617\n",
    "10   11     Patrick Daley Thompson          3659 SOUTH HALSTED STREET  60609\n",
    "11   12            George Cardenas           3476 SOUTH ARCHER AVENUE  60608\n",
    "12   13                Marty Quinn            6500 SOUTH PULASKI ROAD  60629\n",
    "13   14            Edward M. Burke              2650 WEST 51ST STREET  60632\n",
    "14   15           Raymond A. Lopez              1650 WEST 63RD STREET  60636\n",
    "15   16            Toni L. Foulkes              3045 WEST 63RD STREET  60629\n",
    "16   17             David H. Moore          7313 SOUTH ASHLAND AVENUE  60636\n",
    "17   18          Derrick G. Curtis            8359 SOUTH PULASKI ROAD  60652\n",
    "18   19          Matthew J. O'Shea         10400 SOUTH WESTERN AVENUE  60643\n",
    "19   20          Willie B. Cochran    6357 SOUTH COTTAGE GROVE AVENUE  60637\n",
    "20   21    Howard B. Brookins, Jr.  9011 SOUTH ASHLAND AVENUE, UNIT B  60620\n",
    "21   22              Ricardo Munoz        2500 SOUTH ST. LOUIS AVENUE  60623\n",
    "22   23        Michael R. Zalewski           6247 SOUTH ARCHER AVENUE  60638\n",
    "23   24         Michael Scott, Jr.           1158 SOUTH KEELER AVENUE  60624\n",
    "24   25       Daniel \"Danny\" Solis      1800 SOUTH BLUE ISLAND AVENUE  60608\n",
    "25   26          Roberto Maldonado          2511 WEST DIVISION STREET  60622\n",
    "26   27        Walter Burnett, Jr.             4 NORTH WESTERN AVENUE  60612\n",
    "27   28             Jason C. Ervin              2602 WEST 16TH STREET  60612\n",
    "28   29           Chris Taliaferro             6272 WEST NORTH AVENUE  60639\n",
    "29   30          Ariel E. Reyboras        3559 NORTH MILWAUKEE AVENUE  60641\n",
    "30   31  Milagros \"Milly\" Santiago            2521 NORTH PULASKI ROAD  60639\n",
    "31   32           Scott Waguespack         2657 NORTH CLYBOURN AVENUE  60614\n",
    "32   33               Deborah Mell         3001 WEST IRVING PARK ROAD  60618\n",
    "33   34           Carrie M. Austin              507 WEST 111TH STREET  60628\n",
    "34   35        Carlos Ramirez-Rosa           2710 NORTH SAWYER AVENUE  60647\n",
    "35   36           Gilbert Villegas                 6934 WEST DIVERSEY  60607\n",
    "36   37              Emma M. Mitts           4924 WEST CHICAGO AVENUE  60651\n",
    "37   38           Nicholas Sposato          3821  NORTH HARLEM AVENUE  60634\n",
    "38   39           Margaret Laurino          4404 WEST LAWRENCE AVENUE  60630\n",
    "39   40        Patrick J. O'Connor          5850 NORTH LINCOLN AVENUE  60659\n",
    "40   41      Anthony V. Napolitano           7442 NORTH HARLEM AVENUE  60631\n",
    "41   42             Brendan Reilly   325 WEST HURON STREET, SUITE 510  60654\n",
    "42   43             Michelle Smith          2523 NORTH HALSTED STREET  60614\n",
    "43   44                 Tom Tunney        3223 NORTH SHEFFIELD AVENUE  60657\n",
    "44   45              John S. Arena        4754 NORTH MILWAUKEE AVENUE  60630\n",
    "45   46            James Cappleman         4544 NORTH BROADWAY AVENUE  60640\n",
    "46   47                Ameya Pawar          4243 NORTH LINCOLN AVENUE  60618\n",
    "47   48             Harry Osterman         5533 NORTH BROADWAY AVENUE  60640\n",
    "48   49                  Joe Moore        7356 NORTH GREENVIEW AVENUE  60626\n",
    "49   50       Debra L. Silverstein    2949 WEST DEVON AVENUE, SUITE A  60659\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ward  pop_2000  pop_2010 change                                  address   zip\n",
    "0    1     52951     56149     6%              2765 WEST SAINT MARY STREET   60647\n",
    "1    2     54361     55805     3%                 WM WASTE MANAGEMENT 1500   60622\n",
    "2    3     40385     53039    31%                      17 EAST 38TH STREET   60653\n",
    "3    4     51953     54589     5%  31ST ST HARBOR BUILDING LAKEFRONT TRAIL   60653\n",
    "4    5     55302     51455    -7%  JACKSON PARK LAGOON SOUTH CORNELL DRIVE   60637\n",
    "       \n",
    "\n",
    "census_altered\n",
    "\n",
    "ward  pop_2000  pop_2010 change                                            address    zip\n",
    "None     52951     56149     6%                        2765 WEST SAINT MARY STREET  60647\n",
    "   2     54361     55805     3%                           WM WASTE MANAGEMENT 1500  60622\n",
    "   3     40385     53039    31%                                17 EAST 38TH STREET  60653\n",
    "   4     51953     54589     5%            31ST ST HARBOR BUILDING LAKEFRONT TRAIL  60653\n",
    "   5     55302     51455    -7%            JACKSON PARK LAGOON SOUTH CORNELL DRIVE  60637\n",
    "   6     54989     52341    -5%                               150 WEST 74TH STREET  60636\n",
    "   7     54593     51581    -6%                          8549 SOUTH OGLESBY AVENUE  60617\n",
    "   8     54039     51687    -4%                         1346-1352 EAST 75TH STREET  60649\n",
    "   9     52008     51519    -1%                 11039-11059 SOUTH WENTWORTH AVENUE  60628\n",
    "  10     56613     51535    -9%                               10534 SOUTH AVENUE F  46394\n",
    "  11     64228     51497   -20%                            943-947 WEST 14TH PLACE  60607\n",
    "  12     68922     52235   -24%                         CP 46 STEVENSON EXPRESSWAY  60632\n",
    "  13     64382     53722   -17%                    SOUTH RAMP SOUTH LARAMIE AVENUE  60638\n",
    "  14     80143     54031   -33%                              4540 WEST 51ST STREET  60632\n",
    "  15     56057     51501    -8%    CHICAGO FIRE DEPARTMENT ENGINE COMPANY 123 2215  60632\n",
    "  16     50205     51954     3%                             6036 SOUTH WOOD STREET  60636\n",
    "  17     49264     51846     5%                       7216 SOUTH WINCHESTER AVENUE  60636\n",
    "  18     55043     52992    -4%                          3286 WEST COLUMBUS AVENUE  60652\n",
    "  19     54546     51525    -6%                        9999 SOUTH FRANCISCO AVENUE  60805\n",
    "  20     51854     52372     1%                     DAN RYAN EXPRESSWAY PARK MANOR  60621\n",
    "  21     51751     51632     0%                     8852-8854 SOUTH EMERALD AVENUE  60620\n",
    "  22     59734     53515   -10%                              4233 WEST 36TH STREET  60632\n",
    "  23     63691     53728   -16%  CHICAGO MIDWAY INTERNATIONAL AIRPORT WEST 62ND...  60629\n",
    "  24     50879     54909     8%                       1635 SOUTH CHRISTIANA AVENUE  60623\n",
    "  25     55954     54539    -3%                      1632-1746 SOUTH MILLER STREET  60608\n",
    "  26     56841     53516    -6%             LITTLE CUBS FIELD COMFORT STATION 1400  60622\n",
    "  27     61287     52939   -14%                      2151-2153 WEST CHICAGO AVENUE  60651\n",
    "  28     49423     55199    12%                        RML SPECIALTY HOSPITAL 3435  60624\n",
    "  29     61949     55267   -11%                        1241 NORTH RIDGELAND AVENUE  60302\n",
    "  30     72698     55560   -24%                          5118 WEST FLETCHER STREET  60641\n",
    "  31     65045     53724   -17%                          2854 NORTH KEATING AVENUE  60641\n",
    "  32     57204     55184    -4%                        2901 NORTH WASHTENAW AVENUE  60618\n",
    "  33     63695     55598   -13%                    4041-4043 NORTH RICHMOND STREET  60625\n",
    "  34     49922     51599     3%                    11544-11546 SOUTH PEORIA STREET  60827\n",
    "  35     57588     55281    -4%                           3634 WEST BELMONT AVENUE  60618\n",
    "  36     63376     54766   -14%                       2918 NORTH RUTHERFORD AVENUE  60634\n",
    "  37     56120     51538    -8%                         4738-4748 WEST RICE STREET  60651\n",
    "  38     66011     56001   -15%                    7307-7331 WEST IRVING PARK ROAD  60706\n",
    "  39     64291     55882   -13%                  QUEEN OF ALL SAINTS BASILICA 6280  60646\n",
    "  40     58652     55319    -6%                         5536 NORTH ARTESIAN AVENUE  60645\n",
    "  41     56127     55991     0%                          1652 SOUTH CLIFTON AVENUE  60068\n",
    "  42     68102     55870   -18%                          410-420 WEST GRAND AVENUE  60654\n",
    "  43     57668     56170    -3%                              LINCOLN PARK ZOO 2001  60614\n",
    "  44     58758     56058    -5%                         507-513 WEST ALDINE AVENUE  60657\n",
    "  45     60653     55967    -8%       CONGREGATIONAL CHURCH OF JEFFERSON PARK 5320  60630\n",
    "  46     56587     53784    -5%                 UPTOWN BROADWAY BUILDING 4743-4763  60640\n",
    "  47     52108     55074     6%                           2153 WEST BERTEAU AVENUE  60618\n",
    "  48     56246     55014    -2%                         1025 WEST HOLLYWOOD AVENUE  60660\n",
    "  49     59435     54633    -8%                             1426 WEST ESTES AVENUE  60645\n",
    "  50     62383     55809   -11%                       2638 WEST NORTH SHORE AVENUE  60645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd722ad4-781b-4116-a115-71b585e14d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of the wards_altered table to view the change \n",
    "print(wards_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards_altered and census tables on the ward column\n",
    "wards_altered_census = wards_altered.merge(census, on='ward')\n",
    "\n",
    "# Print the shape of wards_altered_census\n",
    "print('wards_altered_census table shape:', wards_altered_census.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the first few rows of the census_altered table to view the change \n",
    "print(census_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards and census_altered tables on the ward column\n",
    "wards_census_altered = wards.merge(census_altered, on='ward')\n",
    "\n",
    "# Print the shape of wards_census_altered\n",
    "print('wards_census_altered table shape:', wards_census_altered.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a3519-5681-40cd-b8a4-3fb108d593db",
   "metadata": {},
   "source": [
    "## One-to-many relationships\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**In the last lesson, we learned how to merge two DFs together with .merge() method.  In this lesson, we'll discuss different types of relationships between tables.  In particular, we'll discuss the one-to-many relationship.  But first, lets quickly consider what a one-to-one relationshipis.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In a one-to-one relationship, every row in the left table is related to one and only one row in the right table.  Recall the relationship between the wards table and census table we learned earlier.  \n",
    "\n",
    "\n",
    "# And what is a one-to-many relationship?  In a one-to-many relationship, every row in the left table is related to one or more rows in the right table.  \n",
    "\n",
    "-------------------     -------------       a one-to-many relationship sample in .merge()\n",
    "A    | B    | C          C    | D\n",
    "A1   | B1   | C1         C1   | D1\n",
    "A2   | B2   | C2         C1   | D2\n",
    "A3   | B3   | C3         C1   | D3\n",
    "                         C1   | D4\n",
    "\n",
    "\n",
    "Within each 'ward', there are many 'businesses'.  Image we will merge the wards table with a table of licensed businesses in each ward.  The business license data is stored at another table called 'licenses'.  It holds info such as the business address and ward the business is located in.  The two DFs are related to each other by their ward column.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "When we merge the two tables together with the .merge() method, setting the on= attribute to the column 'ward', the resulting table has both local ward data and business license data.  Notice that ward 1 and its alderman Joe is repeated in the resulting table because the licenses table has many businesses in the 1st ward.  The Pandas takes care of the one-to-many relationship for us and doesnt require anything on our end.  We can use the same syntax as we did with one-to-one relationship.  By printing the shape, we can see our original tablehas 50 rows, while after merging with the licenses table, the resulting table has 1000 rows.  \n",
    "\n",
    "When you merge tables that have a one-to-many relationship, the number of rows returned will likely be different than the number in the left table.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a583af59-7f78-4336-819c-6ef294583b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  account ward  aid                   business               address    zip\n",
      "0  307071    3  743       REGGIE'S BAR & GRILL       2105 S STATE ST  60616\n",
      "1      10   10  829                 HONEYBEERS   13200 S HOUSTON AVE  60633\n",
      "2   10002   14  775                CELINA DELI     5089 S ARCHER AVE  60632\n",
      "3   10005   12  NaN  KRAFT FOODS NORTH AMERICA        2005 W 43RD ST  60609\n",
      "4   10044   44  638  NEYBOUR'S TAVERN & GRILLE  3651 N SOUTHPORT AVE  60613\n",
      "  ward            alderman                          address    zip\n",
      "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647\n",
      "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622\n",
      "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609\n",
      "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616\n",
      "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649\n",
      "  ward            alderman               address_ward zip_ward account  aid  \\\n",
      "0    1  Proco \"Joe\" Moreno  2058 NORTH WESTERN AVENUE    60647   12024  NaN   \n",
      "1    1  Proco \"Joe\" Moreno  2058 NORTH WESTERN AVENUE    60647   14446  743   \n",
      "2    1  Proco \"Joe\" Moreno  2058 NORTH WESTERN AVENUE    60647   14624  775   \n",
      "3    1  Proco \"Joe\" Moreno  2058 NORTH WESTERN AVENUE    60647   14987  NaN   \n",
      "4    1  Proco \"Joe\" Moreno  2058 NORTH WESTERN AVENUE    60647   15642  814   \n",
      "\n",
      "               business              address_lic zip_lic  \n",
      "0   DIGILOG ELECTRONICS       1038 N ASHLAND AVE   60622  \n",
      "1      EMPTY BOTTLE INC   1035 N WESTERN AVE 1ST   60622  \n",
      "2  LITTLE MEL'S HOT DOG    2205 N CALIFORNIA AVE   60647  \n",
      "3    MR. BROWN'S LOUNGE   2301 W CHICAGO AVE 1ST   60622  \n",
      "4          Beat Kitchen  2000-2100 W DIVISION ST   60622  \n"
     ]
    }
   ],
   "source": [
    "licenses = pd.read_pickle('licenses.p')\n",
    "\n",
    "print(licenses.head())\n",
    "print(ward.head())\n",
    "\n",
    "\n",
    "\n",
    "ward_license = ward.merge(licenses, on='ward', suffixes=('_ward', '_lic'))\n",
    "print(ward_license.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cf916-41b2-466d-8b73-d03ddbe3493e",
   "metadata": {},
   "source": [
    "## One-to-many classification\n",
    "\n",
    "Understanding the difference between a one-to-one and one-to-many relationship is a useful skill. In this exercise, consider a set of tables from an e-commerce website. The hypothetical tables are the following:\n",
    "\n",
    "    A customer table with information about each customer\n",
    "    A cust_tax_info table with customers unique tax IDs\n",
    "    An orders table with information about each order\n",
    "    A products table with details about each unique product sold\n",
    "    An inventory table with information on how much total inventory is available to sell for each product\n",
    "\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Select the relationship type that is most appropriate for the relationship between the different tables: One-to-one, or One-to-many.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf875bb-4c74-49f8-b392-dea036e51d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relationship between products and inventory are one-to-one relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cde94-78f8-4de4-ae0f-339a799d28d4",
   "metadata": {},
   "source": [
    "## One-to-many merge\n",
    "\n",
    "A business may have one or multiple owners. In this exercise, you will continue to gain experience with one-to-many merges by merging a table of business owners, called 'biz_owners', to the 'licenses' table.  Recall from the video lesson, \n",
    "# with a one-to-many relationship, a row in the left table may be repeated if it is related to multiple rows in the right table. \n",
    "In this lesson, you will explore this further by finding out what is the most common business owner title. (i.e., secretary, CEO, or vice president)\n",
    "\n",
    "The licenses and biz_owners DataFrames are loaded for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Starting with the 'licenses' table on the left, merge it to the 'biz_owners' table on the column 'account', and save the results to a variable named 'licenses_owners'.\n",
    "#    Group licenses_owners by title and count the number of accounts for each title. Save the result as counted_df\n",
    "    Sort counted_df by the number of accounts in descending order, and save this as a variable named sorted_df.\n",
    "    Use the .head() method to print the first few rows of the sorted_df.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93084364-4456-4ca7-8b74-7cf12eacdbd4",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8218a72d-e2f0-4958-80e0-bd9bbe1907d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  account ward  aid              business              address    zip  \\\n",
      "0  307071    3  743  REGGIE'S BAR & GRILL      2105 S STATE ST  60616   \n",
      "1      10   10  829            HONEYBEERS  13200 S HOUSTON AVE  60633   \n",
      "2      10   10  829            HONEYBEERS  13200 S HOUSTON AVE  60633   \n",
      "3   10002   14  775           CELINA DELI    5089 S ARCHER AVE  60632   \n",
      "4   10002   14  775           CELINA DELI    5089 S ARCHER AVE  60632   \n",
      "\n",
      "  first_name last_name      title  \n",
      "0     ROBERT     GLICK     MEMBER  \n",
      "1      PEARL   SHERMAN  PRESIDENT  \n",
      "2      PEARL   SHERMAN  SECRETARY  \n",
      "3     WALTER    MROZEK    PARTNER  \n",
      "4     CELINA    BYRDAK    PARTNER  \n",
      "(19497, 9)\n",
      "title            account\n",
      "ASST. SECRETARY  16301      3\n",
      "                 57770      3\n",
      "                 11071      2\n",
      "                 16964      2\n",
      "                 1949       2\n",
      "                           ..\n",
      "VICE PRESIDENT   85943      1\n",
      "                 85956      1\n",
      "                 86083      1\n",
      "                 86202      1\n",
      "                 86216      1\n",
      "Name: account, Length: 17934, dtype: int64\n",
      "title               title             \n",
      "ASST. SECRETARY     ASST. SECRETARY        111\n",
      "BENEFICIARY         BENEFICIARY              4\n",
      "CEO                 CEO                    110\n",
      "DIRECTOR            DIRECTOR               146\n",
      "EXECUTIVE DIRECTOR  EXECUTIVE DIRECTOR      10\n",
      "GENERAL PARTNER     GENERAL PARTNER         21\n",
      "INDIVIDUAL          INDIVIDUAL             268\n",
      "LIMITED PARTNER     LIMITED PARTNER         26\n",
      "MANAGER             MANAGER                134\n",
      "MANAGING MEMBER     MANAGING MEMBER        878\n",
      "MEMBER              MEMBER                 884\n",
      "NOT APPLICABLE      NOT APPLICABLE          11\n",
      "OTHER               OTHER                 1200\n",
      "PARTNER             PARTNER                451\n",
      "PRESIDENT           PRESIDENT             6259\n",
      "PRINCIPAL OFFICER   PRINCIPAL OFFICER       63\n",
      "SECRETARY           SECRETARY             5205\n",
      "SHAREHOLDER         SHAREHOLDER            590\n",
      "SOLE PROPRIETOR     SOLE PROPRIETOR       1658\n",
      "SPOUSE              SPOUSE                  34\n",
      "TREASURER           TREASURER              447\n",
      "TRUSTEE             TRUSTEE                  6\n",
      "VICE PRESIDENT      VICE PRESIDENT         970\n",
      "Name: title, dtype: int64\n",
      "                    account\n",
      "title                      \n",
      "PRESIDENT              6259\n",
      "SECRETARY              5205\n",
      "SOLE PROPRIETOR        1658\n",
      "OTHER                  1200\n",
      "VICE PRESIDENT          970\n",
      "MEMBER                  884\n",
      "MANAGING MEMBER         878\n",
      "SHAREHOLDER             590\n",
      "PARTNER                 451\n",
      "TREASURER               447\n",
      "INDIVIDUAL              268\n",
      "DIRECTOR                146\n",
      "MANAGER                 134\n",
      "ASST. SECRETARY         111\n",
      "CEO                     110\n",
      "PRINCIPAL OFFICER        63\n",
      "SPOUSE                   34\n",
      "LIMITED PARTNER          26\n",
      "GENERAL PARTNER          21\n",
      "NOT APPLICABLE           11\n",
      "EXECUTIVE DIRECTOR       10\n",
      "TRUSTEE                   6\n",
      "BENEFICIARY               4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "biz_owners = pd.read_pickle('business_owners.p')\n",
    "\n",
    "\n",
    "licenses_owners = licenses.merge(biz_owners, on='account')\n",
    "print(licenses_owners.head())\n",
    "print(licenses_owners.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "print(licenses_owners.groupby('title')['account'].value_counts())\n",
    "# ***************************************************************************************************************** #\n",
    "print(licenses_owners.groupby('title')['title'].value_counts())\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "counted_df = licenses_owners.groupby('title')['account'].count()\n",
    "# ***************************************************************************************************************** #\n",
    "counted_df = licenses_owners.groupby('title').agg({'account':'count'})\n",
    "\n",
    "\n",
    "counted_df = counted_df.sort_values('account', ascending=False)\n",
    "\n",
    "\n",
    "print(counted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f8d89-444b-48c3-acfb-18c951d990ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the licenses and biz_owners table on account\n",
    "licenses_owners = licenses.merge(biz_owners, on='account')\n",
    "\n",
    "# Group the results by title then count the number of accounts\n",
    "counted_df = licenses_owners.groupby('title').agg({'account':'count'})\n",
    "\n",
    "# Sort the counted_df in desending order\n",
    "sorted_df = counted_df.sort_values('account', ascending=False)\n",
    "\n",
    "# Use .head() method to print the first few rows of sorted_df\n",
    "print(sorted_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39832022-8642-4891-af20-5efd45bb826f",
   "metadata": {},
   "source": [
    "## Merging multiple DataFrames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**In our last lesson, we learned how to merge two tables with a one-to-many relationship using the .merge() method.  Merging data like this is a necessary skill to bring together data from different sources to answer some more complex data questions.  Sometimes we need to merge together more than just two tables to complete our analysis.  \n",
    "\n",
    "\n",
    "In the previous lesson, we used two tables form the city of Chicago.  One table contained business licenses issued by the city, the other table listed info about the local neighborhoods called wards, including the local goverment official's office.  Now we also have a table of businesses that have received small business grant money from Chicago.  \n",
    "# *******************************************************************************************************************\n",
    "# The grants are funded by taxpayers money.  Therefore, it would be helpful to analyze how much grant money each business received and in what ward that business is located.  We then could determine if one ward's business received a disproportionately large anount of grant money.  \n",
    "\n",
    "\n",
    "\n",
    "To pull all of this information togerther, lets first connect our grants table to our licenses table.  The two tables are related by their company name and location.  Lets pause here for a moment.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "If we merge the two tables only using the zip column, then the 60616 zip of Reggie's Bar from the licenses table will be matched to multiple business in the grants table with the same zip.  \n",
    "\n",
    "Our code sample prints the first few rows and some columns of the merged table.  The output of the merge duplicates Reggie's Bar for each matching zip in the grats table, which is not what we want.  If instead, we merge on address only, there is a small risk that the address would repeat in different parts of the city.  Therefore, the best option is to merge the tables using the combination of both address and zip code.  (???????????????)\n",
    "\n",
    "We merge the two DFs as shown before, except in this case, we pass a list of the column names we want to merge on the the on= argument.  This allows us to use multiple columns in the merge.  As before, the matching rows between the two DFs are returned with the columns from the grants table table listed first.  However, when we merge on two columns, in this case address and zip code, we are requiring that both the address and zip code of a row in the left table match the address and zip code of a row in the right table in order for them tobe linked to each other in the merge.  \n",
    "\n",
    "We can now extend this example to third table.  First, we merge the grands table with the wards table on the ward column again, adding suffixes= to the repeated column names.  Note that we're using Python's backslash line continuation method to add the second merge on the next line.  Python will read this as just one line of code.  Don't forget your backslash.  \n",
    "\n",
    "Now our output table has information about grants, business, and wards.  We can now complete our analysis.  We can now sum the grants by ward and plot the results.  Some wards have received more grants than others.  \n",
    "\n",
    "\n",
    "We could continue to merge dditional tables as needed.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(licenses.head())\n",
    "\n",
    "----------------------------------------------------------------------------\n",
    "  account ward  aid                   business               address    zip\n",
    "0  307071    3  743       REGGIE'S BAR & GRILL       2105 S STATE ST  60616\n",
    "1      10   10  829                 HONEYBEERS   13200 S HOUSTON AVE  60633\n",
    "2   10002   14  775                CELINA DELI     5089 S ARCHER AVE  60632\n",
    "3   10005   12  NaN  KRAFT FOODS NORTH AMERICA        2005 W 43RD ST  60609\n",
    "4   10044   44  638  NEYBOUR'S TAVERN & GRILLE  3651 N SOUTHPORT AVE  60613\n",
    "\n",
    "\n",
    "\n",
    "grants = pd.read_csv('Small_Business_Grant_Agreements.csv')\n",
    "\n",
    "print(grants.head())\n",
    "\n",
    "-----------------------------------------------------------\n",
    "    address          | zip   | grant     | company\n",
    "0   1000 S Kostn...  | 60624 | 148914.50 | Nationwide F...\n",
    "1   1000 W 35th ST   | 60609 | 100000.00 | Small Batch,...\n",
    "2   1000 W Fulto...  | 60612 | 34412.50  | Fulton Marke...\n",
    "3   10008 S West...  | 60643 | 12285.32  | Law Offices...\n",
    "4   1002 W Argyl...  | 60640 | 28998.75  | Masala's Ind...\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "grants_licenses = grants.merge(licenses, on='zip')\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "print(grants_licenses.loc[grants_licenses['business']=='Reggie's Bar & Grill', \n",
    "                         ['grant', 'company', 'account', 'ward', 'business']])\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "    grant     | company         | account   | ward  | business\n",
    "0   136443.07 | Cedars Medit... | 307071    | 3     | Reggie's Bar & Grill\n",
    "1   39943.15  | Darryl & Fyl... | 307071    | 3     | Reggie's Bar & Grill\n",
    "2   31250.0   | Jgf Management  | 307071    | 3     | Reggie's Bar & Grill\n",
    "3   143427.79 | Hyde Park An... | 307071    | 3     | Reggie's Bar & Grill\n",
    "4   69500.00  | Zberry Inc      | 307071    | 3     | Reggie's Bar & Grill\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "grants.merge(licenses, on=['address', 'zip'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "grants_licenses_ward = grants.merge(licenses, on=['zip', 'address']) \\ \n",
    "                                    .merge(wards, on='ward', suffixes=('_bus', '_ward'))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grants_licenses_ward.groupby('ward')['grand'].agg('sum').plot(kind='bar', y='grant')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f03ce525-e278-451a-9d38-3f0bbc71a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  account ward  aid                   business               address    zip\n",
      "0  307071    3  743       REGGIE'S BAR & GRILL       2105 S STATE ST  60616\n",
      "1      10   10  829                 HONEYBEERS   13200 S HOUSTON AVE  60633\n",
      "2   10002   14  775                CELINA DELI     5089 S ARCHER AVE  60632\n",
      "3   10005   12  NaN  KRAFT FOODS NORTH AMERICA        2005 W 43RD ST  60609\n",
      "4   10044   44  638  NEYBOUR'S TAVERN & GRILLE  3651 N SOUTHPORT AVE  60613\n"
     ]
    }
   ],
   "source": [
    "print(licenses.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c113c-0ece-4cdd-91b2-3910e46d9d06",
   "metadata": {},
   "source": [
    "## Total riders in a month\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Your goal is to find the total number of rides provided to passengers passing through the Wilson station (station_name == 'Wilson') when riding Chicago's public transportation system on weekdays (day_type == 'Weekday') in July (month == 7). Luckily, Chicago provides this detailed data, but it is in three different tables. You will work on merging these tables together to answer the question. This data is different from the business related data you have seen so far, but all the information you need to answer the question is provided.\n",
    "\n",
    "The cal, ridership, and stations DataFrames have been loaded for you. The relationship between the tables can be seen in the diagram below.\n",
    "\n",
    "cal           ridership        stations\n",
    "  year -        station_id ----- station_id\n",
    "  month --    - year             station_name\n",
    "  day ---    -- month            location\n",
    "  day_type  --- day\n",
    "                rides\n",
    "\n",
    "Table diagram. The cal table relates to ridership via year, month, and day. The ridership table relates to the stations table via station_id.\n",
    "Instructions 1/3\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Merge the ridership and cal tables together, starting with the ridership table on the left and save the result to the variable ridership_cal. If you code takes too long to run, your merge conditions might be incorrect.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Extend the previous merge to three tables by also merging the stations table.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Create a variable called 'filter_criteria' to select the appropriate rows from the merged table so that you can sum the rides column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "de2385e6-f5d2-4217-bddc-206fd98dd95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  station_id        station_name                 location\n",
      "0      40010  Austin-Forest Park  (41.870851, -87.776812)\n",
      "1      40020         Harlem-Lake  (41.886848, -87.803176)\n",
      "2      40030        Pulaski-Lake  (41.885412, -87.725404)\n",
      "3      40040        Quincy/Wells   (41.878723, -87.63374)\n",
      "4      40050               Davis   (42.04771, -87.683543)\n",
      "   year  month  day        day_type\n",
      "0  2019      1    1  Sunday/Holiday\n",
      "1  2019      1    2         Weekday\n",
      "2  2019      1    3         Weekday\n",
      "3  2019      1    4         Weekday\n",
      "4  2019      1    5        Saturday\n",
      "  station_id  year  month  day  rides\n",
      "0      40010  2019      1    1    576\n",
      "1      40010  2019      1    2   1457\n",
      "2      40010  2019      1    3   1543\n",
      "3      40010  2019      1    4   1621\n",
      "4      40010  2019      1    5    719\n",
      "     station_id  year  month  day  rides        day_type\n",
      "2807      41660  2019     11    8  21170         Weekday\n",
      "1683      40010  2019      7    7    657  Sunday/Holiday\n",
      "1033      41440  2019      5   25   1130        Saturday\n",
      "221       41260  2019      1   25   1394         Weekday\n",
      "3025      40080  2019     12    3   4909         Weekday\n",
      "2272      40540  2019      9   10   7444         Weekday\n",
      "2781      40010  2019     11    6   2000         Weekday\n",
      "     station_id  year  month  day  rides        day_type        station_name  \\\n",
      "1459      40120  2019     12   31   1680         Weekday         35th/Archer   \n",
      "597       40080  2019      8   21   4799         Weekday            Sheridan   \n",
      "2252      41500  2019      3    4   2411         Weekday      Montrose-Brown   \n",
      "2996      41660  2019      3   18  20833         Weekday          Lake/State   \n",
      "299       40010  2019     10   27    596  Sunday/Holiday  Austin-Forest Park   \n",
      "47        40010  2019      2   17    444  Sunday/Holiday  Austin-Forest Park   \n",
      "1265      40120  2019      6   20   2640         Weekday         35th/Archer   \n",
      "311       40010  2019     11    8   1792         Weekday  Austin-Forest Park   \n",
      "\n",
      "                     location  \n",
      "1459  (41.829353, -87.680622)  \n",
      "597   (41.953775, -87.654929)  \n",
      "2252  (41.961756, -87.675047)  \n",
      "2996  (41.884809, -87.627813)  \n",
      "299   (41.870851, -87.776812)  \n",
      "47    (41.870851, -87.776812)  \n",
      "1265  (41.829353, -87.680622)  \n",
      "311   (41.870851, -87.776812)  \n",
      "\n",
      "\n",
      "140005\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "stations = pd.read_pickle('stations.p')\n",
    "cal = pd.read_pickle('cta_calendar.p')\n",
    "ridership = pd.read_pickle('cta_ridership.p')\n",
    "\n",
    "\n",
    "print(stations.head())\n",
    "print(cal.head())\n",
    "print(ridership.head())\n",
    "\n",
    "\n",
    "ridership_cal = ridership.merge(cal, on=['year', 'month', 'day'])\n",
    "print(ridership_cal.sample(7))\n",
    "\n",
    "\n",
    "\n",
    "ridership_cal_stations = ridership_cal.merge(stations, on='station_id')\n",
    "print(ridership_cal_stations.sample(8))\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "filter_criteria = ridership_cal_stations.loc[(ridership_cal_stations['station_name']=='Wilson') & \n",
    "                                         (ridership_cal_stations['month']==7) & \n",
    "                                         (ridership_cal_stations['day_type']=='Weekday'), 'rides'].sum()#value_counts()\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(filter_criteria)\n",
    "\n",
    "\n",
    "\n",
    "# To feel and to think hwo we get there, whats the best approach doing it.  Think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ca0b7-050c-49f3-aa53-ba78ca8fe980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n",
    "            \t\t\t\t.merge(stations, on='station_id')\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "\n",
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n",
    "\t\t\t\t\t\t\t.merge(stations, on='station_id')\n",
    "\n",
    "\n",
    "\n",
    "# Create a filter to filter ridership_cal_stations\n",
    "filter_criteria = ((ridership_cal_stations['month'] == ____) \n",
    "                   & (ridership_cal_stations['day_type'] == ____) \n",
    "                   & (ridership_cal_stations['station_name'] == ____))\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** \n",
    "\n",
    "# Use .loc and the filter to select for rides\n",
    "print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "# ***************************************************************************************************************** #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895df3e-968e-4483-b45d-7c0512888d2b",
   "metadata": {},
   "source": [
    "## Three table merge\n",
    "\n",
    "To solidify the concept of a three DataFrame merge, practice another exercise. A reasonable extension of our review of Chicago business data would include looking at demographics information about the neighborhoods where the businesses are. A table with the median income by zip code has been provided to you. You will merge the 'licenses' and 'wards' tables with this new income-by-zip-code table called 'zip_demo'.\n",
    "\n",
    "The licenses, wards, and zip_demo DataFrames have been loaded for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Starting with the 'licenses' table, merge to it the 'zip_demo' table on the 'zip' column. Then merge the resulting table to the wards table on the 'ward' column. Save result of the three merged tables to a variable named licenses_zip_ward.\n",
    "    Group the results of the three merged tables by the column alderman and find the median income.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b397c220-8ea8-4b1c-b768-e8d5ec5c57af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  account ward  aid                   business               address    zip\n",
      "0  307071    3  743       REGGIE'S BAR & GRILL       2105 S STATE ST  60616\n",
      "1      10   10  829                 HONEYBEERS   13200 S HOUSTON AVE  60633\n",
      "2   10002   14  775                CELINA DELI     5089 S ARCHER AVE  60632\n",
      "3   10005   12  NaN  KRAFT FOODS NORTH AMERICA        2005 W 43RD ST  60609\n",
      "4   10044   44  638  NEYBOUR'S TAVERN & GRILLE  3651 N SOUTHPORT AVE  60613\n",
      "  ward            alderman                          address    zip\n",
      "0    1  Proco \"Joe\" Moreno        2058 NORTH WESTERN AVENUE  60647\n",
      "1    2       Brian Hopkins       1400 NORTH  ASHLAND AVENUE  60622\n",
      "2    3          Pat Dowell          5046 SOUTH STATE STREET  60609\n",
      "3    4    William D. Burns  435 EAST 35TH STREET, 1ST FLOOR  60616\n",
      "4    5  Leslie A. Hairston            2325 EAST 71ST STREET  60649\n",
      "     zip  income\n",
      "0  60630   70122\n",
      "1  60640   50488\n",
      "2  60622   87143\n",
      "3  60614  100116\n",
      "4  60608   41226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alderman\n",
       "Ameya Pawar                   66246.0\n",
       "Anthony A. Beale              38206.0\n",
       "Anthony V. Napolitano         82226.0\n",
       "Ariel E. Reyboras             41307.0\n",
       "Brendan Reilly               110215.0\n",
       "Brian Hopkins                 87143.0\n",
       "Carlos Ramirez-Rosa           66246.0\n",
       "Carrie M. Austin              38206.0\n",
       "Chris Taliaferro              55566.0\n",
       "Daniel \"Danny\" Solis          41226.0\n",
       "David H. Moore                33304.0\n",
       "Deborah Mell                  66246.0\n",
       "Debra L. Silverstein          50554.0\n",
       "Derrick G. Curtis             65770.0\n",
       "Edward M. Burke               42335.0\n",
       "Emma M. Mitts                 36283.0\n",
       "George Cardenas               33959.0\n",
       "Gilbert Villegas              41307.0\n",
       "Gregory I. Mitchell           24941.0\n",
       "Harry Osterman                45442.0\n",
       "Howard B. Brookins, Jr.       33304.0\n",
       "James Cappleman               79565.0\n",
       "Jason C. Ervin                41226.0\n",
       "Joe Moore                     39163.0\n",
       "John S. Arena                 70122.0\n",
       "Leslie A. Hairston            28024.0\n",
       "Margaret Laurino              70122.0\n",
       "Marty Quinn                   67045.0\n",
       "Matthew J. O'Shea             59488.0\n",
       "Michael R. Zalewski           42335.0\n",
       "Michael Scott, Jr.            31445.0\n",
       "Michelle A. Harris            32558.0\n",
       "Michelle Smith               100116.0\n",
       "Milagros \"Milly\" Santiago     41307.0\n",
       "Nicholas Sposato              62223.0\n",
       "Pat Dowell                    46340.0\n",
       "Patrick Daley Thompson        41226.0\n",
       "Patrick J. O'Connor           50554.0\n",
       "Proco \"Joe\" Moreno            87143.0\n",
       "Raymond A. Lopez              33959.0\n",
       "Ricardo Munoz                 31445.0\n",
       "Roberto Maldonado             68223.0\n",
       "Roderick T. Sawyer            32558.0\n",
       "Scott Waguespack              68223.0\n",
       "Susan Sadlowski Garza         38417.0\n",
       "Tom Tunney                    88708.0\n",
       "Toni L. Foulkes               27573.0\n",
       "Walter Burnett, Jr.           87143.0\n",
       "William D. Burns             107811.0\n",
       "Willie B. Cochran             28024.0\n",
       "Name: income, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licenses = pd.read_pickle('licenses.p')\n",
    "wards = pd.read_pickle('ward.p')\n",
    "zip_demo = pd.read_pickle('zip_demo.p')\n",
    "\n",
    "\n",
    "print(licenses.head())\n",
    "print(wards.head())\n",
    "print(zip_demo.head())\n",
    "\n",
    "\n",
    "licenses_zip_ward = licenses.merge(zip_demo, on='zip').merge(wards, on='ward')\n",
    "licenses_zip_ward.groupby('alderman')['income'].median()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac7a58-3d52-4c57-a012-c4e2cddb43df",
   "metadata": {},
   "source": [
    "## One-to-many merge with multiple tables\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# In this exercise, assume that you are looking to start a business in the city of Chicago. Your perfect idea is to start a company that uses goats to mow the lawn for other businesses. However, you have to choose a location in the city to put your goat farm. You need a location with a great deal of space and relatively few businesses and people around to avoid complaints about the smell. You will need to merge three tables to help you choose your location. The land_use table has info on the percentage of vacant land by city ward. The census table has population by ward, and the licenses table lists businesses by ward.\n",
    "\n",
    "The land_use, census, and licenses tables have been loaded for you.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Merge 'land_use' and 'census' on the 'ward' column. Merge the result of this with 'licenses' on the 'ward' column, using the suffix '_cen' for the left table and '_lic' for the right table. Save this to the variable 'land_cen_lic'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Group 'land_cen_lic' by 'ward', 'pop_2010' (the population in 2010), and 'vacant', then count the number of accounts. Save the results to 'pop_vac_lic'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Sort 'pop_vac_lic' by 'vacant', 'account', 'andpop_2010' in descending, ascending, and ascending order respectively. Save it as sorted_pop_vac_lic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "763972ed-0885-4d2e-928f-de6177493700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ward  residential  commercial  industrial  vacant  other  pop_2000  \\\n",
      "0    1           41           9           2       2     46     52951   \n",
      "1    1           41           9           2       2     46     52951   \n",
      "2    1           41           9           2       2     46     52951   \n",
      "3    1           41           9           2       2     46     52951   \n",
      "4    1           41           9           2       2     46     52951   \n",
      "\n",
      "   pop_2010 change                  address_cen zip_cen account  aid  \\\n",
      "0     56149     6%  2765 WEST SAINT MARY STREET   60647   12024  NaN   \n",
      "1     56149     6%  2765 WEST SAINT MARY STREET   60647   14446  743   \n",
      "2     56149     6%  2765 WEST SAINT MARY STREET   60647   14624  775   \n",
      "3     56149     6%  2765 WEST SAINT MARY STREET   60647   14987  NaN   \n",
      "4     56149     6%  2765 WEST SAINT MARY STREET   60647   15642  814   \n",
      "\n",
      "               business              address_lic zip_lic  \n",
      "0   DIGILOG ELECTRONICS       1038 N ASHLAND AVE   60622  \n",
      "1      EMPTY BOTTLE INC   1035 N WESTERN AVE 1ST   60622  \n",
      "2  LITTLE MEL'S HOT DOG    2205 N CALIFORNIA AVE   60647  \n",
      "3    MR. BROWN'S LOUNGE   2301 W CHICAGO AVE 1ST   60622  \n",
      "4          Beat Kitchen  2000-2100 W DIVISION ST   60622  \n",
      "  ward  pop_2010  vacant  account\n",
      "0    1     56149       2      253\n",
      "1   10     51535      14      130\n",
      "2   11     51497       5      201\n",
      "3   12     52235       4      255\n",
      "4   13     53722       1      101\n",
      "   ward  pop_2010  vacant  account\n",
      "47    7     51581      19       80\n",
      "12   20     52372      15      123\n",
      "1    10     51535      14      130\n",
      "16   24     54909      13       98\n",
      "7    16     51954      13      156\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "land_use = pd.read_pickle('land_use.p')\n",
    "census = pd.read_pickle('census.p')\n",
    "licenses = pd.read_pickle('licenses.p')\n",
    "\n",
    "\n",
    "\n",
    "# Merge land_use and census and merge result with licenses including suffixes\n",
    "land_cen_lic = land_use.merge(census, on='ward').merge(licenses, on='ward', suffixes=('_cen', '_lic'))\n",
    "print(land_cen_lic.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge land_use and census and merge result with licenses including suffixes\n",
    "land_cen_lic = land_use.merge(census, on='ward') \\\n",
    "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the # of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward', 'pop_2010', 'vacant'], \n",
    "                                   as_index=False).agg({'account':'count'}).sort_index(level=['ward', 'pop_2010', 'vacant'])\n",
    "#*******************************************************************************************************************#\n",
    "\n",
    "print(pop_vac_lic.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge land_use and census and merge result with licenses including suffixes\n",
    "land_cen_lic = land_use.merge(census, on='ward') \\\n",
    "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the # of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'], \n",
    "                                   as_index=False).agg({'account':'count'})\n",
    "\n",
    "# Sort pop_vac_lic and print the results\n",
    "sorted_pop_vac_lic = pop_vac_lic.sort_values(['vacant', 'account', 'pop_2010'], \n",
    "                                             ascending=[False, True, True])\n",
    "\n",
    "# Print the top few rows of sorted_pop_vac_lic\n",
    "print(sorted_pop_vac_lic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee8e30-ee36-4d53-9b91-afd825f38a63",
   "metadata": {},
   "source": [
    "# Sort index we have to specify level=[columns], not needed when we apply it to the sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d91838-6d40-405c-b1a4-378854ec2964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1705e43c-2d8c-4a57-9bad-02d1234e0be3",
   "metadata": {},
   "source": [
    "## Left join\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**In this lesson, we'll discuss how a left join works, which is another way to merge two tables.  Before we start about left joins, lets quickly review what we have learned so far.  \n",
    "\n",
    "In chapter 1, we introduced the Pandas df.merge() method that allows us to combine two tables by specifying one or more key columns to link the tables by.  By default, the .merge() method performs an how='inner' join, returning only the rows of data with matching values in the key columns of both tables.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this lesson, we'll talk about the idea of a left join.  A left join returns all rows of data from the left table and only those rows from the right table where key columns match.  \n",
    "\n",
    "\n",
    "To help us learn more about left joins and other concepts in this chapter, we'll use data from The Movie Database, a company-built movie database  with info on thousands of movies, their casts, and popularity.  In our next example, we have two tables from The Movie Database that we want to merge.  \n",
    "\n",
    "Our first table, named movies, holds info about individual movies such as the title name and its popularity.  Additionally, each movie is given an ID number.  Our second table is named taglines, which contains a movie ID number and the tag line for the movie.  To merge these tables with a left join, we use our merge method similar to what we learned in chapter 1.  Here we list the movie table first and merge  it to the taglines table on the on='id' column in both tables.  However, notice an additional argument named how=, this argument defines how to merge the two tables.  Remember that Pandas use NaN to denote missing data.  \n",
    "\n",
    "After the merge, our resulting table has a 4805 rows.  This is because we are returning all of the rows of data from the movies table, and the relationship between the movies table and taglines table is one-to-one.  Therefore, in a one-to-one merge like this, a left join will always return the same number rows as the left table.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "left table                     right table                      result table\n",
    "\n",
    "------------------------      -------------------              ----------------------------------\n",
    " A      | B      | C          C       | D                       A      | B     | C     | D\n",
    " A2     | B2     | C2         C1      | D1                      A2     | B2    | C2    | D2\n",
    " A3     | B3     | C3         C2      | D2                      A3     | B3    | C3    |\n",
    " A4     | B4     | C4         C4      | D4                      A4     | B4    | C4    | D4\n",
    "                              C5      | D5\n",
    "\n",
    "\n",
    "movies = pd.read_csv('tmdb_movies.csv')\n",
    "print(movies.head())\n",
    "print(movies.shape)\n",
    "\n",
    "------------------------------------------------------------\n",
    "      id                 title  popularity release_date\n",
    "0    257          Oliver Twist   20.415572   2005-09-23\n",
    "1  14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
    "2  38365             Grown Ups   38.864027   2010-06-24\n",
    "3   9672              Infamous    3.680896   2006-11-16\n",
    "4  12819       Alpha and Omega   12.300789   2010-09-17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "161a716a-11d7-48b1-b04a-0d0a44d48aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                 title  popularity release_date\n",
      "0    257          Oliver Twist   20.415572   2005-09-23\n",
      "1  14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
      "2  38365             Grown Ups   38.864027   2010-06-24\n",
      "3   9672              Infamous    3.680896   2006-11-16\n",
      "4  12819       Alpha and Omega   12.300789   2010-09-17\n",
      "(4803, 4)\n",
      "       id                                         tagline\n",
      "0   19995                     Enter the World of Pandora.\n",
      "1     285  At the end of the world, the adventure begins.\n",
      "2  206647                           A Plan No One Escapes\n",
      "3   49026                                 The Legend Ends\n",
      "4   49529            Lost in our world, found in another.\n",
      "(3955, 2)\n",
      "\n",
      "\n",
      "      id                 title  popularity release_date  \\\n",
      "0    257          Oliver Twist   20.415572   2005-09-23   \n",
      "1  14290  Better Luck Tomorrow    3.877036   2002-01-12   \n",
      "2  38365             Grown Ups   38.864027   2010-06-24   \n",
      "3   9672              Infamous    3.680896   2006-11-16   \n",
      "4  12819       Alpha and Omega   12.300789   2010-09-17   \n",
      "\n",
      "                                           tagline  \n",
      "0                                              NaN  \n",
      "1             Never underestimate an overachiever.  \n",
      "2  Boys will be boys. . . some longer than others.  \n",
      "3          There's more to the story than you know  \n",
      "4                           A Pawsome 3D Adventure  \n",
      "(4803, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_pickle('movies.p')\n",
    "print(movies.head())\n",
    "print(movies.shape)\n",
    "\n",
    "\n",
    "taglines = pd.read_pickle('taglines.p')\n",
    "print(taglines.head())\n",
    "print(taglines.shape)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "movies_taglines = movies.merge(taglines, on='id', how='left')\n",
    "print(movies_taglines.head())\n",
    "print(movies_taglines.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e5e95-d3cb-4ddb-a9cd-32e94f4f53ad",
   "metadata": {},
   "source": [
    "## Counting missing rows with left join\n",
    "\n",
    "The Movie Database is supported by volunteers going out into the world, collecting data, and entering it into the database. This includes financial data, such as movie budget and revenue. If you wanted to know which movies are still missing data, you could use a left join to identify them. Practice using a left join by merging the 'movies' table and the 'financials' table.\n",
    "\n",
    "The movies and financials tables have been loaded for you.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    What column is likely the best column to merge the two tables on?\n",
    "    Possible Answers\n",
    "\n",
    "    on='budget'\n",
    "    on='popularity'\n",
    "    on='id'\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2     Missing\n",
    "    Merge the 'movies' table, as the left table, with the 'financials' table using a left join, and save the result to 'movies_financials'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3     Missing\n",
    "    Count the number of rows in movies_financials with a null value in the budget column.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9d0d35-9764-4fae-9fc0-fc22cfaa77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id     budget       revenue\n",
      "0   19995  237000000  2.787965e+09\n",
      "1     285  300000000  9.610000e+08\n",
      "2  206647  245000000  8.806746e+08\n",
      "3   49026  250000000  1.084939e+09\n",
      "4   49529  260000000  2.841391e+08\n",
      "(3229, 3)\n",
      "      id                 title  popularity release_date\n",
      "0    257          Oliver Twist   20.415572   2005-09-23\n",
      "1  14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
      "2  38365             Grown Ups   38.864027   2010-06-24\n",
      "3   9672              Infamous    3.680896   2006-11-16\n",
      "4  12819       Alpha and Omega   12.300789   2010-09-17\n",
      "(4803, 4)\n",
      "      id                 title  popularity release_date      budget  \\\n",
      "0    257          Oliver Twist   20.415572   2005-09-23  50000000.0   \n",
      "1  14290  Better Luck Tomorrow    3.877036   2002-01-12         NaN   \n",
      "2  38365             Grown Ups   38.864027   2010-06-24  80000000.0   \n",
      "3   9672              Infamous    3.680896   2006-11-16  13000000.0   \n",
      "4  12819       Alpha and Omega   12.300789   2010-09-17  20000000.0   \n",
      "\n",
      "       revenue  \n",
      "0   42093706.0  \n",
      "1          NaN  \n",
      "2  271430189.0  \n",
      "3    1151330.0  \n",
      "4   39300000.0  \n",
      "(4803, 6)\n",
      "\n",
      "\n",
      "id                 0\n",
      "title              0\n",
      "popularity         0\n",
      "release_date       1\n",
      "budget          1574\n",
      "revenue         1574\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "financials = pd.read_pickle('financials.p')\n",
    "print(financials.head())\n",
    "print(financials.shape)\n",
    "\n",
    "\n",
    "print(movies.head())\n",
    "print(movies.shape)\n",
    "\n",
    "\n",
    "movies_financials = movies.merge(financials, on='id', how='left')\n",
    "print(movies_financials.head())\n",
    "print(movies_financials.shape)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(movies_financials.isna().sum())\n",
    "\n",
    "\n",
    "# If you wanted to know which movies are still missing data, you could use a left join to identify them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f7b80-e0a0-4d8b-8cb4-b034d1c23f55",
   "metadata": {},
   "source": [
    "## Enriching a dataset\n",
    "\n",
    "Setting how='left' with the .merge() method is a useful technique for enriching or enhancing a dataset with additional information from a different table. In this exercise, you will start off with a sample of movie data from the movie series Toy Story. Your goal is to enrich this data by adding the marketing tag line for each movie. You will compare the results of a left join versus an inner join.\n",
    "\n",
    "The 'toy_story' DataFrame contains the Toy Story movies. The toy_story and 'taglines' DataFrames have been loaded for you.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Merge toy_story and taglines on the id column with a left join, and save the result as toystory_tag.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    With toy_story as the left table, merge to it taglines on the id column with an inner join, and save as toystory_tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646888f-5c63-4656-ae7c-94ee304c6a97",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e098679-8147-4ab9-aa7c-616fb93650b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id        title  popularity release_date\n",
      "103   10193  Toy Story 3   59.995418   2010-06-16\n",
      "2637    863  Toy Story 2   73.575118   1999-10-30\n",
      "3716    862    Toy Story   73.640445   1995-10-30\n",
      "\n",
      "\n",
      "      id        title  popularity release_date                   tagline\n",
      "0  10193  Toy Story 3   59.995418   2010-06-16  No toy gets left behind.\n",
      "1    863  Toy Story 2   73.575118   1999-10-30        The toys are back!\n",
      "2    862    Toy Story   73.640445   1995-10-30                       NaN\n",
      "(3, 5)\n",
      "\n",
      "\n",
      "      id        title  popularity release_date                   tagline\n",
      "0  10193  Toy Story 3   59.995418   2010-06-16  No toy gets left behind.\n",
      "1    863  Toy Story 2   73.575118   1999-10-30        The toys are back!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#df[df['ids'].str.contains(\"ball\")]\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "\n",
    "toy_storys = movies[movies['title'].str.contains('Toy Story')].copy()\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "\n",
    "print(toy_storys)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Merge the toy_story and taglines tables with a left join\n",
    "toystory_tag = toy_storys.merge(taglines, on='id', how='left')\n",
    "\n",
    "# Print the rows and shape of toystory_tag\n",
    "print(toystory_tag)\n",
    "print(toystory_tag.shape)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "toystory_tag_in = toy_storys.merge(taglines, on='id', how='inner')\n",
    "print(toystory_tag_in)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14420f-cdbc-4764-8624-251a36508fce",
   "metadata": {},
   "source": [
    "## How many rows with a left join?\n",
    "\n",
    "Select the true statement about left joins.\n",
    "\n",
    "Try running the following code statements in the IPython shell.\n",
    "\n",
    "    left_table.merge(one_to_one, on='id', how='left').shape\n",
    "    left_table.merge(one_to_many, on='id', how='left').shape\n",
    "\n",
    "Note that the left_table starts out with 4 rows.\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    The output of a one-to-one merge with a left join will have more rows than the left table.\n",
    "    The output of a one-to-one merge with a left join will have fewer rows than the left table.\n",
    "#    The output of a one-to-many merge with a left join will have greater than or equal rows than the left table.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbacb8-cf91-412c-8b82-f7344ba6803a",
   "metadata": {},
   "source": [
    "## Other joins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**You now know how to use the .merge() method to perform an how='inner' and how='left' join.  The .merge() method supports two other join types.  Lets start with the right-join.  It will return all of the rows from the right table and includes only those rows from the left table that matching values.  It is the mirror opposite of the left-join.  \n",
    "\n",
    "\n",
    "For this lesson, lets look at another table called 'movie_to_genres'.  Movies can have multiple genres, and this table lists different genres for each movie.  For our right join example, lets take a sample of this data subsetting to develop a table of movies from the tv movie genre.  \n",
    "\n",
    "Our goal is to merge it with the movies table.  We will set movies as our left table and merge it with the tv_genre table.  We want to use a right join to check that our movies table is not missing data.  In addition to showing a right join, this example also allows us to look at another feature.  Notice that the column with the movie id number in the movies table is named id, and in the tv_genre table it is named movie_id.  The .merge() method has a feature to take this into account.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The code for this merge has some new elements.  First of all, we set how= argument to right so that the merge performs a right join.  Additionally, we introduce two new arguments, named left_on= and right_on=.  They allow us to tell the .merge() wich key columns from each table to merge the tables.  \n",
    "\n",
    "\n",
    "Lets move on to our last type of join.  Our last type of join is called outer-join.  An outer join will return all of the rows from both tables regardless if there is a match between the tables.  For an example of this, we filter the movie_to_genres table as before into two very small tables.  One table has data on family movies, and the other has comedy movies.  \n",
    "\n",
    "\n",
    "\n",
    "left table                     right table                      result table     Outer-Join\n",
    "\n",
    "------------------------      -------------------              ----------------------------------\n",
    " A      | B      | C          C       | D                       A      | B     | C     | D\n",
    " A2     | B2     | C2         C1      | D1                             |       | C1    | D1\n",
    " A3     | B3     | C3         C2      | D2                      A2     | B2    | C2    | D2\n",
    " A4     | B4     | C4         C4      | D4                      A3     | B3    | C3    |\n",
    "                              C5      | D5                      A4     | B4    | C4    | D4\n",
    "                                                                       |       | C5    | D5\n",
    "\n",
    "\n",
    "f = movie_to_genres['genre'] == 'Family'\n",
    "family = movie_to_genres[f].head()\n",
    "\n",
    "\n",
    "g = movie_to_genres['genres'] == 'Comedy'\n",
    "comedy = movies_to_genres[g].head()\n",
    "\n",
    "\n",
    "family_comedy = family.merge(comedy, how='outer', on='movie_id', suffixes=('_fam', '_com'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8db4a8a1-5dfa-4121-b858-e25b046db447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movie_id      genre\n",
      "4656     10488    Fantasy\n",
      "7880     27022      Drama\n",
      "6656     15699    Foreign\n",
      "5034     10996  Adventure\n",
      "8949     45649     Comedy\n",
      "4572     10386  Animation\n",
      "2659      6615    Romance\n",
      "(12160, 2)\n",
      "          id                           title  popularity release_date\n",
      "989   323967                          Walter    2.659006   2015-03-13\n",
      "3439    9276                     The Faculty   21.535126   1998-12-25\n",
      "2348    7552          Fun with Dick and Jane   25.159168   2005-12-21\n",
      "40     18923  A Home at the End of the World    5.483531   2004-07-23\n",
      "935    47607                  Tiny Furniture    2.380332   2010-11-12\n",
      "3684   23730                        C.H.U.D.    4.455440   1984-08-31\n",
      "1788   10740                           Birth    7.698075   2004-09-08\n",
      "(4803, 4)\n",
      "       movie_id     genre\n",
      "7443      22488  TV Movie\n",
      "11096    205321  TV Movie\n",
      "10790    153397  TV Movie\n",
      "4998      10947  TV Movie\n",
      "10835    158150  TV Movie\n",
      "5994      13187  TV Movie\n",
      "11282    231617  TV Movie\n",
      "(8, 2)\n",
      "       id                      title  popularity release_date     genre\n",
      "7  231617  Signed, Sealed, Delivered    1.444476   2013-10-13  TV Movie\n",
      "5  158150        How to Fall in Love    1.923514   2012-07-21  TV Movie\n",
      "0   10947        High School Musical   16.536374   2006-01-20  TV Movie\n",
      "3   78814       We Have Your Husband    0.102003   2011-11-12  TV Movie\n",
      "4  153397                   Restless    0.812776   2012-12-07  TV Movie\n",
      "6  205321                  Sharknado   20.466433   2013-07-11  TV Movie\n",
      "1   13187  A Charlie Brown Christmas    8.701183   1965-12-09  TV Movie\n",
      "(8, 5)\n",
      "id              0\n",
      "title           0\n",
      "popularity      0\n",
      "release_date    0\n",
      "genre           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "movie_to_genres = pd.read_pickle('movie_to_genres.p')\n",
    "print(movie_to_genres.sample(7))\n",
    "print(movie_to_genres.shape)\n",
    "\n",
    "\n",
    "movies = pd.read_pickle('movies.p')\n",
    "print(movies.sample(7))\n",
    "print(movies.shape)\n",
    "\n",
    "\n",
    "tv_genre = movie_to_genres[movie_to_genres['genre']=='TV Movie'].copy()\n",
    "print(tv_genre.sample(7))\n",
    "print(tv_genre.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "tv_movies = movies.merge(tv_genre, how='right', left_on='id', right_on='movie_id').drop('movie_id', axis=1)\n",
    "print(tv_movies.sample(7))\n",
    "print(tv_movies.shape)\n",
    "\n",
    "print(tv_movies.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196167e6-34c7-417a-891c-7653f256dd32",
   "metadata": {},
   "source": [
    "## Right join to find unique movies\n",
    "\n",
    "Most of the recent big-budget science fiction movies can also be classified as action movies. You are given a table of science fiction movies called 'scifi_movies' and another table of action movies called 'action_movies'. \n",
    "# Your goal is to find which movies are considered only science fiction movies. \n",
    "Once you have this table, you can merge the movies table in to see the movie names. Since this exercise is related to science fiction movies, use a right join as your superhero power to solve this problem.\n",
    "\n",
    "The movies, scifi_movies, and action_movies tables have been loaded for you.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Merge 'action_movies' and 'scifi_movies' tables with a right join on 'movie_id'. Save the result as 'action_scifi'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Update the merge to add suffixes, where '_act' and '_sci' are suffixes for the left and right tables, respectively.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    From 'action_scifi', subset only the rows where the 'genre_act' column is null.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Merge 'movies' and 'scifi_only' using the 'id' column in the left table and the 'movie_id' column in the right table with an inner join.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166de28-e13f-419c-a7b8-1c1e8d9a0680",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8ad77b5-7134-400a-9e36-4be8060c9473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id genre_act        genre_sci\n",
      "0        11    Action  Science Fiction\n",
      "1        18    Action  Science Fiction\n",
      "2        19       NaN  Science Fiction\n",
      "3        38       NaN  Science Fiction\n",
      "4        62       NaN  Science Fiction\n",
      "     movie_id genre_act        genre_sci\n",
      "297     15028       NaN  Science Fiction\n",
      "499    227156       NaN  Science Fiction\n",
      "518    286217       NaN  Science Fiction\n",
      "329     20542       NaN  Science Fiction\n",
      "18        168       NaN  Science Fiction\n",
      "431     70981       NaN  Science Fiction\n",
      "519    289180       NaN  Science Fiction\n",
      "         id                               title  popularity release_date  \\\n",
      "204    9035                             Slither   17.327279   2006-03-31   \n",
      "63     3028  Jekyll and Hyde ... Together Again    0.805312   1982-11-10   \n",
      "57       78                        Blade Runner   94.056131   1982-06-25   \n",
      "21    11687                        The Visitors    8.893676   1993-01-27   \n",
      "16    20455                  Digimon: The Movie    3.738700   2000-03-17   \n",
      "167  326576           Dawn of the Crescent Moon    0.049469   2014-04-05   \n",
      "207   68684                           Detention   10.424596   2011-03-16   \n",
      "\n",
      "    genre_act        genre_sci  \n",
      "204       NaN  Science Fiction  \n",
      "63        NaN  Science Fiction  \n",
      "57        NaN  Science Fiction  \n",
      "21        NaN  Science Fiction  \n",
      "16        NaN  Science Fiction  \n",
      "167       NaN  Science Fiction  \n",
      "207       NaN  Science Fiction  \n"
     ]
    }
   ],
   "source": [
    "scifi_movies = movie_to_genres[movie_to_genres['genre']=='Science Fiction']\n",
    "action_movies = movie_to_genres[movie_to_genres['genre']=='Action']\n",
    "\n",
    "\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right', suffixes=('_act', '_sci'))\n",
    "print(action_scifi.head())\n",
    "\n",
    "\n",
    "# From action_scifi, subset only the rows where the genre_act column is null.\n",
    "scifi_only = action_scifi[action_scifi['genre_act'].isna()].copy()\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "\n",
    "print(scifi_only.sample(7))\n",
    "\n",
    "\n",
    "\n",
    "# Merge movies and scifi_only using the id column in the left table and the movie_id column in \n",
    "# the right table with an inner join.\n",
    "merged_df = movies.merge(scifi_only, how='inner', left_on='id', right_on='movie_id').drop('movie_id', axis=1)\n",
    "print(merged_df.sample(7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69f1da53-455a-45b0-b9c9-bc3ab3a2d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    movie_id            genre\n",
      "2         11  Science Fiction\n",
      "17        18  Science Fiction\n",
      "20        19  Science Fiction\n",
      "38        38  Science Fiction\n",
      "49        62  Science Fiction\n",
      "    movie_id   genre\n",
      "3         11  Action\n",
      "14        18  Action\n",
      "25        22  Action\n",
      "26        24  Action\n",
      "42        58  Action\n",
      "\n",
      "\n",
      "         id               title  popularity release_date            genre\n",
      "119    8408     Day of the Dead   18.025206   1985-07-19  Science Fiction\n",
      "94    10253  Dragon Wars: D-War    4.822033   2007-02-08  Science Fiction\n",
      "244   22824     The Fourth Kind   22.888173   2009-11-06  Science Fiction\n",
      "166    9824         Mystery Men   12.948627   1999-08-06  Science Fiction\n",
      "323    5876            The Mist   37.104353   2007-11-21  Science Fiction\n",
      "500  347548         Containment    2.122451   2015-07-09  Science Fiction\n",
      "280   15028       Clockstoppers    5.894163   2002-03-17  Science Fiction\n",
      "         id                                  title  popularity release_date  \\\n",
      "351   72331        Abraham Lincoln: Vampire Hunter   38.634767   2012-06-20   \n",
      "118   10569                      Small Time Crooks   11.423533   2000-05-19   \n",
      "280    1735  The Mummy: Tomb of the Dragon Emperor   60.034162   2008-07-01   \n",
      "900    9772                          Air Force One   36.387950   1997-07-25   \n",
      "1132  11370                          The Musketeer    3.296933   2001-09-07   \n",
      "438   76349                                   1911    4.065212   2011-07-03   \n",
      "381   36648                         Blade: Trinity   49.660055   2004-12-08   \n",
      "\n",
      "       genre  \n",
      "351   Action  \n",
      "118   Action  \n",
      "280   Action  \n",
      "900   Action  \n",
      "1132  Action  \n",
      "438   Action  \n",
      "381   Action  \n",
      "\n",
      "\n",
      "        id                         title  popularity release_date  \\\n",
      "283    173  20,000 Leagues Under the Sea    9.358197   1954-12-23   \n",
      "19   38055                      Megamind   68.757242   2010-10-28   \n",
      "336  10077            A Sound of Thunder    7.342892   2005-05-15   \n",
      "455  14353       Repo! The Genetic Opera    5.897479   2008-07-18   \n",
      "407  34086               Spaced Invaders    1.586598   1990-04-27   \n",
      "140  23963                     Pontypool    4.680206   2009-03-06   \n",
      "428    841                          Dune   25.876582   1984-12-14   \n",
      "\n",
      "    genre_action      genre_scifi  \n",
      "283          NaN  Science Fiction  \n",
      "19        Action  Science Fiction  \n",
      "336       Action  Science Fiction  \n",
      "455          NaN  Science Fiction  \n",
      "407          NaN  Science Fiction  \n",
      "140          NaN  Science Fiction  \n",
      "428       Action  Science Fiction  \n"
     ]
    }
   ],
   "source": [
    "scifi = movie_to_genres[movie_to_genres['genre']=='Science Fiction']\n",
    "action = movie_to_genres[movie_to_genres['genre']=='Action']\n",
    "\n",
    "print(scifi.head())\n",
    "print(action.head())\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "scifi_movies = movies.merge(scifi, how='inner', left_on='id', right_on='movie_id').drop('movie_id', axis=1)\n",
    "action_movies = movies.merge(action, how='inner', left_on='id', right_on='movie_id').drop('movie_id', axis=1)\n",
    "\n",
    "print(scifi_movies.sample(7))\n",
    "print(action_movies.sample(7))\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "action_scifi = action_movies.merge(scifi_movies, \n",
    "                                   how='right', \n",
    "                                   on=['id', 'title', 'popularity', 'release_date'], \n",
    "                                   suffixes=('_action', '_scifi'))\n",
    "\n",
    "print(action_scifi.sample(7))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf3c50-7b44-491d-a143-cd98b724e9d7",
   "metadata": {},
   "source": [
    "## Popular genres with right join\n",
    "\n",
    "# What are the genres of the most popular movies? \n",
    "To answer this question, you need to merge data from the 'movies' and 'movie_to_genres' tables. In a table called 'pop_movies', the top 10 most popular movies in the movies table have been selected. To ensure that you are analyzing all of the popular movies, merge it with the 'movie_to_genres' table using a right join. To complete your analysis, count the number of different genres. Also, the two tables can be merged by the movie ID. However, in 'pop_movies' that column is called id, and in 'movies_to_genres' it's called movie_id.\n",
    "\n",
    "The pop_movies and movie_to_genres tables have been loaded for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Merge 'movie_to_genres' and 'pop_movies' using a right join. Save the results as 'genres_movies'.\n",
    "    Group genres_movies by genre and count the number of id values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce80fa85-aa23-4241-a56b-6723cd23c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                    title  popularity release_date\n",
      "4546  211672                  Minions  875.581305   2015-06-17\n",
      "4343  157336             Interstellar  724.247784   2014-11-05\n",
      "1966  293660                 Deadpool  514.569956   2016-02-09\n",
      "2423  118340  Guardians of the Galaxy  481.098624   2014-07-30\n",
      "4220   76341       Mad Max: Fury Road  434.278564   2015-05-13\n",
      "   movie_id      genre         title  popularity release_date\n",
      "0    211672     Comedy       Minions  875.581305   2015-06-17\n",
      "1    211672  Adventure       Minions  875.581305   2015-06-17\n",
      "2    211672  Animation       Minions  875.581305   2015-06-17\n",
      "3    211672     Family       Minions  875.581305   2015-06-17\n",
      "4    157336      Drama  Interstellar  724.247784   2014-11-05\n",
      "genre\n",
      "Action             7\n",
      "Adventure          9\n",
      "Animation          2\n",
      "Comedy             3\n",
      "Drama              2\n",
      "Family             2\n",
      "Fantasy            1\n",
      "Science Fiction    6\n",
      "Thriller           4\n",
      "Name: movie_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pop_movies = movies.sort_values('popularity', ascending=False)[:10].copy()\n",
    "\n",
    "print(pop_movies.head())\n",
    "\n",
    "\n",
    "\n",
    "genres_movies = movie_to_genres.merge(pop_movies, \n",
    "                                      left_on='movie_id', right_on='id', \n",
    "                                      how='right').drop('id',axis=1)\n",
    "print(genres_movies.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "print(genres_movies.groupby('genre')['movie_id'].count())#value_counts()\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "genres_movies.groupby('genre')['movie_id'].count().plot(kind='bar', rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7c817-205a-45c3-bdfc-938373c9ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use right join to merge the movie_to_genres and pop_movies tables\n",
    "genres_movies = ____.merge(____, how='____', \n",
    "                                      ____, \n",
    "                                      ____)\n",
    "\n",
    "# Count the number of genres\n",
    "genre_count = genres_movies.groupby('____').agg({'id':'count'})\n",
    "\n",
    "# Plot a bar chart of the genre_count\n",
    "genre_count.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2464a34-2034-4d37-8d9d-d88299ea222a",
   "metadata": {},
   "source": [
    "## Using outer join to select actors\n",
    "\n",
    "# One cool aspect of using an outer join is that, because it returns all rows from both merged tables and null where they do not match, you can use it to find rows that do not have a match in the other table. \n",
    "To try for yourself, you have been given two tables with a list of actors from two popular movies: Iron Man 1 and Iron Man 2. Most of the actors played in both movies. Use an outer join to find actors who did not act in both movies.\n",
    "\n",
    "The Iron Man 1 table is called 'iron_1_actors', and Iron Man 2 table is called 'iron_2_actors'. Both tables have been loaded for you and a few rows printed so you can see the structure.\n",
    "\n",
    "Venn graph with no overlap\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Save to 'iron_1_and_2' the merge of 'iron_1_actors' (left) with 'iron_2_actors' tables with an outer join on the 'id' column, and set suffixes to ('_1','_2').\n",
    "#    Create an index that returns True if name_1 or name_2 are null, and False otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2415729-f24e-4089-8d12-33139e00d4b6",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e5f88d4-8e94-421a-a7ed-ec2f08ac1ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Actor                   Title\n",
      "0  [Emil Eifrem,Hugo Weaving,Laurence Fishburne,C...              The Matrix\n",
      "1  [Hugo Weaving,Laurence Fishburne,Carrie-Anne M...     The Matrix Reloaded\n",
      "2  [Hugo Weaving,Laurence Fishburne,Carrie-Anne M...  The Matrix Revolutions\n",
      "3           [Al Pacino,Charlize Theron,Keanu Reeves]    The Devil's Advocate\n",
      "4  [James Marshall,Kevin Pollak,J.T. Walsh,Aaron ...          A Few Good Men\n",
      "                                               Actor               Title\n",
      "4  [James Marshall,Kevin Pollak,J.T. Walsh,Aaron ...      A Few Good Men\n",
      "8  [Helen Hunt,Jack Nicholson,Cuba Gooding Jr.,Gr...  As Good as It Gets\n",
      "\n",
      "\n",
      "                                             Actor_1               Title  \\\n",
      "0  [James Marshall,Kevin Pollak,J.T. Walsh,Aaron ...      A Few Good Men   \n",
      "1                                                NaN  As Good as It Gets   \n",
      "\n",
      "                                             Actor_2  \n",
      "0                                                NaN  \n",
      "1  [Helen Hunt,Jack Nicholson,Cuba Gooding Jr.,Gr...  \n",
      "                                             Actor_1               Title  \\\n",
      "0  [James Marshall,Kevin Pollak,J.T. Walsh,Aaron ...      A Few Good Men   \n",
      "1                                                NaN  As Good as It Gets   \n",
      "\n",
      "                                             Actor_2  index  \n",
      "0                                                NaN   True  \n",
      "1  [Helen Hunt,Jack Nicholson,Cuba Gooding Jr.,Gr...   True  \n",
      "\n",
      "\n",
      "                                             Actor_1               Title  \\\n",
      "0  [James Marshall,Kevin Pollak,J.T. Walsh,Aaron ...      A Few Good Men   \n",
      "1                                                NaN  As Good as It Gets   \n",
      "\n",
      "                                             Actor_2  index  \n",
      "0                                                NaN   True  \n",
      "1  [Helen Hunt,Jack Nicholson,Cuba Gooding Jr.,Gr...   True  \n"
     ]
    }
   ],
   "source": [
    "actors = pd.read_csv('actors_movies.csv')\n",
    "print(actors.head())\n",
    "\n",
    "\n",
    "iron_1_actors = actors[actors['Title'].str.contains('Good')].copy()\n",
    "print(iron_1_actors)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "iron_1_actors = actors[actors['Title']=='A Few Good Men'].copy()\n",
    "iron_2_actors = actors[actors['Title']=='As Good as It Gets'].copy()\n",
    "\n",
    "iron_1_and_2 = iron_1_actors.merge(iron_2_actors, on='Title', how='outer', suffixes=('_1', '_2'))\n",
    "print(iron_1_and_2)\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "iron_1_and_2['index'] = (iron_1_and_2['Actor_1'].isna()) | (iron_1_and_2['Actor_2'].isna())\n",
    "\n",
    "print(iron_1_and_2)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "m = ((iron_1_and_2['Actor_1'].isnull()) | (iron_1_and_2['Actor_2'].isnull()))\n",
    "\n",
    "\n",
    "\n",
    "# Print the first few rows of iron_1_and_2\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "print(iron_1_and_2[m].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e7ffc-c56f-49c4-9405-0dceea753ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes\n",
    "iron_1_and_2 = iron_1_actors.merge(iron_2_actors, \n",
    "                                     on='id', \n",
    "                                     how='outer', \n",
    "                                     suffixes=('_1','_2'))\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "# Create an index that returns true if name_1 or name_2 are null\n",
    "m = ((iron_1_and_2['name_1'].isnull()) | \n",
    "     (iron_1_and_2['name_2'].isnull()))\n",
    "\n",
    "\n",
    "# Print the first few rows of iron_1_and_2\n",
    "print(iron_1_and_2[m].head())\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4f0b0-8939-47d6-a047-05b17174fdd5",
   "metadata": {},
   "source": [
    "## Merging a table to itself\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**In this lesson, we'll talk about merge a table to itself.  This type of merge is also referred as a self join.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# So when would you ever need to merge a table to itself?  \n",
    "\n",
    "The table shown here is called sequels and has three columns, it contains a column for movie id, title, and sequel.  The sequel number refers to the movie id that is sequel to the original movie.  For example in the second row the movie is titled Toy Story, and it has an id equal to 862.  The sequel number of this row is 863, and its the movie id for Toy Story 2, the sequel to Toy Story.  \n",
    "\n",
    "If we would like to see a table with the movies and the corresponding sequel movie in one row of the table, we will need to merge the table to itself.  To complete this merge, we set the sequel table as input to the merge method for both left and right tables.  We can think of it as merging two copies of the same table.  All of the aspects we have reviewed regarding merging two tables still apply here.  Therefore, we can merge the tables on different columns.  We'll use the left_on= and right_on= attributes to match rows where the sequel's id matches the original movie's id.  Finally setting the suffixes argument in the merge method allows us to identify which columns describe the original movie and which describe the sequel.  Here we listed the original movie and its sequel in one row.  \n",
    "\n",
    "\n",
    "Now that we have our result table from the merge, we could select only the titles columns, and we can see that we've successfully linked each movie to its sequel.  ( I guess this is where the chop DF to half and stack comes from )\n",
    "\n",
    "\n",
    "Pausing here is a good time to highlight again that when merging a table to itself, we can use the different types of joins we have already reviewed.  Lets take the same merge form earlier but make it a left join.  Now the resulting table will show all of our original movie info.  If the sequel movie exists in the table, it will fill out the rest of the row.  If you compare this to our earlier merge, you now see movies like Avatar and Titanic in the result table.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "You might need to merge a table to itself when working with tables that have hierarchical relationship, like employee and manager.  You might use this on a sequential relationship such as logistic movements.  Graph data, such as networks of friends, might also required this technique.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d18b56b4-0bdb-43bd-88b8-4f12a27e7c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id         title  sequel\n",
      "0  19995        Avatar    <NA>\n",
      "1    862     Toy Story     863\n",
      "2    863   Toy Story 2   10193\n",
      "3    597       Titanic    <NA>\n",
      "4  24428  The Avengers    <NA>\n",
      "   id_x                                            title_x  sequel_x   id_y  \\\n",
      "0   862                                          Toy Story       863    863   \n",
      "1   863                                        Toy Story 2     10193  10193   \n",
      "2   675          Harry Potter and the Order of the Phoenix       767    767   \n",
      "3   121              The Lord of the Rings: The Two Towers       122    122   \n",
      "4   120  The Lord of the Rings: The Fellowship of the Ring       121    121   \n",
      "\n",
      "                                         title_y  sequel_y  \n",
      "0                                    Toy Story 2     10193  \n",
      "1                                    Toy Story 3      <NA>  \n",
      "2         Harry Potter and the Half-Blood Prince      <NA>  \n",
      "3  The Lord of the Rings: The Return of the King      <NA>  \n",
      "4          The Lord of the Rings: The Two Towers       122  \n",
      "\n",
      "\n",
      "                                             title_x  \\\n",
      "0                                          Toy Story   \n",
      "1                                        Toy Story 2   \n",
      "2          Harry Potter and the Order of the Phoenix   \n",
      "3              The Lord of the Rings: The Two Towers   \n",
      "4  The Lord of the Rings: The Fellowship of the Ring   \n",
      "\n",
      "                                         title_y  \n",
      "0                                    Toy Story 2  \n",
      "1                                    Toy Story 3  \n",
      "2         Harry Potter and the Half-Blood Prince  \n",
      "3  The Lord of the Rings: The Return of the King  \n",
      "4          The Lord of the Rings: The Two Towers  \n"
     ]
    }
   ],
   "source": [
    "sequels = pd.read_pickle('sequels.p')\n",
    "print(sequels.head())\n",
    "\n",
    "\n",
    "original_sequels = sequels.merge(sequels, left_on='sequel', right_on='id')\n",
    "print(original_sequels.head())\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(original_sequels[['title_x', 'title_y']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21698880-6031-4bfd-8a74-f94e41b6407a",
   "metadata": {},
   "source": [
    "## Self join\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Merging a table to itself can be useful when you want to compare values in a column to other values in the same column. In this exercise, you will practice this by creating a table that for each movie will list the movie director and a member of the crew on one row. You have been given a table called 'crews', which has columns 'id', 'job', and 'name'. First, merge the table to itself using the movie ID. \n",
    "# This merge will give you a larger table where for each movie, every job is matched against each other. ????????????\n",
    "Then select only those rows with a director in the left table, and avoid having a row where the director's job is listed in both the left and right tables. This filtering will remove job combinations that aren't with the director.\n",
    "\n",
    "The crews table has been loaded for you.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "#    To a variable called 'crews_self_merged', merge the 'crews' table to itself on the 'id' column using an inner join, setting the suffixes to '_dir' and '_crew' for the left and right tables respectively.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2     Missing\n",
    "    Create a Boolean index, named 'boolean_filter', that selects rows from the left table with the job of 'Director' and avoids rows with the job of 'Director' in the right table.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3     Missing\n",
    "    Use the .head() method to print the first few rows of direct_crews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5e3c3-a7bb-4c2c-9a6a-3abfbe067606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we make it into a list of infos? pretty interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c67ff9d7-a311-4b43-be68-2d19b708c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  department             job               name\n",
      "0  19995     Editing          Editor  Stephen E. Rivkin\n",
      "2  19995       Sound  Sound Designer  Christopher Boyes\n",
      "4  19995  Production         Casting          Mali Finn\n",
      "6  19995   Directing        Director      James Cameron\n",
      "7  19995     Writing          Writer      James Cameron\n",
      "(42502, 4)\n",
      "4775\n",
      "102899    54\n",
      "8914      52\n",
      "19995     52\n",
      "686       52\n",
      "10733     52\n",
      "24        50\n",
      "10012     50\n",
      "Name: id, dtype: int64\n",
      "\n",
      "\n",
      "       id department_dir         job_dir           name_dir  \\\n",
      "50  19995        Editing          Editor  Stephen E. Rivkin   \n",
      "51  19995        Editing          Editor  Stephen E. Rivkin   \n",
      "52  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "53  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "54  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "55  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "56  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "57  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "58  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "59  19995          Sound  Sound Designer  Christopher Boyes   \n",
      "\n",
      "      department_crew                 job_crew          name_crew  \n",
      "50               Crew            CG Supervisor      Matthias Menz  \n",
      "51               Crew            CG Supervisor   Philippe Rebours  \n",
      "52            Editing                   Editor  Stephen E. Rivkin  \n",
      "53              Sound           Sound Designer  Christopher Boyes  \n",
      "54         Production                  Casting          Mali Finn  \n",
      "55          Directing                 Director      James Cameron  \n",
      "56            Writing                   Writer      James Cameron  \n",
      "57                Art             Set Designer    Richard F. Mays  \n",
      "58  Costume & Make-Up           Costume Design     Mayes C. Rubeo  \n",
      "59             Camera  Director of Photography        Mauro Fiore  \n",
      "         id department_dir        job_dir           name_dir  \\\n",
      "2700  19995           Crew  CG Supervisor   Philippe Rebours   \n",
      "2701  19995           Crew  CG Supervisor   Philippe Rebours   \n",
      "2702  19995           Crew  CG Supervisor   Philippe Rebours   \n",
      "2703  19995           Crew  CG Supervisor   Philippe Rebours   \n",
      "2704    285      Directing       Director     Gore Verbinski   \n",
      "2705    285      Directing       Director     Gore Verbinski   \n",
      "2706    285      Directing       Director     Gore Verbinski   \n",
      "2707    285      Directing       Director     Gore Verbinski   \n",
      "2708    285      Directing       Director     Gore Verbinski   \n",
      "2709    285      Directing       Director     Gore Verbinski   \n",
      "2710    285      Directing       Director     Gore Verbinski   \n",
      "2711    285      Directing       Director     Gore Verbinski   \n",
      "2712    285      Directing       Director     Gore Verbinski   \n",
      "2713    285      Directing       Director     Gore Verbinski   \n",
      "2714    285      Directing       Director     Gore Verbinski   \n",
      "2715    285      Directing       Director     Gore Verbinski   \n",
      "2716    285      Directing       Director     Gore Verbinski   \n",
      "2717    285     Production       Producer  Jerry Bruckheimer   \n",
      "2718    285     Production       Producer  Jerry Bruckheimer   \n",
      "2719    285     Production       Producer  Jerry Bruckheimer   \n",
      "\n",
      "        department_crew                     job_crew          name_crew  \n",
      "2700               Crew                CG Supervisor         Andy Lomas  \n",
      "2701               Crew                CG Supervisor   Sebastian Marino  \n",
      "2702               Crew                CG Supervisor      Matthias Menz  \n",
      "2703               Crew                CG Supervisor   Philippe Rebours  \n",
      "2704          Directing                     Director     Gore Verbinski  \n",
      "2705         Production                     Producer  Jerry Bruckheimer  \n",
      "2706            Editing                       Editor  Stephen E. Rivkin  \n",
      "2707              Sound      Original Music Composer        Hans Zimmer  \n",
      "2708         Production           Executive Producer       Mike Stenson  \n",
      "2709         Production                     Producer        Eric McLeod  \n",
      "2710         Production                      Casting     Denise Chamian  \n",
      "2711                Art            Production Design     Rick Heinrichs  \n",
      "2712         Production                      Casting     Priscilla John  \n",
      "2713  Costume & Make-Up               Costume Design           Liz Dann  \n",
      "2714  Costume & Make-Up       Makeup Department Head           Ve Neill  \n",
      "2715               Crew  Special Effects Coordinator         Allen Hall  \n",
      "2716              Sound                 Music Editor  Barbara McDermott  \n",
      "2717          Directing                     Director     Gore Verbinski  \n",
      "2718         Production                     Producer  Jerry Bruckheimer  \n",
      "2719            Editing                       Editor  Stephen E. Rivkin  \n",
      "(834194, 7)\n",
      "834194\n",
      "\n",
      "\n",
      "           id department_dir   job_dir       name_dir department_crew  \\\n",
      "156     19995      Directing  Director  James Cameron         Editing   \n",
      "157     19995      Directing  Director  James Cameron           Sound   \n",
      "158     19995      Directing  Director  James Cameron      Production   \n",
      "159     19995      Directing  Director  James Cameron       Directing   \n",
      "160     19995      Directing  Director  James Cameron         Writing   \n",
      "...       ...            ...       ...            ...             ...   \n",
      "834189  25975      Directing  Director       Jon Gunn       Directing   \n",
      "834190  25975      Directing  Director     Brett Winn      Production   \n",
      "834191  25975      Directing  Director     Brett Winn       Directing   \n",
      "834192  25975      Directing  Director     Brett Winn       Directing   \n",
      "834193  25975      Directing  Director     Brett Winn       Directing   \n",
      "\n",
      "                  job_crew          name_crew  \n",
      "156                 Editor  Stephen E. Rivkin  \n",
      "157         Sound Designer  Christopher Boyes  \n",
      "158                Casting          Mali Finn  \n",
      "159               Director      James Cameron  \n",
      "160                 Writer      James Cameron  \n",
      "...                    ...                ...  \n",
      "834189            Director         Brett Winn  \n",
      "834190  Executive Producer     Clark Peterson  \n",
      "834191            Director   Brian Herzlinger  \n",
      "834192            Director           Jon Gunn  \n",
      "834193            Director         Brett Winn  \n",
      "\n",
      "[47569 rows x 7 columns]\n",
      "\n",
      "\n",
      "            id department_dir   job_dir          name_dir department_crew  \\\n",
      "156      19995      Directing  Director     James Cameron         Editing   \n",
      "157      19995      Directing  Director     James Cameron           Sound   \n",
      "158      19995      Directing  Director     James Cameron      Production   \n",
      "160      19995      Directing  Director     James Cameron         Writing   \n",
      "161      19995      Directing  Director     James Cameron             Art   \n",
      "...        ...            ...       ...               ...             ...   \n",
      "834166   72766      Directing  Director      Edward Burns         Editing   \n",
      "834174  231617      Directing  Director       Scott Smith      Production   \n",
      "834182   25975      Directing  Director  Brian Herzlinger      Production   \n",
      "834186   25975      Directing  Director          Jon Gunn      Production   \n",
      "834190   25975      Directing  Director        Brett Winn      Production   \n",
      "\n",
      "                  job_crew          name_crew  \n",
      "156                 Editor  Stephen E. Rivkin  \n",
      "157         Sound Designer  Christopher Boyes  \n",
      "158                Casting          Mali Finn  \n",
      "160                 Writer      James Cameron  \n",
      "161           Set Designer    Richard F. Mays  \n",
      "...                    ...                ...  \n",
      "834166              Editor       Janet Gaynor  \n",
      "834174  Executive Producer        Scott Smith  \n",
      "834182  Executive Producer     Clark Peterson  \n",
      "834186  Executive Producer     Clark Peterson  \n",
      "834190  Executive Producer     Clark Peterson  \n",
      "\n",
      "[40845 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "crews = pd.read_pickle('crews.p')\n",
    "print(crews[crews['id']==19995].head())\n",
    "print(crews.shape)\n",
    "print(len(crews['id'].unique()))\n",
    "print(crews['id'].value_counts()[16:23])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, on='id', suffixes=('_dir', '_crew'), how='inner')\n",
    "print(crews_self_merged[50:60])\n",
    "print(crews_self_merged[2700:2720])\n",
    "print(crews_self_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outcome = []\n",
    "for i in crews['id'].value_counts():\n",
    "    outcome.append(i*i)\n",
    "print(np.sum(outcome))\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(crews_self_merged[crews_self_merged['job_dir']=='Director'])\n",
    "# ***************************************************************************************************************** #\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(crews_self_merged[(crews_self_merged['job_dir']=='Director') & (crews_self_merged['job_crew']!='Director')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54a5b1-ab55-4c53-b55c-2e65ef0c3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Boolean index to select the appropriate\n",
    "\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n",
    "     (crews_self_merged['job_crew'] != 'Director'))\n",
    "\n",
    "\n",
    "direct_crews = crews_self_merged[boolean_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d470d947-25a2-4d67-bcd6-bdd1b8167b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2704"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52*52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8101db-7d66-4bec-b49b-5ce86adf4d48",
   "metadata": {},
   "source": [
    "## How does pandas handle self joins?\n",
    "\n",
    "Select the false statement about merging a table to itself.\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    You can merge a table to itself with a right join.\n",
    "    1\n",
    "    Merging a table to itself can allow you to compare values in a column to other values in the same column.\n",
    "    2\n",
    "#    The Pandas module limits you to one merge where you merge a table to itself. You cannot repeat this process over and over.\n",
    "    3\n",
    "    Merging a table to itself is like working with two separate tables.\n",
    "    4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81d0e7-c43b-4a48-90be-3ac0b7e813a6",
   "metadata": {},
   "source": [
    "## Merging on indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**So far, we've only looked at merging two tables together using their columns.  In this lesson, we'll discuss how to merge tables using their indexes.  Often the DF indexes are given a unique id that we can use when merging two tables together.  Here, we show the movies table that was introduced earlier in this chapter.  The index is the default 0, 1, 2, ... etc.  auto-increment.  In the second version, the id column is the index for the table.  \n",
    "\n",
    "\n",
    "There are different methods to set the index of a table, but if our data start off in a .csv file, we can use the index_col= argument of the pd.read_csv() method.  Recall our example to merge the movies and taglines tables using the id column with a left join.  Lets recreate that merge using the index which is now the id for the tables.  \n",
    "# *******************************************************************************************************************\n",
    "Our merge statement looks identical to before.  However in this case we are inputting  to the on= argument the index level name which is called id.  The .merge() method automatically adjusts to accept index names  or column names.  The returned table looks as before, except the id is the index.  \n",
    "\n",
    "\n",
    "Lets try a multi-indexes merge.  Here we have two tables with a multi-index that holds the movie id and cast id.  The first table, named samuel, has the movie and cast id for a group of movies that Samuel L. Jackson acked in.  The second table, named cast, has the movie id and cast id for a number of movie characters.  Lets merge these two tables on their multi-indexes.  In this merge, we pass in a list of multi-index level names to the on= argument, just like we did when merging on multiple columns.  \n",
    "\n",
    "\n",
    "There is one more thing regarding merging on indexes.  If the index names are different between the two tables that we want to merge, then we can use the left_on= and right_on= arguments of the merge method.  ( actually we dont, just pass left_index=True and right_index=True would be fine )\n",
    "\n",
    "Here why I got duplicated indexes from both tables?   Interesting\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets go back to our movies table, shown in the top panel, and merge it with our movies_to_genres table shown in the lower panel.  In this merge, since we list the movies table as the left table we set left_on= arg equal to id and right_on= arg equal to movie_id.  Additionally, since we are merging on indexes, we need to set left_index= and right_index= to True.  \n",
    "\n",
    "( actually, since we are merging two tables with their indexes linked to each other, thus just set left_index=True and right_index=True would be fine, no need left_on= and right_on= args, even their index column names are differently )\n",
    "\n",
    "\n",
    "    moves_genres = movies.merge(movie_to_genres, left_index=True, right_index=True)\n",
    "\n",
    "--------------------------------------------------------------\n",
    "      id                 title  popularity release_date\n",
    "0    257          Oliver Twist   20.415572   2005-09-23\n",
    "1  14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
    "2  38365             Grown Ups   38.864027   2010-06-24\n",
    "3   9672              Infamous    3.680896   2006-11-16\n",
    "4  12819       Alpha and Omega   12.300789   2010-09-17\n",
    "\n",
    "\n",
    "-----------------------------------------------------------\n",
    "                      title  popularity release_date\n",
    "id                                                  \n",
    "257            Oliver Twist   20.415572   2005-09-23\n",
    "14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
    "38365             Grown Ups   38.864027   2010-06-24\n",
    "9672               Infamous    3.680896   2006-11-16\n",
    "12819       Alpha and Omega   12.300789   2010-09-17\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268c6908-a0d8-4dda-92f8-ffeea4d95944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                 title  popularity release_date\n",
      "0    257          Oliver Twist   20.415572   2005-09-23\n",
      "1  14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
      "2  38365             Grown Ups   38.864027   2010-06-24\n",
      "3   9672              Infamous    3.680896   2006-11-16\n",
      "4  12819       Alpha and Omega   12.300789   2010-09-17\n",
      "                      title  popularity release_date\n",
      "id                                                  \n",
      "257            Oliver Twist   20.415572   2005-09-23\n",
      "14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
      "38365             Grown Ups   38.864027   2010-06-24\n",
      "9672               Infamous    3.680896   2006-11-16\n",
      "12819       Alpha and Omega   12.300789   2010-09-17\n",
      "       id                                         tagline\n",
      "0   19995                     Enter the World of Pandora.\n",
      "1     285  At the end of the world, the adventure begins.\n",
      "2  206647                           A Plan No One Escapes\n",
      "3   49026                                 The Legend Ends\n",
      "4   49529            Lost in our world, found in another.\n",
      "                                               tagline\n",
      "id                                                    \n",
      "19995                      Enter the World of Pandora.\n",
      "285     At the end of the world, the adventure begins.\n",
      "206647                           A Plan No One Escapes\n",
      "49026                                  The Legend Ends\n",
      "49529             Lost in our world, found in another.\n"
     ]
    }
   ],
   "source": [
    "movies = pd.read_pickle('movies.p')\n",
    "print(movies.head())\n",
    "\n",
    "\n",
    "movies2 = movies.set_index('id')\n",
    "print(movies2.head())\n",
    "#print(help(pd.read_pickle))\n",
    "\n",
    "\n",
    "taglines = pd.read_pickle('taglines.p')\n",
    "print(taglines.head())\n",
    "\n",
    "taglines2 = taglines.set_index('id')\n",
    "print(taglines2.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95585f4e-5edf-4389-a1fe-a6697f277200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 character  gender    id               name\n",
      "movie_id cast_id                                           \n",
      "5        22        Jezebel       1  3122        Sammi Davis\n",
      "         23          Diana       1  3123  Amanda de Cadenet\n",
      "         24         Athena       1  3124     Valeria Golino\n",
      "         25        Elspeth       1  3125            Madonna\n",
      "         26            Eva       1  3126          Ione Skye\n",
      "Himself                      909\n",
      "                             683\n",
      "Dancer                       375\n",
      "Additional Voices (voice)    323\n",
      "Herself                      229\n",
      "Reporter                     136\n",
      "Doctor                       132\n",
      "Nurse                        117\n",
      "Waitress                     108\n",
      "Bartender                     93\n",
      "Name: character, dtype: int64\n",
      "                 character  gender      id               name\n",
      "movie_id cast_id                                             \n",
      "16       22         Doctor       2    1640  Stellan Skarsgård\n",
      "55       35         Doctor       0   23875  Patricio Castillo\n",
      "77       12         Doctor       2     539      Thomas Lennon\n",
      "115      74         Doctor       2  116907    Marshall Manesh\n",
      "153      33         Doctor       0  238826    Osamu Shigematu\n",
      "                 character_x  gender_x    id_x             name_x character_y  \\\n",
      "movie_id cast_id                                                                \n",
      "16       22           Doctor         2    1640  Stellan Skarsgård      Doctor   \n",
      "55       35           Doctor         0   23875  Patricio Castillo      Doctor   \n",
      "77       12           Doctor         2     539      Thomas Lennon      Doctor   \n",
      "115      74           Doctor         2  116907    Marshall Manesh      Doctor   \n",
      "153      33           Doctor         0  238826    Osamu Shigematu      Doctor   \n",
      "\n",
      "                  gender_y    id_y             name_y  \n",
      "movie_id cast_id                                       \n",
      "16       22              2    1640  Stellan Skarsgård  \n",
      "55       35              0   23875  Patricio Castillo  \n",
      "77       12              2     539      Thomas Lennon  \n",
      "115      74              2  116907    Marshall Manesh  \n",
      "153      33              0  238826    Osamu Shigematu  \n",
      "                 character_x  gender_x    id_x             name_x character_y  \\\n",
      "movie_id cast_id                                                                \n",
      "16       22           Doctor         2    1640  Stellan Skarsgård      Doctor   \n",
      "55       35           Doctor         0   23875  Patricio Castillo      Doctor   \n",
      "77       12           Doctor         2     539      Thomas Lennon      Doctor   \n",
      "115      74           Doctor         2  116907    Marshall Manesh      Doctor   \n",
      "153      33           Doctor         0  238826    Osamu Shigematu      Doctor   \n",
      "\n",
      "                  gender_y    id_y             name_y  \n",
      "movie_id cast_id                                       \n",
      "16       22              2    1640  Stellan Skarsgård  \n",
      "55       35              0   23875  Patricio Castillo  \n",
      "77       12              2     539      Thomas Lennon  \n",
      "115      74              2  116907    Marshall Manesh  \n",
      "153      33              0  238826    Osamu Shigematu  \n",
      "         title  popularity release_date            genre\n",
      "5   Four Rooms   22.876230   1995-12-09            Crime\n",
      "5   Four Rooms   22.876230   1995-12-09           Comedy\n",
      "11   Star Wars  126.393695   1977-05-25  Science Fiction\n",
      "11   Star Wars  126.393695   1977-05-25           Action\n",
      "11   Star Wars  126.393695   1977-05-25        Adventure\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "casts2 = pd.read_pickle('casts.p').set_index(['movie_id', 'cast_id'])\n",
    "print(casts.head())\n",
    "\n",
    "\n",
    "print(casts2['character'].value_counts().head(10))\n",
    "\n",
    "\n",
    "samuel = casts2[casts2['character']=='Doctor']\n",
    "print(samuel.head())\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "samuel_cast = samuel.merge(casts2, on=['movie_id', 'cast_id'])\n",
    "print(samuel_cast.head())\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "samuel_cast2 = samuel.merge(casts2, left_index=True, right_index=True)\n",
    "print(samuel_cast2.head())\n",
    "\n",
    "\n",
    "\n",
    "movies = pd.read_pickle('movies.p').set_index('id')\n",
    "movie_to_genres = pd.read_pickle('movie_to_genres.p').set_index('movie_id')\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "moves_genres = movies.merge(movie_to_genres, left_index=True, right_index=True)\n",
    "\n",
    "print(moves_genres.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1216db4-2919-4981-b3e2-2c2514e4e1e3",
   "metadata": {},
   "source": [
    "## Index merge for movie ratings\n",
    "\n",
    "To practice merging on indexes, you will merge movies and a table called ratings that holds info about movie ratings. Make sure your merge returns all of the rows from the movies table and not all the rows of ratings table need to be included in the result.\n",
    "\n",
    "The movies and ratings tables have been loaded for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Merge movies and ratings on the index and save to a variable called movies_ratings, ensuring that all of the rows from the movies table are returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "58c35c72-b340-43cd-8328-7c9c9dd578ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        vote_average  vote_count\n",
      "id                              \n",
      "19995            7.2     11800.0\n",
      "285              6.9      4500.0\n",
      "206647           6.3      4466.0\n",
      "49026            7.6      9106.0\n",
      "49529            6.1      2124.0\n",
      "                      title  popularity release_date\n",
      "id                                                  \n",
      "257            Oliver Twist   20.415572   2005-09-23\n",
      "14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
      "38365             Grown Ups   38.864027   2010-06-24\n",
      "9672               Infamous    3.680896   2006-11-16\n",
      "12819       Alpha and Omega   12.300789   2010-09-17\n",
      "                      title  popularity release_date  vote_average  vote_count\n",
      "id                                                                            \n",
      "257            Oliver Twist   20.415572   2005-09-23           6.7       274.0\n",
      "14290  Better Luck Tomorrow    3.877036   2002-01-12           6.5        27.0\n",
      "38365             Grown Ups   38.864027   2010-06-24           6.0      1705.0\n",
      "9672               Infamous    3.680896   2006-11-16           6.4        60.0\n",
      "12819       Alpha and Omega   12.300789   2010-09-17           5.3       124.0\n",
      "                      title  popularity release_date  vote_average  vote_count\n",
      "id                                                                            \n",
      "257            Oliver Twist   20.415572   2005-09-23           6.7       274.0\n",
      "14290  Better Luck Tomorrow    3.877036   2002-01-12           6.5        27.0\n",
      "38365             Grown Ups   38.864027   2010-06-24           6.0      1705.0\n",
      "9672               Infamous    3.680896   2006-11-16           6.4        60.0\n",
      "12819       Alpha and Omega   12.300789   2010-09-17           5.3       124.0\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_pickle('ratings.p').set_index('id')\n",
    "print(ratings.head())\n",
    "\n",
    "\n",
    "movies = pd.read_pickle('movies.p').set_index('id')\n",
    "print(movies.head())\n",
    "\n",
    "\n",
    "movies_ratings = movies.merge(ratings, left_index=True, right_index=True)\n",
    "print(movies_ratings.head())\n",
    "\n",
    "\n",
    "\n",
    "movies_ratings2 = movies.merge(ratings, on='id')\n",
    "print(movies_ratings2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7eeb4a-2223-41be-98d7-e02fb04f7bbf",
   "metadata": {},
   "source": [
    "## Do sequels earn more?\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "It is time to put together many of the aspects that you have learned in this chapter. \n",
    "\n",
    "# In this exercise, you'll find out which movie sequels earned the most compared to the original movie. \n",
    "\n",
    "To answer this question, you will merge a modified version of the sequels and financials tables where their index is the movie ID. You will need to choose a merge type that will return all of the rows from the sequels table and not all the rows of financials table need to be included in the result. \n",
    "\n",
    "From there, you will join the resulting table to itself so that you can compare the revenue values of the original movie to the sequel. \n",
    "\n",
    "Next, you will calculate the difference between the two revenues and sort the resulting dataset.  ( Jurassic Park III sequel Jurassic Park World )\n",
    "\n",
    "The sequels and financials tables have been provided.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    With the 'sequels' table on the left, merge to it the 'financials' table on index named id, ensuring that all the rows from the sequels are returned and some rows from the other table may not be returned, Save the results to 'sequels_fin'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2     Missing\n",
    "    Merge the 'sequels_fin' table to itself with an 'inner' join, where the left and right tables merge on sequel and id respectively with suffixes equal to ('_org','_seq'), saving to 'orig_seq'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3     Missing\n",
    "    3. Select the 'title_org', 'title_seq', and 'diff' columns of 'orig_seq' and save this as 'titles_diff'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4     Missing\n",
    "    4. Sort by 'titles_diff' by 'diff' in descending order and print the first few rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbee7fdb-4ccf-482a-abf8-7bed2c1b50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           budget       revenue\n",
      "id                             \n",
      "19995   237000000  2.787965e+09\n",
      "285     300000000  9.610000e+08\n",
      "206647  245000000  8.806746e+08\n",
      "49026   250000000  1.084939e+09\n",
      "49529   260000000  2.841391e+08\n",
      "              title  sequel\n",
      "id                         \n",
      "19995        Avatar    <NA>\n",
      "862       Toy Story     863\n",
      "863     Toy Story 2   10193\n",
      "597         Titanic    <NA>\n",
      "24428  The Avengers    <NA>\n",
      "              title  sequel       budget       revenue\n",
      "id                                                    \n",
      "19995        Avatar    <NA>  237000000.0  2.787965e+09\n",
      "862       Toy Story     863   30000000.0  3.735540e+08\n",
      "863     Toy Story 2   10193   90000000.0  4.973669e+08\n",
      "597         Titanic    <NA>  200000000.0  1.845034e+09\n",
      "24428  The Avengers    <NA>  220000000.0  1.519558e+09\n",
      "\n",
      "\n",
      "    sequel                                            title_x sequel_x  \\\n",
      "id                                                                       \n",
      "862    863                                          Toy Story      863   \n",
      "863  10193                                        Toy Story 2    10193   \n",
      "675    767          Harry Potter and the Order of the Phoenix      767   \n",
      "121    122              The Lord of the Rings: The Two Towers      122   \n",
      "120    121  The Lord of the Rings: The Fellowship of the Ring      121   \n",
      "\n",
      "        budget_x    revenue_x                                        title_y  \\\n",
      "id                                                                             \n",
      "862   30000000.0  373554033.0                                    Toy Story 2   \n",
      "863   90000000.0  497366869.0                                    Toy Story 3   \n",
      "675  150000000.0  938212738.0         Harry Potter and the Half-Blood Prince   \n",
      "121   79000000.0  926287400.0  The Lord of the Rings: The Return of the King   \n",
      "120   93000000.0  871368364.0          The Lord of the Rings: The Two Towers   \n",
      "\n",
      "    sequel_y     budget_y     revenue_y  \n",
      "id                                       \n",
      "862    10193   90000000.0  4.973669e+08  \n",
      "863     <NA>  200000000.0  1.066970e+09  \n",
      "675     <NA>  250000000.0  9.339592e+08  \n",
      "121     <NA>   94000000.0  1.118889e+09  \n",
      "120      122   79000000.0  9.262874e+08  \n",
      "sequel                  135397\n",
      "title_x      Jurassic Park III\n",
      "sequel_x                135397\n",
      "budget_x            93000000.0\n",
      "revenue_x          368780809.0\n",
      "title_y         Jurassic World\n",
      "sequel_y                  <NA>\n",
      "budget_y           150000000.0\n",
      "revenue_y         1513528810.0\n",
      "Name: 331, dtype: object\n",
      "\n",
      "\n",
      "       sequel                     title_x sequel_x     budget_x    revenue_x  \\\n",
      "id                                                                             \n",
      "331    135397           Jurassic Park III   135397   93000000.0  368780809.0   \n",
      "272       155               Batman Begins      155  150000000.0  374218673.0   \n",
      "10138   68721                  Iron Man 2    68721  200000000.0  623933331.0   \n",
      "863     10193                 Toy Story 2    10193   90000000.0  497366869.0   \n",
      "10764   37724           Quantum of Solace    37724  200000000.0  586090727.0   \n",
      "...       ...                         ...      ...          ...          ...   \n",
      "49018   91586                   Insidious    91586    1500000.0   97009150.0   \n",
      "16781   15670      Madea's Family Reunion    15670    6000000.0   57231524.0   \n",
      "2662     1696       House of 1000 Corpses     1696    7000000.0   16829545.0   \n",
      "765       766                Evil Dead II      766    3600000.0    5923044.0   \n",
      "16186   16781  Diary of a Mad Black Woman    16781          NaN          NaN   \n",
      "\n",
      "                      title_y sequel_y     budget_y     revenue_y  \\\n",
      "id                                                                  \n",
      "331            Jurassic World     <NA>  150000000.0  1.513529e+09   \n",
      "272           The Dark Knight     <NA>  185000000.0  1.004558e+09   \n",
      "10138              Iron Man 3     <NA>  200000000.0  1.215440e+09   \n",
      "863               Toy Story 3     <NA>  200000000.0  1.066970e+09   \n",
      "10764                 Skyfall     <NA>  200000000.0  1.108561e+09   \n",
      "...                       ...      ...          ...           ...   \n",
      "49018    Insidious: Chapter 2     <NA>          NaN           NaN   \n",
      "16781      Madea Goes to Jail     <NA>          NaN           NaN   \n",
      "2662      The Devil's Rejects     <NA>          NaN           NaN   \n",
      "765          Army of Darkness     <NA>          NaN           NaN   \n",
      "16186  Madea's Family Reunion    15670    6000000.0  5.723152e+07   \n",
      "\n",
      "              gains  \n",
      "id                   \n",
      "331    1.144748e+09  \n",
      "272    6.303398e+08  \n",
      "10138  5.915067e+08  \n",
      "863    5.696028e+08  \n",
      "10764  5.224703e+08  \n",
      "...             ...  \n",
      "49018           NaN  \n",
      "16781           NaN  \n",
      "2662            NaN  \n",
      "765             NaN  \n",
      "16186           NaN  \n",
      "\n",
      "[90 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "financials = pd.read_pickle('financials.p').set_index('id')\n",
    "print(financials.head())\n",
    "\n",
    "sequels = pd.read_pickle('sequels.p').set_index('id')\n",
    "print(sequels.head())\n",
    "\n",
    "\n",
    "sequels_financials = sequels.merge(financials, how='left', left_index=True, right_index=True)\n",
    "print(sequels_financials.head())\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# From there, you will join the resulting table to itself so that you can compare the revenue values\n",
    "# of the original movie to the sequel.\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "\n",
    "sequels_financials_self_merge = sequels_financials.merge(sequels_financials, left_on='sequel', right_index=True)\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "\n",
    "\n",
    "print(sequels_financials_self_merge.head())\n",
    "print(sequels_financials_self_merge.loc[331])\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Next, you will calculate the difference between the two revenues and sort the resulting dataset.\n",
    "\n",
    "sequels_financials_self_merge['gains'] = sequels_financials_self_merge['revenue_y'] - sequels_financials_self_merge['revenue_x']\n",
    "print(sequels_financials_self_merge.sort_values('gains', ascending=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7fc57-bead-45a1-b977-874f4fa8d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, \n",
    "                             how='inner', \n",
    "                             left_on='sequel',\n",
    "                             right_on='id', \n",
    "                             right_index=True,\n",
    "                             suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq\n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Select the title_org, title_seq, and diff\n",
    "titles_diff = orig_seq[['title_org', 'title_seq', 'diff']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the first rows of the sorted titles_diff\n",
    "print(titles_diff.sort_values('diff', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422f9ca-3061-45f9-811a-bab3274aadf0",
   "metadata": {},
   "source": [
    "## Filtering joins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**Welcome the the third chapter.  In this lesson, we'll discuss a type of join called filtering join.  The Pandas doesnt direct support for filtering joins, but we will learn how to replicate them.  \n",
    "\n",
    "\n",
    "# So far, we have only worked with mutating joins, which combines data from two tables.  However, filtering joins filter observations from one table based on wheather or not they match an observation in another table.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets start with a semi-join.  A semi-join filters the left table down to those observations that have a match in the right table.  \n",
    "It is similar to an inner join where only the intersection between the tables in returned, but unlike an inner join, but unlike an inner join, only the columns from the left table are shown.  Finally, no duplicate rows from the left table are returned, even if there is a one-to-many relationship.  \n",
    "\n",
    "Lets look at an example, For this chapter, the dataset we will use is from an online music streaming service.  In this new dataset, we have a table of song genres shown here.  There is also a table of top-rated song tracks.  The gid column connects the two tables.  \n",
    "\n",
    "# Lets say we want to find what genres appear in our table of top songs.  A semi-join would return only the columns from the genre table and not the tracks.  \n",
    "\n",
    "First, lets merge two tables with an inner join.  For the next step in the technique, lets uses a method called .isin(), which compares every gid in the genres table to the gid in the genres_tracks table.  This will tell us if our genre appear in our merged genres_tracks table.  This line of code returns a boolean series of True or False values.  To combine everything, we use that line of code to subset the genres table.  \n",
    "\n",
    "This is called a filtering join because we've filtered the genres table by whats in the top_tracks table.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Now lets talk about anti-joins.  An anti-join returns the observations in the left table that do not have a matching observation in the right table.  It also only returns the columns from the left table.  \n",
    "\n",
    "Going back to our example, instead of finding which genres are in the table of top_tracks, lets now find which genres are not with an anti-join.  \n",
    "\n",
    "First step is to use a left join returning all of the rows from the left table.  Then in the merging process, we use a indicator= argument and set it equal to True.  With indicator= set to True, the .merge() method then adds a column called _merge to the output.  This column tells the source of each row.  Say first 4 rows found a match in both tables, whereas the last can only be found in the left table.  \n",
    "Next, we use the .loc[] accessor and '_merge' column to select the rows that only appear in the left table and return only the gid column from the genres_tracks table.  We now have a list of gids not in the tracks table.  \n",
    "In our final step we use the .isin() method to filter for the rows with gids in our gid_list.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------\n",
    "   gid   | name\n",
    "0  1     | Rock\n",
    "1  2     | Jazz\n",
    "2  3     | Metal\n",
    "3  4     | Alternative ...\n",
    "4  5     | Rock and Roll\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------\n",
    "   tid   | name            | aid   | mtid  | gid   | composer         | u+price\n",
    "   1     | For Those Ab... | 1     | 1     | 1     | Angus Young, ... | 0.99\n",
    "   2     | Balls to the... | 2     | 2     | 1     | nan              | 0.99\n",
    "   3     | Fast As a Shark | 3     | 2     | 1     | F. Baltes, S...  | 0.99\n",
    "   4     | Restless and... | 3     | 2     | 1     | F. Baltes, S...  | 0.99\n",
    "   5     | Princess of ... | 3     | 2     | 1     | Deaffy & R.A...  | 0.99\n",
    "\n",
    "\n",
    "\n",
    "genres_tracks = genres.merge(top_tracks, on='gid')\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "genres['gid'].isin(genres_tracks['gid'])\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "genres[genres['gid'].isin(genres_tracks['gid'])]\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "genres_tracks = genres.merge(top_tracks, on='gid', how='left', indicator=True)\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "gid_list = genres_tracks.loc[genres_tracks['merge']=='left_only', 'gid']\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "no_top_genres = genres[genres['gid'].isin(gid_list)]\n",
    "\n",
    "\n",
    "\n",
    "# Think how those process linked and take us to what we want\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4991bab5-ee4c-4412-948e-f9e4fcaa6e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        title  popularity release_date  \\\n",
      "10253                      Dragon Wars: D-War    4.822033   2007-02-08   \n",
      "131634  The Hunger Games: Mockingjay - Part 2  127.284427   2015-11-18   \n",
      "120467               The Grand Budapest Hotel   74.417456   2014-02-26   \n",
      "408           Snow White and the Seven Dwarfs   80.171283   1937-12-20   \n",
      "3170                                    Bambi   47.651878   1942-08-14   \n",
      "11688                The Emperor's New Groove   51.113717   2000-12-09   \n",
      "10368                      Tea with Mussolini    2.238646   1999-03-25   \n",
      "\n",
      "          genre _merge  \n",
      "10253    Horror   both  \n",
      "131634   Action   both  \n",
      "120467    Drama   both  \n",
      "408     Fantasy   both  \n",
      "3170     Family   both  \n",
      "11688   Fantasy   both  \n",
      "10368    Comedy   both  \n",
      "both          12160\n",
      "left_only         0\n",
      "right_only        0\n",
      "Name: _merge, dtype: int64\n",
      "['Four Rooms' 'Star Wars' 'Finding Nemo' ... '8 Days' 'Running Forever'\n",
      " 'To Be Frank, Sinatra at 100']\n",
      "4772\n",
      "                              title  popularity release_date            genre  \\\n",
      "5                        Four Rooms   22.876230   1995-12-09            Crime   \n",
      "5                        Four Rooms   22.876230   1995-12-09           Comedy   \n",
      "11                        Star Wars  126.393695   1977-05-25  Science Fiction   \n",
      "11                        Star Wars  126.393695   1977-05-25           Action   \n",
      "11                        Star Wars  126.393695   1977-05-25        Adventure   \n",
      "...                             ...         ...          ...              ...   \n",
      "426469             Growing Up Smith    0.710870   2017-02-03            Drama   \n",
      "433715                       8 Days    0.015295   2014-06-15         Thriller   \n",
      "433715                       8 Days    0.015295   2014-06-15            Drama   \n",
      "447027              Running Forever    0.028756   2015-10-27           Family   \n",
      "459488  To Be Frank, Sinatra at 100    0.050625   2015-12-12      Documentary   \n",
      "\n",
      "       _merge  \n",
      "5        both  \n",
      "5        both  \n",
      "11       both  \n",
      "11       both  \n",
      "11       both  \n",
      "...       ...  \n",
      "426469   both  \n",
      "433715   both  \n",
      "433715   both  \n",
      "447027   both  \n",
      "459488   both  \n",
      "\n",
      "[12160 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "movies = pd.read_pickle('movies.p').set_index('id')\n",
    "\n",
    "movie_to_genres = pd.read_pickle('movie_to_genres.p').set_index('movie_id')\n",
    "\n",
    "\n",
    "movies_genres = movies.merge(movie_to_genres, how='right', \n",
    "                             left_index=True, \n",
    "                             right_index=True, \n",
    "                             indicator=True)   ######################################################################\n",
    "\n",
    "print(movies_genres.sample(7))\n",
    "\n",
    "print(movies_genres['_merge'].value_counts())\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "filt_list = movies_genres.loc[movies_genres['_merge']=='both', 'title'].unique()\n",
    "print(filt_list)\n",
    "\n",
    "print(len(filt_list))\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "outcome = movies_genres[movies_genres['title'].isin(filt_list)]    # What if we have to use index instead of column\n",
    "\n",
    "\n",
    "print(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33062af0-9ac9-4ef2-a302-46a890e47d34",
   "metadata": {},
   "source": [
    "## Steps of a semi-join\n",
    "\n",
    "In the last video, you were shown how to perform a semi-join with pandas. In this exercise, you'll solidify your understanding of the necessary steps. Recall that a semi-join filters the left table to only the rows where a match exists in both the left and right tables.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Sort the steps in the correct order of the technique shown to perform a semi-join in pandas.\n",
    "\n",
    "3, Subset the rows of the left table\n",
    "1, Merge the left and right tables on key column using annner_join\n",
    "2, Search if key column in the left table is in the merged tables using the .isin() method creating a boolean series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40114030-9bb8-43f1-b416-931cb530e2f4",
   "metadata": {},
   "source": [
    "## Performing an anti-join\n",
    "\n",
    "In our music streaming company dataset, each customer is assigned an employee representative to assist them. \n",
    "# In this exercise, filter the employee table by a table of top customers, returning only those employees who are not assigned to a customer. \n",
    "The results should resemble the results of an anti-join. The company's leadership will assign these employees additional training so that they can work with high valued customers.\n",
    "\n",
    "The top_cust and employees tables have been provided for you.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Merge 'employees' and 'top_cust' with a left join, setting indicator= argument to True. Save the result to 'empl_cust'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2     Missing\n",
    "    Select the 'srid' column of 'empl_cust' and the rows where _merge is 'left_only'. Save the result to 'srid_list'.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3     Missing\n",
    "    Subset the 'employees' table and select those rows where the 'srid' is in the variable 'srid_list' and print the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ffbaf-2571-4fdb-be59-f4cbcabdc75e",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddab7b1-b106-4410-8220-0c37769f4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]:\n",
    "employees.head()\n",
    "Out[1]:\n",
    "\n",
    "   srid    lname     fname                title  hire_date                     email\n",
    "0     1    Adams    Andrew      General Manager 2002-08-14    andrew@chinookcorp.com\n",
    "1     2  Edwards     Nancy        Sales Manager 2002-05-01     nancy@chinookcorp.com\n",
    "2     3  Peacock      Jane  Sales Support Agent 2002-04-01      jane@chinookcorp.com\n",
    "3     4     Park  Margaret  Sales Support Agent 2003-05-03  margaret@chinookcorp.com\n",
    "4     5  Johnson     Steve  Sales Support Agent 2003-10-17     steve@chinookcorp.com\n",
    "In [2]:\n",
    "top_cust.head()\n",
    "Out[2]:\n",
    "\n",
    "   cid  srid      fname        lname               phone                 fax                     email\n",
    "0    1     3       Luís    Gonçalves  +55 (12) 3923-5555  +55 (12) 3923-5566      luisg@embraer.com.br\n",
    "1    2     5     Leonie       Köhler    +49 0711 2842222                 NaN     leonekohler@surfeu.de\n",
    "2    3     3   François     Tremblay   +1 (514) 721-4711                 NaN       ftremblay@gmail.com\n",
    "3    4     4      Bjørn       Hansen     +47 22 44 22 22                 NaN     bjorn.hansen@yahoo.no\n",
    "4    5     4  František  Wichterlová    +420 2 4172 5555    +420 2 4172 5555  frantisekw@jetbrains.com\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, \n",
    "                            on='srid', \n",
    "                            how='left', \n",
    "                            indicator=True)\n",
    "\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "srid_list = empl_cust.loc[empl_cust['_merge']=='left_only', 'srid']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(employees[employees['srid'].isin(srid_list)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad752a-5939-4b67-89eb-542207570ea1",
   "metadata": {},
   "source": [
    "## Performing a semi-join\n",
    "\n",
    "# Some of the tracks that have generated the most significant amount of revenue are from TV-shows or are other non-musical audio. \n",
    "You have been given a table of invoices that include top revenue-generating items. Additionally, you have a table of non-musical tracks from the streaming service. In this exercise, you'll use a semi-join to find the top revenue-generating non-musical tracks.  The tables 'non_mus_tcks', 'top_invoices', and 'genres' have been loaded for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Merge 'non_mus_tcks' and 'top_invoices' on 'tid' using an inner join. Save the result as 'tracks_invoices'.\n",
    "    Use .isin() to subset the rows of 'non_mus_tck' where 'tid' is in the 'tid' column of 'tracks_invoices'. Save the result as 'top_tracks'.\n",
    "#    Group top_tracks by 'gid' and count the 'tid' rows. Save the result to 'cnt_by_gid'.\n",
    "    Merge 'cnt_by_gid' with the 'genres' table on 'gid' and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aabe84-756d-41cf-826f-174a04385ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_invoices = non_mus_tcks.merge(top_invoices, on='tid')\n",
    "\n",
    "\n",
    "top_tracks = non_mus_tck[non_mus_tck['tid'].isin(tracks_invoices['tid'])]\n",
    "\n",
    "\n",
    "cnt_by_gid = top_tracks.groupby('gid')['tid'].count()\n",
    "\n",
    "\n",
    "print(cnt_by_gid.merge(genres, on='gid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193267ff-47c7-4479-acc3-26b173e73aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_invoices.head()\n",
    "Out[1]:\n",
    "\n",
    "    tid       name  aid  mtid  gid  u_price  ilid  iid  uprice  quantity\n",
    "0  2850    The Fix  228     3   21     1.99   473   88    1.99         1\n",
    "1  2850    The Fix  228     3   21     1.99  2192  404    1.99         1\n",
    "2  2868  Walkabout  230     3   19     1.99   476   88    1.99         1\n",
    "3  2868  Walkabout  230     3   19     1.99  2194  404    1.99         1\n",
    "4  3177   Hot Girl  249     3   19     1.99  1668  306    1.99         1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge the non_mus_tck and top_invoices tables on tid\n",
    "tracks_invoices = non_mus_tck.merge(____)\n",
    "\n",
    "# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\n",
    "top_tracks = _____[non_mus_tcks['tid'].isin(____)]\n",
    "\n",
    "# Group the top_tracks by gid and count the tid rows\n",
    "cnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':____})\n",
    "\n",
    "# Merge the genres table to cnt_by_gid on gid and print\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b339678-abc9-49df-aed3-6eb33902abef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Concatenate DataFrames together vertically\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**In this lesson, we'll talk about how to connect two tables verticaly.  So far in this course, we have only discussed how to merge two tables, which mainly grows them horizontally.  But what if we wanted to grow them vertically?  \n",
    "\n",
    "# We can use the .concat() method ( function ) to concatenate, or stick tables together vertically or horizontally.  \n",
    "In this lesson, we'll focus on vertical concatenation.  \n",
    "\n",
    "# Often the data for different periods of time will come in multiple tables.  \n",
    "But if we want to analyze it together, we'll need to combine them into one.  Here are three separate tables of invoice data from our streaming service.  Notice the column headers are the same.  The separate tables are named 'inv_jan', 'inv_feb', 'inv_mar' etc.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "We can pass a list of table names into Pandas' pd.concat() method ( function actually ) to combine the tables in the order they are passed in.  To concatenate vertically, the axis= argument should be set to 0, and its default.  The result is a vertically combined table.  Notice each table's index value was retained.  If the index contains no valuable information, then we can ignore it in the .concat() method by setting ignore_index=True argument.  So the result is that the index will go from 0 to n-1.  ( with no duplicated 0, 1, 2, 3 ... index from both tables. )\n",
    "\n",
    "    pd.concat([inv_jan, inv_feb, inv_mar, inv_apr], ignore_index=True)   #axis=0\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Now, suppose we wanted to associate specific keys with each of the pieces of our three original tables.  We can provide a list of labels to the keys= argument.  Make sure that ingore_index=False, since you cant add a key and ignore that index as the same time.  This results in a table with a multi-index, with the label on the first level.  \n",
    "\n",
    "    pd.concat([inv_jan, inv_feb, inv_mar, inv_apr], ignore_index=False, keys=['jan', 'feb', 'mar', 'apr'])\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# What if we need to combine tables that have different column names?  \n",
    "The inv_feb table now has a column added for billing country.  The concat method by default will include all of the columns in the different tablesits combining.  The sort= argument, if true, will alphabetically sort the different column names in the result.  We can see in the result that the billing country for jan invoice is NaN.  However there are values for the feb invoice.  \n",
    "\n",
    "# If we only want the matching columns between tables, we can set join= argument to join='inner'.  While the default value is join='outer'.  Additionally, the sort= argument has no effect when join='inner'.  \n",
    "\n",
    "    pd.concat([inv_jan, inv_feb], sort=True, join='inner')  #axis=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "# Lets  briefly talk about .append().  The .ppend() is a simplified .concat() method.  It supports the ignore_index= and sort= arguments.  However it doesnt support keys= or join= args, join is always set to outer.  Lets combine these tables with the .append() method.  \n",
    "\n",
    "The .append() is a DF method therefore, we list the 'inv_jan' table first to call the method.  We add the other tables as a list, and set the ignore_index= and sort= arguments similar to the .concat() method.  In our output, we see null values for the billing country, except for inv_feb table.  cause .append() function performs outer join.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.concat([inv_jan, inv_feb, inv_mar, inv_apr], ignore_index=True)   #axis=0\n",
    "\n",
    "\n",
    "-----------------------------------------------------\n",
    "   iid   | cid   | invoice_date    | total\n",
    "0  1     | 2     | 2009-01-01      | 1.98\n",
    "1  2     | 4     | 2009-01-02      | 3.96\n",
    "2  3     | 8     | 2009-01-03      | 5.94\n",
    "3  4     | 38    | 2009-02-01      | 1.98\n",
    "4  5     | 40    | 2009-02-01      | 1.98\n",
    "5  6     | 42    | 2009-02-02      | 3.96\n",
    "6  7     | 17    | 2009-03-04      | 1.98\n",
    "7  8     | 19    | 2009-03-04      | 1.98\n",
    "8  9     | 21    | 2009-03-05      | 3.96\n",
    "   \n",
    "   \n",
    "   \n",
    "pd.concat([inv_jan, inv_feb, inv_mar, inv_apr], axis=0, ingore_index=False, keys=['jan', 'feb', 'mar', 'apr'])\n",
    "   \n",
    "-----------------------------------------------------\n",
    "       iid   | cid   | invoice_date    | total\n",
    "jan 0  1     | 2     | 2009-01-01      | 1.98\n",
    "    1  2     | 4     | 2009-01-02      | 3.96\n",
    "    2  3     | 8     | 2009-01-03      | 5.94\n",
    "feb 0  4     | 38    | 2009-02-01      | 1.98\n",
    "    1  5     | 40    | 2009-02-01      | 1.98\n",
    "    2  6     | 42    | 2009-02-02      | 3.96\n",
    "mar 0  7     | 17    | 2009-03-04      | 1.98\n",
    "    1  8     | 19    | 2009-03-04      | 1.98\n",
    "    2  9     | 21    | 2009-03-05      | 3.96\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------  inv_jan table\n",
    "   iid   | cid   | invoice_date    | total\n",
    "0  1     | 2     | 2009-01-01      | 1.98\n",
    "1  2     | 4     | 2009-01-02      | 3.96\n",
    "2  3     | 8     | 2009-01-03      | 5.94\n",
    "\n",
    "\n",
    "------------------------------------------------------------   inv_feb table\n",
    "   iid   | cid   | invoice_date    | total   | bill_ctry\n",
    "0  1     | 2     | 2009-01-01      | 1.98    | Germany\n",
    "1  2     | 4     | 2009-01-02      | 3.96    | France\n",
    "2  3     | 8     | 2009-01-03      | 5.94    | France\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.concat([inv_jan, inv_feb], sort=True)  #axis=0\n",
    "\n",
    "-----------------------------------------------------------\n",
    "  bill_ctry  | cid   | iid   | invoice_date    | total \n",
    "0 NaN        | 2     | 1     | 2009-01-01      | 1.98  \n",
    "1 NaN        | 4     | 2     | 2009-01-02      | 3.96  \n",
    "2 NaN        | 8     | 3     | 2009-01-03      | 5.94  \n",
    "0 Germany    | 2     | 1     | 2009-01-01      | 1.98  \n",
    "1 France     | 4     | 2     | 2009-01-02      | 3.96  \n",
    "2 France     | 8     | 3     | 2009-01-03      | 5.94  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.concat([inv_jan, inv_feb], sort=True, join='inner')  #axis=0\n",
    "\n",
    "-----------------------------------------------------------\n",
    "   cid   | iid   | invoice_date    | total \n",
    "0  2     | 1     | 2009-01-01      | 1.98  \n",
    "1  4     | 2     | 2009-01-02      | 3.96  \n",
    "2  8     | 3     | 2009-01-03      | 5.94  \n",
    "0  2     | 1     | 2009-01-01      | 1.98  \n",
    "1  4     | 2     | 2009-01-02      | 3.96  \n",
    "2  8     | 3     | 2009-01-03      | 5.94  \n",
    "\n",
    "\n",
    "\n",
    "inv_jan.append([inv_feb, inv_mar, inv_apr], ignore_index=True, sort=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb289ec-835f-4969-9f5d-35d2b5d29873",
   "metadata": {},
   "source": [
    "## Concatenation basics\n",
    "\n",
    "You have been given a few tables of data with musical track info for different albums from the metal band, Metallica. The track info comes from their Ride The Lightning, Master Of Puppets, and St. Anger albums. Try various features of the .concat() method by concatenating the tables vertically together in different ways.\n",
    "\n",
    "The tables tracks_master, tracks_ride, and tracks_st have loaded for you.\n",
    "Instructions 1/3\n",
    "30 XP\n",
    "\n",
    "    Question 1\n",
    "    Concatenate 'tracks_master', 'tracks_ride', and 'tracks_st', in that order, setting sort to True.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Concatenate 'tracks_master', 'tracks_ride', and 'tracks_st', where the index goes from 0 to n-1.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question3\n",
    "    Concatenate tracks_master, tracks_ride, and tracks_st, showing only columns that are in all tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07dfe68-b83f-4329-8a43-7a948ec89db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], sort=True, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "tracks_from_albums_nicer = pd.concat([tracks_master, tracks_ride, tracks_st], \n",
    "                                     sort=True, \n",
    "                                     ignore_index=True, \n",
    "                                     join='inner')\n",
    "# ***************************************************************************************************************** #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428a193-acf3-4141-9db0-66fad42b12ea",
   "metadata": {},
   "source": [
    "## Concatenating with keys\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The leadership of the music streaming company has come to you and asked you for assistance in analyzing sales for a recent business quarter. They would like to know which month in the quarter saw the highest average invoice total. You have been given three tables with invoice data named inv_jul, inv_aug, and inv_sep. Concatenate these tables into one to create a graph of the average monthly invoice total.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Concatenate the three tables together vertically in order with the oldest month first, adding '7Jul', '8Aug', and '9Sep' as keys for their respective months, and save to variable avg_inv_by_month.\n",
    "    Use the .agg() method to find the average of the total column from the grouped invoices.\n",
    "    Create a bar chart of avg_inv_by_month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5673f-0b64-4ca3-8d4d-f383f924099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep], \n",
    "                            sort=True, \n",
    "                            ignore_index=False,\n",
    "                            keys=['7Jul', '8Aug', '9Sep'])\n",
    "\n",
    "\n",
    "                                           ### Just keep thinking ###################################################\n",
    "                                           ##########################################################################\n",
    "avg_inv_by_month = inv_jul_thr_sep.groupby(level=0)['total'].mean()\n",
    "\n",
    "\n",
    "\n",
    "avg_inv_by_month.plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72444b92-c614-4715-bced-da6ff89e01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tables and add keys\n",
    "inv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep], \n",
    "                            keys=['7Jul','8Aug','9Sep'])\n",
    "\n",
    "# Group the invoices by the index keys and find avg of the total column\n",
    "avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total':'mean'})\n",
    "\n",
    "# Bar plot of avg_inv_by_month\n",
    "avg_inv_by_month.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c128b39-cc8b-4a17-ad80-6b69d40c6eb3",
   "metadata": {},
   "source": [
    "## Using the append method\n",
    "\n",
    "The .concat() method is excellent when you need a lot of control over how concatenation is performed. However, if you do not need as much control, then the .append() method is another option. You'll try this method out by appending the track lists together from different Metallica albums. From there, you will merge it with the invoice_items table to determine which track sold the most.\n",
    "\n",
    "The tables tracks_master, tracks_ride, tracks_st, and invoice_items have loaded for you.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Use the .append() method to combine (in this order) tracks_ride, tracks_master, and tracks_st together vertically, and save to metallica_tracks.\n",
    "    Merge metallica_tracks and invoice_items on tid with an inner join, and save to tracks_invoices.\n",
    "#    For each tid and name in tracks_invoices, sum the quantity sold column, and save as tracks_sold.\n",
    "#    Sort tracks_sold in descending order by the quantity column, and print the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea203d2c-fa06-4866-8997-5930d04f51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metallica_tracks = tracks_ride.append(['tracks_master', 'tracks_st'], \n",
    "                                      ignore_index=True, \n",
    "                                      sort=True)\n",
    "\n",
    "\n",
    "tracks_invoices = metallica_tracks.merge(invoice_items, on='tid', how='inner')\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "tracks_sold = tracks_invoices.groupby(['tid', 'name'])['quantity'].sum()\n",
    "\n",
    "\n",
    "\n",
    "tracks_sold.sort_values('quantity', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befd619-c16d-4557-8e2f-8fab9a9cd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .append() method to combine the tracks tables\n",
    "metallica_tracks = ____.append(____, sort=False)\n",
    "\n",
    "# Merge metallica_tracks and invoice_items\n",
    "tracks_invoices = ____\n",
    "\n",
    "# For each tid and name sum the quantity sold\n",
    "tracks_sold = tracks_invoices.groupby(['tid','name']).agg(____)\n",
    "\n",
    "# Sort in decending order by quantity and print the results\n",
    "print(tracks_sold.sort_values(____))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b008342-07d6-4db9-8610-6c5fe3dc8665",
   "metadata": {},
   "source": [
    "## Verifying integrity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**In this lesson, we'll talk about verifying the integrity of our data.  Both the .merge() method and pd.concat() function have special features that allows us to verify the structure of our data.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "When merging two tables, we might expect the tables to have a one-to-one relationship.  However, one of the columns we are merging on may have a duplicated value, which will trun the relationship into a one-to-many.  When concatenating tables vertically, we might unintentionally create duplicated records if a records exists in both tables.  \n",
    "\n",
    "# The validate= and verify_integrity= args of the .merge() method and pd.concat() function respectively will allow us to verify the data.  \n",
    "\n",
    "Lets start with .merge() method.  If we provide the validate= argument one of these key strings ( 'one_to_one', 'one_to_many', 'mang_to_one', 'many_to_many' ), it will validate the relationship between the two tables.  For example, if we specify we want a one_to_one relationship, but it turns out the relationship between two tables is not one-to-one, then an error is raised.  \n",
    "\n",
    "In this example, we want to merge these two tables on the column tid, again, the data is from our music service.  The first table is named tracks, and the second is called specs for the technical specifications of each track.  Each track should have one set of specifications, so this should be a one-to-one merge.  However, notice that the specs table has two rows with a tid value equal to 2.  Therefore merging these two tables now becomes, unintentionally, a one-to-many relationship.  Lets merge the two tables with the tracks table on the left and specs on the right. Additionally, lets set the validate= argument equal to 'one_to_one' string key.  In the result, a MergeError is raised.  telling us 'Merge keys are not unique in right dataset: not a one-to-one merge'.  Python tells us that the right table has duplicates, so its not a one-to-one merge.  \n",
    "\n",
    "We know that we should handle those duplicates properly before merging, Now we'll merge album nformation with the tracks table.  For every album there are multiple tracks, so this should be a one-to-many relationship.  When we set the validate='one_to_many' argument as 'one_to_many' string key no error is raised.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets now talk about the pd.concat(varify_integrity=False) function.  It has argument verify_integrity= which by default is False.  However, if set to varify_integrity=True, it will check if there are duplicates in the index and raise an error if there are.  Note that it will only check the index values and not the columns.  \n",
    "\n",
    "To try out this feature, we will attempt to concatenate these two tables.  inv_feb and inv_mar table.  However, both tables were modified so the index contains invoice ids and it has set to be index.  Also notice that invoice id number 9 is in both tables, its a duplicate records.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets try to cancatenate the two tables together with the varify_integrity=True.  The pd.concat() function raises a ValueError stating that the 'Indexes have overlapping values'.  If set pd.concat([inv_feb, inv_mar], verify_integrity=False), the result now combines two tables with invoice id f number 9 repeated twice.  \n",
    "\n",
    "\n",
    "\n",
    "# Often our data is not clean, and it may not always be evidant if data has the expected structure.  Therefore, verifying this structure is useful, saving us from having a mean skewed by duplicate values, or from creating inaccurate plots.  \n",
    "If you receive a MergeError or a ValueError, you can fix the incorrect data or drop duplicate rows. In general, you should look to correct the issue.  \n",
    "\n",
    "\n",
    "\n",
    "-------------------     -------------       a one-to-many relationship sample in .merge()\n",
    "A    | B    | C          C    | D\n",
    "A1   | B1   | C1         C1   | D1\n",
    "A2   | B2   | C2         C1   | D2\n",
    "A3   | B3   | C3         C1   | D3\n",
    "                         C1   | D4\n",
    "\n",
    "-------------------     \n",
    "A    | B    | C         \n",
    "A1   | B1   | C1   \n",
    "A2   | B2   | C2   \n",
    "A3   | B3   | C3   \n",
    "\n",
    "-------------------     duplicated record when pd.concat() two tables vertically\n",
    "A    | B    | C         \n",
    "A3   | B3   | C3   \n",
    "A4   | B4   | C4   \n",
    "A5   | B5   | C5   \n",
    "\n",
    "\n",
    "\n",
    "tracks.merge(specs, on='tid', how='inner', validate='one_to_one')\n",
    "\n",
    "\n",
    "tracks table\n",
    "-------------------------------------------------------------\n",
    "   tid  | name             | aid  | mtid  | gid  | uprice\n",
    "0  2    | Balls to the ... | 2    | 2     | 1    | 0.99\n",
    "1  3    | Fast As a Shark  | 3    | 2     | 1    | 0.99\n",
    "2  4    | Restless and ... | 2    | 2     | 1    | 0.99\n",
    "\n",
    "specs table\n",
    "--------------------------------------\n",
    "   tid  | milliseconds | bytes\n",
    "0  2    | 342562       | 5510424\n",
    "1  3    | 230619       | 3990994\n",
    "2  2    | 252051       | 4331779\n",
    "\n",
    "\n",
    "\n",
    "pd.concat([inv_feb, inv_mar], verify_integrity=True)\n",
    "\n",
    "------------------------------------------  inv_feb table\n",
    "      | cid   | invoice_date    | total\n",
    "iid   |       |                 |\n",
    "7     | 2     | 2009-01-01      | 1.98\n",
    "8     | 4     | 2009-01-02      | 3.96\n",
    "9     | 8     | 2009-01-03      | 5.94\n",
    "\n",
    "\n",
    "---------------------------------------------------------   inv_mar table\n",
    "      | cid   | invoice_date    | total   | bill_ctry\n",
    "iid   |       |                 |         |\n",
    "9     | 2     | 2009-01-01      | 1.98    | Germany\n",
    "15    | 4     | 2009-01-02      | 3.96    | France\n",
    "16    | 8     | 2009-01-03      | 5.94    | France\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d7ed6-6197-4770-b311-0c0d2bbfb0a5",
   "metadata": {},
   "source": [
    "## Validating a merge\n",
    "\n",
    "You have been given 2 tables, artists, and albums. Use the IPython shell to merge them using artists.merge(albums, on='artid').head(). Adjust the validate argument to answer which statement is False.\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    You can use 'many_to_many' without an error, since there is a duplicate key in one of the tables.\n",
    "    You can use 'one_to_many' without error, since there is a duplicate key in the right table.\n",
    "    You can use 'many_to_one' without an error, since there is a duplicate key in the left table.\n",
    "    \n",
    "Hint\n",
    "\n",
    "    If the .merge() method raises an error, it will tell you which table has duplicate key values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303f6d6-c9e7-4b02-9a36-13ae22cec186",
   "metadata": {},
   "source": [
    "## Concatenate and merge to find common songs\n",
    "\n",
    "The senior leadership of the streaming service is requesting your help again. You are given the historical files for a popular playlist in the classical music genre in 2018 and 2019. Additionally, you are given a similar set of files for the most popular pop music genre playlist on the streaming service in 2018 and 2019. Your goal is to concatenate the respective files to make a large classical playlist table and overall popular music table. Then filter the classical music table using a semi-join to return only the most popular classical music tracks.\n",
    "\n",
    "The tables classic_18, classic_19, and pop_18, pop_19 have been loaded for you. Additionally, pandas has been loaded as pd.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#    Concatenate the classic_18 and classic_19 tables vertically where the index goes from 0 to n-1, and save to classic_18_19.\n",
    "    Concatenate the pop_18 and pop_19 tables vertically where the index goes from 0 to n-1, and save to pop_18_19.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2     Missing\n",
    "    With classic_18_19 on the left, merge it with pop_18_19 on tid using an inner join.\n",
    "    Use .isin() to filter classic_18_19 where tid is in classic_pop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f34d1b-5f50-4866-bfb2-893cb3760ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
    "\n",
    "\n",
    "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "classic_pop = classic_18_19.merge(pop_18_19, on='tid', how='inner')\n",
    "\n",
    "\n",
    "# ***************************************************************************************************************** #\n",
    "# ***************************************************************************************************************** #\n",
    "popular_classic = classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c50c06-f3dc-4d66-8113-64f4f2db43fa",
   "metadata": {},
   "source": [
    "## Using merge_ordered()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **In this last chapter we'll start discussing .merge_ordered() method.  This method can merge time-series and other orded data.  \n",
    "# *******************************************************************************************************************\n",
    "The .merge_ordered() method will allow us to merge the left and right tables shown here.  We can see the output of the .merge_ordered() when we merge on column c.  The results are similar to the standard .merge() method with join='outer' argument, but here the results are sorted.  The sorted results make this a useful method for ordered or time-series data.  \n",
    "\n",
    "So lets give context to this method.  It has many same arguments we have already covered with the .merge() method.  \n",
    "\n",
    "    They both contain on= argument to allow us to merge two tables on different columns.  \n",
    "    Both method support different join types, default how='outer' here,\n",
    "    Also, both method support suffixes= argument for overlapping column names, \n",
    "    \n",
    "    But .merge() is a method, while pd.merge_ordered() is a function, thus calling these is diff\n",
    "\n",
    "Earlierin the course, we called the .merge() method by first listing a table and calling the method afterward.  For .merge_ordered() you need to first call Pandas then .merge_ordered()\n",
    "\n",
    "\n",
    "In this chapter, we will be working with financial, microeconomic, and stock merket data.  In this example we have a table of the stock prices of the Applr corporation from February to June 2007.  We also have a table of the stock price for McDonald's corporation from January to may 2007, and we want to merge them.  The first two arguments (not in a list) are the left and right tables.  We set the on=argument equal to date, set the suffixes= argument equal to ('_apple', '_McDonald') to determine which table the data originated.  \n",
    "\n",
    "    pd.merge_ordered(appl, mcd, on='date', suffixes=('_aapl', '_mcd'))\n",
    "\n",
    "The result table are sorted by date, there isnt a value for Apple in January or a value for McDonald for June, since values for these time period are not available in the two original tables.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "We can fill in this missing data using a technique called forward filling.  It will interpolate missing data by filling the missing values with the previous value.  Going back to our stock example from before, we now set the fill_method='ffill' argument as forward-fill.  In the result, notice that the missing value for McDonald in the last row is now filled in with the row before it.  Notice the missing value for Apple in the first row is still missing since there isnt a row before the first row to copy into he missing value for Apple.  \n",
    "\n",
    "You might think about using the .merge_ordered() method instead of the regular .merge() method when you are working with order or time-series data.  Additionally, the fill-forward feature is useful for handling missing data, as most machine learning algorithms require that there are no missing values.  \n",
    "\n",
    "\n",
    "-------------------     -------------           ----------------------------\n",
    "A    | B    | C          C    | D               A    | B    | C    | D\n",
    "A3   | B3   | C3         C4   | D4              A1   | B1   | C1   | D1\n",
    "A2   | B2   | C2         C2   | D2              A2   | B2   | C2   | D2\n",
    "A1   | B1   | C1         C1   | D1              A3   | B3   | C3   | \n",
    "                                                     |      | C4   | D4\n",
    "\n",
    "\n",
    "----------------------------\n",
    "   date        | close\n",
    "0  2007-02-01  | 12.087143\n",
    "1  2007-03-01  | 13.272857\n",
    "2  2007-04-01  | 14.257143\n",
    "3  2007-05-01  | 17.312857\n",
    "   \n",
    "\n",
    "---------------             -------------- forward filling\n",
    "A     | B                   A    | B \n",
    "A1    | B1                  A1   | B1                    \n",
    "A2    |                     A2   | B1   forward-fill\n",
    "A3    | B3                  A3   | B3\n",
    "A4    |                     A4   | B3   forward-fill\n",
    "A5    | B5                  A5   | B5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81118f92-3905-4c05-9671-76030cf5118f",
   "metadata": {},
   "source": [
    "## Correlation between GDP and S&P500\n",
    "\n",
    "In this exercise, you want to analyze stock returns from the S&P 500. You believe there may be a relationship between the returns of the S&P 500 and the GDP of the US. Merge the different datasets together to compute the correlation.\n",
    "\n",
    "Two tables have been provided for you, named sp500, and gdp. As always, pandas has been imported for you as pd.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Use merge_ordered() to merge gdp and sp500 using a left join on year and date. Save the results as gdp_sp500.\n",
    "    Print gdp_sp500 and look at the returns for the year 2018.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2     Missing\n",
    "    Use merge_ordered(), again similar to before, to merge gdp and sp500 use the function's ability to interpolate missing data to forward fill the missing value for returns, assigning this table to the variable gdp_sp500.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3     Missing\n",
    "    Subset the gdp_sp500 table, select the gdp and returns columns, and save as gdp_returns.\n",
    "    Print the correlation matrix of the gdp_returns table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a35ca6c-94cf-4eda-8074-38494a2e45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Date  Returns\n",
      "0  2008   -38.49\n",
      "1  2009    23.45\n",
      "2  2010    12.78\n",
      "3  2011     0.00\n",
      "4  2012    13.41\n",
      "    Country Name Country Code     Indicator Name  Year           GDP\n",
      "0          China          CHN  GDP (current US$)  2010  6.087160e+12\n",
      "1        Germany          DEU  GDP (current US$)  2010  3.417090e+12\n",
      "2          Japan          JPN  GDP (current US$)  2010  5.700100e+12\n",
      "3  United States          USA  GDP (current US$)  2010  1.499210e+13\n",
      "4          China          CHN  GDP (current US$)  2011  7.551500e+12\n",
      "    Year           GDP\n",
      "3   2010  1.499210e+13\n",
      "7   2011  1.554260e+13\n",
      "11  2012  1.619700e+13\n",
      "15  2012  1.619700e+13\n",
      "19  2013  1.678480e+13\n",
      "   Year           GDP    Date  Returns\n",
      "0  2010  1.499210e+13  2010.0    12.78\n",
      "1  2011  1.554260e+13  2011.0     0.00\n",
      "2  2012  1.619700e+13  2012.0    13.41\n",
      "3  2012  1.619700e+13  2012.0    13.41\n",
      "4  2013  1.678480e+13  2013.0    29.60\n"
     ]
    }
   ],
   "source": [
    "sp_500 = pd.read_csv('S&P500.csv')\n",
    "print(sp_500.head())\n",
    "\n",
    "\n",
    "gdp = pd.read_csv('WorldBank_GDP.csv')\n",
    "print(gdp.head())\n",
    "\n",
    "\n",
    "us_gdp = gdp.loc[gdp['Country Code']=='USA', ['Year', 'GDP']]\n",
    "print(us_gdp.head())\n",
    "\n",
    "\n",
    "\n",
    "gdp_sp500 = pd.merge_ordered(us_gdp, sp_500, how='left', left_on='Year', right_on='Date')\n",
    "print(gdp_sp500.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367fdd6-673b-4f51-9df0-028e340fc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use merge_ordered() to merge gdp and sp500 on year and date\n",
    "gdp_sp500 = pd.merge_ordered(____, ____, left_on=____, right_on=____, \n",
    "                             how=____)\n",
    "\n",
    "# Print gdp_sp500\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df3b14-fc4f-420e-8f95-a798893fb9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911284b-dde3-4cf8-ba0f-9d991bb89582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fd3fb-bc0c-48bf-98e0-88327cd39901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7e40e-a570-4099-bf56-d12046f22ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa0a9f-1c1c-44d2-8cb2-ed06ffcd7db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba9805-4ce8-4704-9996-39e435488b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8398a4-108b-460c-bb73-464388214c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf46bf-6dc0-4b8b-ada5-23390ce4d9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ffca8-e093-4155-a122-19cad52b9df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07b7c8-7590-45e3-bdcf-36aa64ae98e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db369f-e76e-4b32-8e23-94ea7ed064c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fd943-5d28-4d7f-a989-67a7815d55f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a50d5-01b3-45e2-ba77-79ac6720d2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24108488-8d1c-4296-a891-f497e0c41fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48940f-b990-4b9e-a020-a4569805e8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e7ba2-767f-4bfe-a129-c72d496f0b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176f9f5-11f8-4c12-a0c6-2459f525eaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791293e-4737-4756-8fcd-3a6f36643fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
