{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdb6c41-92b9-47b2-b660-7e6a4b1e24c2",
   "metadata": {},
   "source": [
    "## Cleaning Data in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055a7c9-8970-4132-829c-2b27d95c641c",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions. Data cleaning is an essential task in data science. Without properly cleaned data, the results of any data analysis or machine learning model could be inaccurate. In this course, you will learn how to identify, diagnose, and treat a variety of data cleaning problems in Python, ranging from simple to advanced. You will deal with improper data types, check that your data is in the correct range, handle missing data, perform record linkage, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef6cb7-259a-431e-90c6-1daf990df19d",
   "metadata": {},
   "source": [
    "##  Common data problems\n",
    "Free\n",
    "0%\n",
    "\n",
    "In this chapter, you'll learn how to overcome some of the most common dirty data problems. You'll convert data types, apply range constraints to remove future data points, and remove duplicated data points to avoid double-counting.\n",
    "\n",
    "    Data type constraints    50 xp\n",
    "    Common data types    100 xp\n",
    "    Numeric data or ... ?    100 xp\n",
    "    Summing strings and concatenating numbers    100 xp\n",
    "    Data range constraints    50 xp\n",
    "    Tire size constraints    100 xp\n",
    "    Back to the future    100 xp\n",
    "    Uniqueness constraints    50 xp\n",
    "    How big is your subset?    50 xp\n",
    "    Finding duplicates   100 xp\n",
    "    Treating duplicates    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781858e-0a46-4241-98dd-6547f3c63dc5",
   "metadata": {},
   "source": [
    "##  Text and categorical data problems\n",
    "0%\n",
    "\n",
    "Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, you’ll learn how to fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency.\n",
    "\n",
    "    Membership constraints    50 xp\n",
    "    Members only    100 xp\n",
    "    Finding consistency    100 xp\n",
    "    Categorical variables    50 xp\n",
    "    Categories of errors    100 xp\n",
    "    Inconsistent categories    100 xp\n",
    "    Remapping categories    100 xp\n",
    "    Cleaning text data    50 xp\n",
    "    Removing titles and taking names    100 xp\n",
    "    Keeping it descriptive    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf42f6f-f930-4e9a-9fd7-7aebf5c266a3",
   "metadata": {},
   "source": [
    "##  Advanced data problems\n",
    "0%\n",
    "\n",
    "In this chapter, you’ll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses.\n",
    "\n",
    "    Uniformity    50 xp\n",
    "    Ambiguous dates    50 xp\n",
    "    Uniform currencies    100 xp\n",
    "    Uniform dates    100 xp\n",
    "    Cross field validation    50 xp\n",
    "    Cross field or no cross field?    100 xp\n",
    "    How's our data integrity?    100 xp\n",
    "    Completeness    50 xp\n",
    "    Is this missing at random?    50 xp\n",
    "    Missing investors    100 xp\n",
    "    Follow the money    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a59e4-6b35-408c-8fb3-1bf98ad897de",
   "metadata": {},
   "source": [
    "##  Record linkage\n",
    "0%\n",
    "\n",
    "Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to link records by calculating the similarity between strings—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset.\n",
    "\n",
    "    Comparing strings    50 xp\n",
    "    Minimum edit distance    50 xp\n",
    "    The cutoff point    100 xp\n",
    "    Remapping categories II    100 xp\n",
    "    Generating pairs    50 xp\n",
    "    To link or not to link?    100 xp\n",
    "    Pairs of restaurants    100 xp\n",
    "    Similar restaurants    100 xp\n",
    "    Linking DataFrames    50 xp\n",
    "    Getting the right index    50 xp\n",
    "    Linking them together!    100 xp\n",
    "    Congratulations!    50 xp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094813d4-e269-4a00-8837-ebe1ea5fb63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa37edc2-e0fe-4481-a3d7-c2f29bf3dbee",
   "metadata": {},
   "source": [
    "## Data type constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The institutor's name is Adel, he'll be our host as we learn how to clean data in Python.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# In this course, we're going to understand how to diagnose different problems in our data and how they can come up during out workflow.  We will also understand the side effects of not treating our data correctly.  And various ways to address different types of dirty data.  \n",
    "\n",
    "\n",
    "In this chapter, we're going to discuss the most common data problems you may encounter and how to address them.  \n",
    "\n",
    "# To understand why we need to clean data, lets remind ourselves of the data science workflow.  In a typical data science workflow, we usually access our raw data, explore and process it, develop insights using visualizations or predictive models, and finally report these insights with dashboards or reports.  \n",
    "\n",
    "\n",
    "Access Data --> Explore and Process Data --> Extract Insights --> Report Insights\n",
    "\n",
    "\n",
    "# Dirty data can appear because of duplicate values, miss-spellings, data type parsing errors and legacy systems.  \n",
    "Without making sure that data is properly clearned in the exploration and processing phase, we will surely compromise the insights and reports subsequently generated.  As the old adage says, garbage in garbage out.  \n",
    "\n",
    "\n",
    "When working with data, there are various types that we may encounter along the way.  We could be working with text data, integers, decimals, dates, zip codes, and others.  Luckily, Python has specific data type objects for various data types that you're probably familiar with by now.  This makes it much easier to manipulate these various data types in Python.  \n",
    "    \n",
    "    Text data, Integers, Decimals, Binary, Dates, Categories\n",
    "    str, int, float, bool, datetime, cateory\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# As such, before preparing to analyze and extract insights from our data, we need to make sure our variables have the correct data types, other wise we risk compromising our anaysis.  Lets take a look at the following example.  \n",
    "\n",
    "\n",
    "Belwo is a head od a DF containing revenue generated and quantity of items sold for a sales order.  We want to calculate the total revenue generated by all sales orders.  As you can see, the Revenue dolumn has the dollar sign on the righthand side.  A close inspection of the DF columns' data types using the \".dtypes\" attribute returns object for the Revenue column, which is what Pandas uses to store strings.  We can also check the data types as well as the number of missing values per column in a DataFrame, by using the \".info()\" method.  \n",
    "\n",
    "Since the Revenue column is a string, summing across all sales order returns one large concatenated string containing each row's string.  To fix this, we need to first remove the $ sign from the string so that Pandas is able to convert the strings into numbers without error (how about we define it at the first place? need to do some test).  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To do this with the \"sales['Revenue'].str.strip('$')\" method, while specifying the string we want to strip as an argument which is in this case the dollar sign.  \n",
    "Since our dollar value do not contain decimals, we then convert the Revenue column to an integer by using the \".astype()\" method , specifying the desired data type as argument.  Had our revenue value been decimal, we would have converted the Revenue column to float.  \n",
    "We can make sure that the Revenue column is now an integer by using the \"assert\" statement, which takes in a condition as input, and returns nothing if that condition is met, and ar error is it is not.  \n",
    "You can test almost anything you can image of by using \"assert\", and we'll see more ways to utilize it as we go along the course.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "# A common type of data seems numeric but actually represents cateories with a finite set of possible categories.  This is called categorical data.  \n",
    "We will look more closely at categorical data in Chapter 2, but lets take a look at this example.  Here we have a marriage status column, which is represented by 0 for never married, 1 for married, 2 for separated, and 3 for divorced.  \n",
    "\n",
    "However it will be imported of type integer, which could lead to misleading results when trying to extract some statistical summaries.  We can solve this by using the same \".astype()\" method seen earlier, but this time specifying the \"category\" data type.  When applying the \".describe()\" method again, we see that the summary statistics are much more aligned with that of a categorical variable, discussinng the number of observations, number of unique values, most frequent category instead of mean and standard deviation.  \n",
    "\n",
    "\n",
    "Now that we have solid understanding of data type constrains - lets get to practice.  \n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Remove $ from Revenue column\n",
    "sales['Revenue'] = sales['Revenue'].str.strip('$')\n",
    "sales['Revenue'] = sales['Revenue'].astype('int')\n",
    "\n",
    "# Verify that Revenue is now an integer\n",
    "assert sales['Revenue'].dtypes == 'int'\n",
    "\n",
    "\n",
    "\n",
    "# Convert to categorical\n",
    "df['marriage_status'] = df['marriage_status'].astype('category')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53a2fc-1b2d-48da-9110-a923c4faea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales.csv')\n",
    "\n",
    "print(sales.head())\n",
    "\n",
    "\n",
    "---------------------------------------------\n",
    "  SalesOrderId  | Revenue     | Quantity\n",
    "0        43659  |     23153$  |       12\n",
    "1        43660  |      1457$  |        2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c075397b-8c4e-45ec-b130-394c52927157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23153\n",
      "<class 'str'>\n",
      "23153\n",
      "<class 'int'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4640/1974750029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#assert int(n_price).dtype() == 'int'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "price = '23153$'\n",
    "\n",
    "\n",
    "n_price = price.strip('$')\n",
    "print(n_price)\n",
    "print(type(n_price))\n",
    "\n",
    "print(int(n_price))\n",
    "print(type(int(n_price)))\n",
    "\n",
    "\n",
    "#assert int(n_price).dtype() == 'int'\n",
    "assert type(int(n_price)) == 'int'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9797795-53fc-4193-95bc-156f95347465",
   "metadata": {},
   "source": [
    "## Common data types\n",
    "\n",
    "Manipulating and analyzing data with incorrect data types could lead to compromised analysis as you go along the data science workflow.\n",
    "\n",
    "When working with new data, you should always check the data types of your columns using the .dtypes attribute or the .info() method which you'll see in the next exercise. Often times, you'll run into columns that should be converted to different data types before starting any analysis.\n",
    "\n",
    "In this exercise, you'll first identify different types of data and correctly map them to their respective types.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Assign each card to what type of data you think it is.\n",
    "\n",
    "Numeric data type: Salary earned monthly, \n",
    "                   Number of items bought in a basket, \n",
    "                   Number of points on customer loyalty card\n",
    "\n",
    "Text:              City of residence, \n",
    "                   First name\n",
    "                   Shipping address of a customer\n",
    "\n",
    "Dates:             Birthdates of clients, \n",
    "                   Order date of a product, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0971b53-b4fd-42ea-930a-01603d66afd4",
   "metadata": {},
   "source": [
    "## Numeric data or ... ?\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called \"ride_sharing\". It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.\n",
    "\n",
    "The \"user_type\" column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "\n",
    "    1 for free riders.\n",
    "    2 for pay per ride.\n",
    "    3 for monthly subscribers.\n",
    "\n",
    "In this instance, you will print the information of ride_sharing using .info() and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset. The pandas package is imported as pd.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the information of ride_sharing.\n",
    "#    Use .describe() to print the summary statistics of the user_type column from ride_sharing.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    By looking at the summary statistics - they don't really seem to offer much description on how users are distributed along their purchase type, why do you think that is?\n",
    "Possible Answers\n",
    "    -The user_type column is not of the correct type, it should be converted to str.\n",
    "    -The user_type column has an infinite set of possible values, it should be converted to category.\n",
    "#    -The user_type column has an finite set of possible values that represent groupings of data, it should be converted to category.     Also other 'int64' values, station_A_id, bike_id, user_birth_year, user_gender\n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "    Question 3\n",
    "#    -Convert \"user_type\" into categorical by assigning it the 'category' data type and store it in the \"user_type_cat\" column.\n",
    "#    -Make sure you converted \"user_type_cat\" correctly by using an \"assert\" statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcee990f-9595-4f4e-ab9c-66e9528957f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration           object\n",
      "station_A_id        int64\n",
      "station_A_name     object\n",
      "station_B_id        int64\n",
      "station_B_name     object\n",
      "bike_id             int64\n",
      "user_type           int64\n",
      "user_birth_year     int64\n",
      "user_gender        object\n",
      "dtype: object\n",
      "     duration  station_A_id  \\\n",
      "0  12 minutes            81   \n",
      "1  24 minutes             3   \n",
      "2   8 minutes            67   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "\n",
      "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480          2             1959   \n",
      "1  Eureka Valley Recreation Center     5193          2             1965   \n",
      "2    The Embarcadero at Steuart St     3652          3             1993   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "category\n",
      "user_type describe\n",
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type, dtype: int64\n",
      "12972\n",
      "0        2\n",
      "1        2\n",
      "4        2\n",
      "5        2\n",
      "6        2\n",
      "        ..\n",
      "25752    2\n",
      "25753    2\n",
      "25756    2\n",
      "25757    2\n",
      "25758    2\n",
      "Name: user_type, Length: 12972, dtype: category\n",
      "Categories (3, int64): [1, 2, 3]\n",
      "0        2\n",
      "1        2\n",
      "4        2\n",
      "5        2\n",
      "6        2\n",
      "        ..\n",
      "25752    2\n",
      "25753    2\n",
      "25756    2\n",
      "25757    2\n",
      "25758    2\n",
      "Name: user_type, Length: 12972, dtype: category\n",
      "Categories (3, int64): [1, 2, 3]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  object  \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(4), object(4)\n",
      "memory usage: 1.8+ MB\n",
      "None\n",
      "       station_A_id  station_B_id       bike_id  user_birth_year\n",
      "count  25760.000000  25760.000000  25760.000000     25760.000000\n",
      "mean      31.023602     89.558579   4107.621467      1983.054969\n",
      "std       26.409263    105.144103   1576.315767        10.010992\n",
      "min        3.000000      3.000000     11.000000      1901.000000\n",
      "25%       15.000000     21.000000   3106.000000      1978.000000\n",
      "50%       21.000000     58.000000   4821.000000      1985.000000\n",
      "75%       67.000000     93.000000   5257.000000      1990.000000\n",
      "max       81.000000    383.000000   6638.000000      2001.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(df.head(3))\n",
    "\n",
    "\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "#print(dir(df['user_type']))\n",
    "\n",
    "print(df['user_type'].dtypes)\n",
    "assert df['user_type'].dtype == 'category' # ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('user_type describe')\n",
    "print(df['user_type'].describe())\n",
    "\n",
    "\n",
    "print(len(df[df['user_type']==2]))\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# filt_list = movies_genres.loc[movies_genres['_merge']=='both', 'title'].unique()\n",
    "print(df.loc[df['user_type']==2, 'user_type'])\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "print(df[df['user_type']==2]['user_type'].copy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48055e6c-63c2-47b3-a701-ada0b5216392",
   "metadata": {},
   "source": [
    "## Summing strings and concatenating numbers\n",
    "\n",
    "In the previous exercise, you were able to identify that category is the correct data type for user_type and convert it \n",
    "# in order to extract relevant statistical summaries that shed light on the distribution of user_type.\n",
    "\n",
    "# Another common data type problem is importing what should be numerical values as strings, \n",
    "as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.\n",
    "\n",
    "In this exercise, you'll be converting the string column duration to the type int. Before that however, you will need to make sure to strip \"minutes\" from the column in order to make sure pandas reads it as numerical. The pandas package has been imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Use the \".strip()\" method to strip duration of \"minutes\" and store it in the \"duration_trim\" column.\n",
    "    Convert \"duration_trim\" to int and store it in the \"duration_time\" column.\n",
    "    Write an \"assert\" statement that checks if duration_time's data type is now an int.\n",
    "    Print the average ride duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "738ded22-e604-4f3a-b0ec-f93a3d873df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration           object\n",
      "station_A_id        int64\n",
      "station_A_name     object\n",
      "station_B_id        int64\n",
      "station_B_name     object\n",
      "bike_id             int64\n",
      "user_type           int64\n",
      "user_birth_year     int64\n",
      "user_gender        object\n",
      "dtype: object\n",
      "0    12 minutes\n",
      "1    24 minutes\n",
      "2     8 minutes\n",
      "3     4 minutes\n",
      "4    11 minutes\n",
      "Name: duration, dtype: object\n",
      "0    12\n",
      "1    24\n",
      "2     8\n",
      "3     4\n",
      "4    11\n",
      "Name: duration, dtype: int64\n",
      "\n",
      "\n",
      "<bound method NDFrame._add_numeric_operations.<locals>.mean of 0        12\n",
      "1        24\n",
      "2         8\n",
      "3         4\n",
      "4        11\n",
      "         ..\n",
      "25755    11\n",
      "25756    10\n",
      "25757    14\n",
      "25758    14\n",
      "25759    29\n",
      "Name: duration, Length: 25760, dtype: int64>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  int64   \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(5), object(3)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.dtypes)\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "\n",
    "print(df['duration'].head())\n",
    "\n",
    "\n",
    "df['duration'] = df['duration'].str.strip('minutes')\n",
    "\n",
    "df['duration'] = df['duration'].astype('int')\n",
    "print(df['duration'].head())\n",
    "\n",
    "print('\\n')\n",
    "print(df['duration'].mean)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261ae62-1f89-472f-8ee3-e8315793665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].____.____()\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ____\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['____'].____ == '____'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f7a79-1ec0-460d-8b95-01691e25bbe2",
   "metadata": {},
   "source": [
    "## Data range constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Hi and welcome back.  In this lesson, we're going to discuss data that should fall within a range.  Lets first start off with some motivation.  \n",
    "\n",
    "Imagine we have a dataset of movies with their respective average rating from a streaming service.  The rating can be any integer between 1 and 5.  After creating a histogram with Matplotlib, we see that there are a few movies with an average rating of 6, which is well above the allowable range. \n",
    "# This is most likely an error in data collection or parsing, where a variable is well beyond its range and treating it is essential to have accurate analysis.  \n",
    "\n",
    "Here is another example, where we see subscription dates in the future for a service.  \n",
    "# Inherently this doesn't make any sence, as well cannot sign up for a service in the future, but these errors exist either due to technical or human error.  \n",
    "We use the datetime package's \".date.today()\" function to get today's date, and we filter the dateset by any subscription date higher than today's date.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# We need to pay attention to the range of our data. \n",
    "\n",
    "\n",
    "\n",
    "# There's a variety of options to deal with out of range data.  \n",
    "\n",
    "The simplest option is to drop the data.  However, depending on the size of your out of range data, you could be loosing out on essential information.  As a rule of thumb, only drop data when a small proportion of your dataset is affected by out of range values, however you really need to understand your dataset before deciding to drop values.  \n",
    "\n",
    "Another option would be setting custom minimums or maximums to your columns.  \n",
    "\n",
    "We could also set the data to be missing, and impute it, but we'll take a look at how to deal with missing data in Chapter 3.  \n",
    "\n",
    "We could also, dependent on the business assumptions behind our data, assign a custom value for any values of our data that go beyond a certain range.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take a look at the movies example mentioned earlier.  We first isolate the movies with ratings higher than 5.  Now if these values are affect a small set of our data, we can drop them.  We can drop them is 2 ways - we can either create a new filtered movies DF where we only keep values of \"avg_rating\" lower than or equal to 5.  Or drop the values by using the \".drop()\" method.  The \".drop()\" method takes in as argument the row indices of movies for which the \"avg_rating\" is higher than 5.  We set the \"inplace=\" argument to True so that values are dropped in place and we don't have to create a new column.  We can make sure this is set in place using assert statement that checks if the maximumof avg_rating is lower lan or equal to 5.  \n",
    "\n",
    "Depending on the assumptions behind our data, we can also change the out of range values to a hard limit.  For example, here we're setting any value of the avg_rating column to 5 if it goes beyond it.  We can do this using the \".loc[]\" method, which returns all cells that fit a custom row and column index.  It takes as first argument the row index, or here all instaces of avg_rating above 5, and second argument the column index, which is here the avg_rating column.  Aagin, we can make sure that this change was done using an assert statement.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take annother look at the date range example mentioned earlier, where we have subscriptions happening inthe future.  We first look at the datetypes (.dtype) of the column with the \".dtype\" attribute.  We can confirm that the subscription_date column is an \"object\" and not a \"datetime\" object.  Datetime objects allow much earsier manipulation of date data, so lets convert it to that.  We do so with \"pd.to_datetime()\" function from Pandas, which takes in as argument the column we want to convert.  We can then test the data type conversion by asserting that the subscription date's column is equal to \"datetime64[ns]\", which is how the data type is represented in Pandas.  \n",
    "\n",
    "Now that the column is in datetime, we can treat it in a variety of ways.  We first create a today_date variable using the datetime function \".date.today()\", which allows us to store today's date.  We can then either drop the rows with exceeding dates similar to how we did in the average rating example, or replace exceeding values with todays date.  \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(movies['avg_rating'])\n",
    "plt.title('Average rating of movies (1-5)')\n",
    "\n",
    "\n",
    "\n",
    "# Import date time\n",
    "import datetime\n",
    "\n",
    "today_date = datetime.date.today()\n",
    "user_signups[user_signups['subscription_date'] > today_date]\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# Output Movies with range > 5\n",
    "movies[movies['avg_rating'] > 5]\n",
    "\n",
    "\n",
    "\n",
    "# Drop values using filtering\n",
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "\n",
    "# Drop values using .drop()\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True)  ################################################\n",
    "\n",
    "assert movies['avg_rating'].max() == 5\n",
    "\n",
    "\n",
    "\n",
    "# Convert avg_rating > 5 to value 5\n",
    "movies.loc[movies['avg_rating']>5, 'avg_rating'] = 5\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataTime\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date'])\n",
    "\n",
    "# Assert that conversion happened\n",
    "assers user_signups['subscription_date'].dtype == 'datetime64[ns]'  #################################################\n",
    "\n",
    "\n",
    "<1> Drop the data\n",
    "today_date = datetime.date.today()\n",
    "\n",
    "# Drop values using filtering\n",
    "user_signups = user_signups[user_signups['subscription_date'] < today_date]\n",
    "# Drop values using .drop()\n",
    "user_signups.drop(user_signups[user_signups['subscription_date'] < today_date].index, inplace = True)  ##############\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "<2> Hardcode dates with upper limit\n",
    "\n",
    "# Replace values using filtering\n",
    "user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date\n",
    "# Assert is true\n",
    "assert user_signups['subscription_date'].max().date() <= today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc5690b8-861f-4968-a047-36524ae5bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Search': [{'Title': 'Batman: The Killing Joke', 'Year': '2016', 'imdbID': 'tt4853102', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BMTdjZTliODYtNWExMi00NjQ1LWIzN2MtN2Q5NTg5NTk3NzliL2ltYWdlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Mask of the Phantasm', 'Year': '1993', 'imdbID': 'tt0106364', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTRiMWM3MGItNjAxZC00M2E3LThhODgtM2QwOGNmZGU4OWZhXkEyXkFqcGdeQXVyNjExODE1MDc@._V1_SX300.jpg'}, {'Title': 'Batman: The Dark Knight Returns, Part 2', 'Year': '2013', 'imdbID': 'tt2166834', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTEzMmE0ZDYtYWNmYi00ZWM4LWJjOTUtYTE0ZmQyYWM3ZjA0XkEyXkFqcGdeQXVyNTA4NzY1MzY@._V1_SX300.jpg'}, {'Title': 'Batman: Year One', 'Year': '2011', 'imdbID': 'tt1672723', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BNTJjMmVkZjctNjNjMS00ZmI2LTlmYWEtOWNiYmQxYjY0YWVhXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Assault on Arkham', 'Year': '2014', 'imdbID': 'tt3139086', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BZDU1ZGRiY2YtYmZjMi00ZDQwLWJjMWMtNzUwNDMwYjQ4ZTVhXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman', 'Year': '1966', 'imdbID': 'tt0060153', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BMmM1OGIzM2UtNThhZS00ZGNlLWI4NzEtZjlhOTNhNmYxZGQ0XkEyXkFqcGdeQXVyNTkxMzEwMzU@._V1_SX300.jpg'}, {'Title': 'Batman: Gotham Knight', 'Year': '2008', 'imdbID': 'tt1117563', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BM2I0YTFjOTUtMWYzNC00ZTgyLTk2NWEtMmE3N2VlYjEwN2JlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Arkham City', 'Year': '2011', 'imdbID': 'tt1568322', 'Type': 'game', 'Poster': 'https://m.media-amazon.com/images/M/MV5BZDE2ZDFhMDAtMDAzZC00ZmY3LThlMTItMGFjMzRlYzExOGE1XkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman Beyond', 'Year': '1999–2001', 'imdbID': 'tt0147746', 'Type': 'series', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTBiZjFlZDQtZjc1MS00YzllLWE5ZTQtMmM5OTkyNjZjMWI3XkEyXkFqcGdeQXVyMTA1OTEwNjE@._V1_SX300.jpg'}, {'Title': 'Son of Batman', 'Year': '2014', 'imdbID': 'tt3139072', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYjdkZWFhNzctYmNhNy00NGM5LTg0Y2YtZWM4NmU2MWQ3ODVkXkEyXkFqcGdeQXVyNTA0OTU0OTQ@._V1_SX300.jpg'}], 'totalResults': '497', 'Response': 'True'}\n",
      "['Batman: The Killing Joke', 'Batman: Mask of the Phantasm', 'Batman: The Dark Knight Returns, Part 2', 'Batman: Year One', 'Batman: Assault on Arkham', 'Batman', 'Batman: Gotham Knight', 'Batman: Arkham City', 'Batman Beyond', 'Son of Batman']\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.omdbapi.com/?t=hackers'\n",
    "url = 'http://www.omdbapi.com/?i=tt3896198&apikey=8c241015'\n",
    "url = 'http://www.omdbapi.com/?s=Batman&page=1&apikey=8c241015'\n",
    "url = 'http://www.omdbapi.com/?s=Batman&page=2&apikey=8c241015'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "r = requests.get(url)\n",
    "j_data = r.json()\n",
    "\n",
    "print(j_data)\n",
    "\n",
    "\n",
    "\n",
    "titles = []\n",
    "with requests.get(url) as r:\n",
    "    j_data = r.json()\n",
    "    content = j_data['Search']\n",
    "    for i in content:\n",
    "        titles.append(i['Title'])\n",
    "    \n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33c05df2-cb75-42de-89e9-69dccb9d1d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     duration  station_A_id  \\\n",
      "0  12 minutes            81   \n",
      "1  24 minutes             3   \n",
      "2   8 minutes            67   \n",
      "3   4 minutes            16   \n",
      "4  11 minutes            22   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "3                            Steuart St at Market St            28   \n",
      "4                              Howard St at Beale St           350   \n",
      "\n",
      "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480          2             1959   \n",
      "1  Eureka Valley Recreation Center     5193          2             1965   \n",
      "2    The Embarcadero at Steuart St     3652          3             1993   \n",
      "3     The Embarcadero at Bryant St     1883          1             1979   \n",
      "4             8th St at Brannan St     4626          2             1994   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "3        Male  \n",
      "4        Male  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  object  \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(4), object(4)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_A_id</th>\n",
       "      <th>station_B_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25760.000000</td>\n",
       "      <td>25760.000000</td>\n",
       "      <td>25760.000000</td>\n",
       "      <td>25760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.023602</td>\n",
       "      <td>89.558579</td>\n",
       "      <td>4107.621467</td>\n",
       "      <td>1983.054969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.409263</td>\n",
       "      <td>105.144103</td>\n",
       "      <td>1576.315767</td>\n",
       "      <td>10.010992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1901.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3106.000000</td>\n",
       "      <td>1978.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>4821.000000</td>\n",
       "      <td>1985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>5257.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>6638.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station_A_id  station_B_id       bike_id  user_birth_year\n",
       "count  25760.000000  25760.000000  25760.000000     25760.000000\n",
       "mean      31.023602     89.558579   4107.621467      1983.054969\n",
       "std       26.409263    105.144103   1576.315767        10.010992\n",
       "min        3.000000      3.000000     11.000000      1901.000000\n",
       "25%       15.000000     21.000000   3106.000000      1978.000000\n",
       "50%       21.000000     58.000000   4821.000000      1985.000000\n",
       "75%       67.000000     93.000000   5257.000000      1990.000000\n",
       "max       81.000000    383.000000   6638.000000      2001.000000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "df.describe()\n",
    "\n",
    "#help(df.drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d396a30-5313-49b7-8f24-51dd411433b5",
   "metadata": {},
   "source": [
    "## Tire size constraints\n",
    "\n",
    "In this lesson, you're going to build on top of the work you've been doing with the ride_sharing DataFrame. You'll be working with the \"tire_sizes\" column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.\n",
    "\n",
    "# In this exercise, you will make sure the tire_sizes column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Convert the \"tire_sizes\" column from category to 'int'.\n",
    "    Use \".loc[]\" method to set all values of tire_sizes above 27 to 27.\n",
    "    Reconvert back tire_sizes to 'category' from int.\n",
    "    Print the description of the tire_sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f6c902-1baf-4688-8bd5-21b7cd884a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 duration  station_A_id  \\\n",
      "0           0      12             81   \n",
      "1           1      24              3   \n",
      "2           2       8             67   \n",
      "3           3       4             16   \n",
      "4           4      11             22   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "3                            Steuart St at Market St            28   \n",
      "4                              Howard St at Beale St           350   \n",
      "\n",
      "                    station_B_name  bike_id user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480         2             1959   \n",
      "1  Eureka Valley Recreation Center     5193         2             1965   \n",
      "2    The Embarcadero at Steuart St     3652         3             1993   \n",
      "3     The Embarcadero at Bryant St     1883         1             1979   \n",
      "4             8th St at Brannan St     4626         2             1994   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "3        Male  \n",
      "4        Male  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3788/633058815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert tire_sizes to integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mride_sharing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tire_sizes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'____'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m____\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'____'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Set all values above 27 to 27\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ride_sharing = pd.read_csv('ride_sharing_new.csv')\n",
    "\n",
    "ride_sharing['duration'] = ride_sharing['duration'].str.strip('minutes')\n",
    "ride_sharing['duration'].astype('int')\n",
    "\n",
    "ride_sharing['user_type'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "print(ride_sharing.head())\n",
    "\n",
    "\n",
    "\n",
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_size'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_size'] > 27, 'tire_size'] = 27  # ***********************************************\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')  # ***************************************\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0113d-fa1f-4406-8656-45de4d583eed",
   "metadata": {},
   "source": [
    "## Back to the future\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "A new update to the data pipeline feeding into the \"ride_sharing\" DataFrame has been updated to register each ride's date. This information is stored in the \"ride_date\" column of the type \"object\", which represents strings in pandas.\n",
    "\n",
    "# A bug was discovered which was relaying rides taken today as taken next year (think other conditions, last year?).\n",
    "To fix this, you will find all instances of the \"ride_date\" column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert \"ride_date\" to a datetime object.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "The datetime package has been imported as dt, alongside all the packages you've been using till now.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Convert ride_date to a datetime object and store it in ride_dt column using to_datetime().\n",
    "    Create the variable today, which stores today's date by using the \"dt.date.today()\" function.\n",
    "#    For all instances of \"ride_dt\" in the future, set them to today's date.\n",
    "    Print the maximum date in the ride_dt column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb53826-f7e6-4948-bd7b-d8c71fca039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "today_date = datetime.date.today()\n",
    "\n",
    "ride_sharing['ride_date'] = ride_sharing['ride_date'].astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "ride_sharing['ride_date'] = ride_sharing.loc[ride_sharing['ride_date'] > today_date, 'ride_date'] = today_date\n",
    "ride_sharing['ride_date'] = ride_sharing[ride_sharing['ride_date']>today_date]['ride_date'] = today_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d47f4-a445-4b81-aec4-a470efe94816",
   "metadata": {},
   "source": [
    "# Your thinking makes you smarter, not stareing at the answer, you learn nothing in looking at it, but learn a lot in figuring it out.  Choose the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae977680-7059-4d50-ab84-7e48225d9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to datetime\n",
    "ride_sharing['ride_dt'] = pd.____(____['____'])\n",
    "\n",
    "# Save today's date\n",
    "today = ____\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.____[____['____'] > ____, '____'] = ____\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].____())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5865b6c-ef93-4273-a400-e04e00372d6e",
   "metadata": {},
   "source": [
    "## Uniqueness constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Hi and welcome to the final lesson of this chapter.  Lets discuss another common data clearning problem, duplicate values.  Duplicate values can be diagnosed when we have the same exact information repeated across multiple row, for some or all column of our DF.  \n",
    "\n",
    "In this example DF containing the names, address, height, and weight of individuals, the rows represented have identical values across all columns.  In this one there are duplicate values for all columns except the height column, which leads us to think its more likely a data entry error than an actural other person.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Apart from data entry and human errors alluded to in the previous slide, duplicate data can also arise because of bugs and design errors wheather in business processes or data pipelines.  However, they often most arise form the necessary act of joining and consolidating data from various resources, which could retain duplicate values.  \n",
    "\n",
    "\n",
    "# Lets first see how to find duplicate values.  \"df.duplicated()\" method --> boolean indexing\n",
    "In this example we're working with a bigger version of the height and weight data seen earlier in this video.  We can find duplicates in a DF by using \"df.duplicated()\" method.  It returns a Series of boolean values that are True for duplicated values, and False for non-duplicated values.  \n",
    "We can see exactly which rows are affected by using boolean indexing (df[df.duplicated()]).  However using \"df.duplicated()\" without playing around with the arguments of the method can lead to misleading results, as all of the columns are required to have duplicated values by default, with all duplicated values being marked as True except for the first occurence.  \n",
    "# Here, can I recall how we do the DF merger and concatnate operations?  Think and think\n",
    "This limit our ability to properly diagnose what type of duplication we have, how to effectively treat it.  To properly calibrate how we go about finding duplicates, we will use 2 arguments from the \"df.duplicated()\" method.  The \"subset=\" argument lets us set a list of column names to check for duplication.  For example, it can allows us to find duplicates for the first and last name columns only.  The \"keep=\" argument lets us keep the first occurrence of a duplicate value by setting it to the tring \"first\", \"last\", or keep all occurances of duplicated values by setting it to \"False\".  In below example, we're checking for duplicates across the first name, last name and address columns\n",
    "and we choosing to keep all duplicates.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "We see the following results, to get a better bird's eye view of the duplicates, we sort the duplicated rows using \"df.sort_values(by='first_name')\" method, choosing \"first_name\" to sort by.  We find that there are 4 sets of duplicated rows, the first 2 being complete duplicates of each other across all columns.  The other 2 being incomplete duplicates of each other with discrepancies across height and weight respectively.  \n",
    "\n",
    "# \".drop_duplicates()\" method\n",
    "The complete duplicates can be treated easily.  All that required is keep one of them only and discard the others.  This can be down with the \".drop_duplicates()\" method, which also takes in the same \"subset=\" and \"keep=\" argument asin the \".duplicated()\" method, as well as the \"inplace=\" argument which drops the duplicated values directly inside the height_weight DF.  Here we are droping complete duplicates only, so its not necessary nor adviable to set a \"subset=\", and since the \"keep=\" argument takes in \"first\" as default, we can keep it as such.  Note that we can also set it as \"last\", but not as \"False\" as it would keep all duplicates.  \n",
    "\n",
    "This leaves us with the other 2 sets of duplicates discussed earlier, which are the same for first_name, last_name and address, but contain discrepancies in height and weight.  \n",
    "\n",
    "# Apart from droping rows with really small discrepancies, we can use a statistical measure to combine each set of duplicated values.  \n",
    "For example, we can combine these 2 rows into 1 by computing the average mean between them, or the maximum, or other statistical measures, this is highly dependent on a common sense understanding of our data, and what type of data we have.  We can do this easily using the \".groupby()\" method, which when chained with \".agg()\" method, let you group by a set of common columns and return statistical values for specific columns when the aggregation is being performed. \n",
    "\n",
    "For example here, we created a dictionary called summaries, which instructs groupby to return the maximum of duplicated rows for the height column, and the mean duplicated rows for the weight column.  \n",
    "We then \".groupby()\" height_weight by the column names defined earlier, and chained it with the \"agg()\" method, which takes in the summaries dictionary we created.  \n",
    "We chain this entire line with the \".reset_index()\" method, so that we can have numbered indices in the final output. \n",
    "We can verify that there are no more duplicate values by running the \".duplicated()\" method again, and use brackets to output duplicate rows.  \n",
    "\n",
    "\n",
    "Now that we have a solid grasp of dupliccation, lets practice.  \n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------\n",
    "first_name   | last_name   | address                                    | height   | weight\n",
    "Justin       | Saddlemyer  | Boulevard du Jardin Botainque 3, Bruxelles | 193 cm   | 87 kg\n",
    "Justin       | Saddlemyer  | Boulevard du Jardin Botainque 3, Bruxelles | 194 cm   | 87 kg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data Entry & Human Error\n",
    "\n",
    "Bugs and design errors\n",
    "\n",
    "Join or merge Errors\n",
    "\n",
    "\n",
    "\n",
    "# Column names to check for duplication\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)  #############################################\n",
    "\n",
    "print(height_weight[duplicates])\n",
    "\n",
    "----------------------------------------------------- Image what will be the output of it\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output duplicate values\n",
    "height_weight[duplicates].sort_values(by='first_name')   # Think why choosing f_name, ll_name, address set duplicate \n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "     first_name  | last_name  | address                              | height  | weight\n",
    " 22        Cole  |    Palmer  |                     8366 At, Street  |    178  | 91\n",
    "102        Cole  |    Palmer  |                     8366 At, Street  |    178  | 91\n",
    " 28     Desirae  |   Shannon  | P.O. Box 643, 5251 Consectetuer, Rd. |    195  | 83\n",
    "103     Desirae  |   Shannon  | P.O. Box 643, 5251 Consectetuer, Rd. |    196  | 83\n",
    "  1        Ivor  |    Pierce  |                   102-3364 Non. Road |    168  | 66\n",
    "101        Ivor  |    Pierce  |                   102-3364 Non. Road |    168  | 88\n",
    " 37        Mary  |     Colon  |                         4674 Ut Rd.  |    179  | 75\n",
    "100        Mary  |     Colon  |                         4674 Ut Rd.  |    179  | 75\n",
    "         \n",
    "         \n",
    "# Drop duplicates\n",
    "height_weight.drop_duplicates(inplace=True)  ########################################################################\n",
    "\n",
    "\n",
    "height_weight.groupby(['first_name', 'last_name', 'address']).agg(np.mean)  ??\n",
    "\n",
    "# Groupby column names and produce statistical summaries\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "summaries = {'height': 'max', 'weight': 'mean'}\n",
    "height_weight = height_weight.groupby(by=column_names).agg(summaries).reset_index()  ################################\n",
    "\n",
    "# Make sure aggregation is done\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)\n",
    "height_weight[duplicates].sort_values(by='first_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c53b81-edc2-42a3-b12c-1ed622b2a063",
   "metadata": {},
   "source": [
    "## How big is your subset?\n",
    "\n",
    "You have the following loans DataFrame which contains loan and credit score data for consumers, and some metadata such as their first and last names. You want to find both complete and incomplete duplicates using \".duplicated()\" method.\n",
    "first_name \tlast_name \t    credit_score \thas_loan\n",
    "Justin \t    Saddlemeyer \t600 \t        1\n",
    "Hadrien \tLacroix \t    450 \t        0\n",
    "\n",
    "Choose the correct usage of \".duplicated()\" below:\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    loans.duplicated()\n",
    "      Because the default method returns both complete and incomplete duplicates.  X Full duplicates in every col\n",
    "    press\n",
    "    1\n",
    "    loans.duplicated(subset = 'first_name')\n",
    "      Because constraining the duplicate rows to the first name lets me find incomplete duplicates as well.  X\n",
    "    press\n",
    "    2\n",
    "    loans.duplicated(subset = ['first_name', 'last_name'], keep = False)\n",
    "      Because subsetting on consumer metadata and not discarding any duplicate returns all duplicated rows.\n",
    "    press   X set \"keep=\" arg to False will keep all duplicates\n",
    "    3\n",
    "#    loans.duplicated(subset = ['first_name', 'last_name'], keep = 'first')\n",
    "      Because this drops all duplicates.\n",
    "    press\n",
    "    4\n",
    "    \n",
    "# The value we set for \"keep=\" arg is for \"mark\" boolean, not keep the records\n",
    "# *******************************************************************************************************************\n",
    "    keep : {'first', 'last', False}, default 'first'\n",
    "        Determines which duplicates (if any) to mark.\n",
    "    \n",
    "        - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n",
    "        - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n",
    "        - False : Mark all duplicates as ``True``.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a93835-bfd1-41c9-b581-403a0c6f59fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function duplicated in module pandas.core.frame:\n",
      "\n",
      "duplicated(self, subset: 'Hashable | Sequence[Hashable] | None' = None, keep: \"Literal['first'] | Literal['last'] | Literal[False]\" = 'first') -> 'Series'\n",
      "    Return boolean Series denoting duplicate rows.\n",
      "    \n",
      "    Considering certain columns is optional.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    subset : column label or sequence of labels, optional\n",
      "        Only consider certain columns for identifying duplicates, by\n",
      "        default use all of the columns.\n",
      "    keep : {'first', 'last', False}, default 'first'\n",
      "        Determines which duplicates (if any) to mark.\n",
      "    \n",
      "        - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n",
      "        - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n",
      "        - False : Mark all duplicates as ``True``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series\n",
      "        Boolean series for each duplicated rows.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Index.duplicated : Equivalent method on index.\n",
      "    Series.duplicated : Equivalent method on Series.\n",
      "    Series.drop_duplicates : Remove duplicate values from Series.\n",
      "    DataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Consider dataset containing ramen rating.\n",
      "    \n",
      "    >>> df = pd.DataFrame({\n",
      "    ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
      "    ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
      "    ...     'rating': [4, 4, 3.5, 15, 5]\n",
      "    ... })\n",
      "    >>> df\n",
      "        brand style  rating\n",
      "    0  Yum Yum   cup     4.0\n",
      "    1  Yum Yum   cup     4.0\n",
      "    2  Indomie   cup     3.5\n",
      "    3  Indomie  pack    15.0\n",
      "    4  Indomie  pack     5.0\n",
      "    \n",
      "    By default, for each set of duplicated values, the first occurrence\n",
      "    is set on False and all others on True.\n",
      "    \n",
      "    >>> df.duplicated()\n",
      "    0    False\n",
      "    1     True\n",
      "    2    False\n",
      "    3    False\n",
      "    4    False\n",
      "    dtype: bool\n",
      "    \n",
      "    By using 'last', the last occurrence of each set of duplicated values\n",
      "    is set on False and all others on True.\n",
      "    \n",
      "    >>> df.duplicated(keep='last')\n",
      "    0     True\n",
      "    1    False\n",
      "    2    False\n",
      "    3    False\n",
      "    4    False\n",
      "    dtype: bool\n",
      "    \n",
      "    By setting ``keep`` on False, all duplicates are True.\n",
      "    \n",
      "    >>> df.duplicated(keep=False)\n",
      "    0     True\n",
      "    1     True\n",
      "    2    False\n",
      "    3    False\n",
      "    4    False\n",
      "    dtype: bool\n",
      "    \n",
      "    To find duplicates on specific column(s), use ``subset``.\n",
      "    \n",
      "    >>> df.duplicated(subset=['brand'])\n",
      "    0    False\n",
      "    1     True\n",
      "    2    False\n",
      "    3     True\n",
      "    4     True\n",
      "    dtype: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.duplicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686092f6-607d-4d0e-926d-8ec9f3ace435",
   "metadata": {},
   "source": [
    "## Finding duplicates\n",
    "\n",
    "# A new update to the data pipeline feeding into \"ride_sharing\" has added the \"ride_id\" column, which represents a unique identifier for each ride.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the \"ride_sharing\" DataFrame.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "In this exercise, you will confirm this suspicion by finding those duplicates. A sample of \"ride_sharing\" is in your environment, as well as all the packages you've been working with thus far.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Find duplicated rows of \"ride_id\"  in the \"ride_sharing\" DataFrame while setting \"keep=\" arg to False.\n",
    "    Subset \"ride_sharing\" on duplicates and sort by \"ride_id\" and assign the results to \"duplicated_rides\".\n",
    "    Print the \"ride_id\", \"duration\" and \"user_birth_year\" columns of \"duplicated_rides\" in that order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de25e1-3624-4ec0-bec2-b5aa1a3005fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)\n",
    "\n",
    "\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "\n",
    "print(duplicated_rides[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a2ecb-7a3a-49f8-ab98-8fd1fa5288ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ____.____(____, ____)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[____].____('____')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['____','____','____']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbab60-bff4-4752-a62a-fe4a09ff1493",
   "metadata": {},
   "source": [
    "## Treating duplicates\n",
    "\n",
    "In the last exercise, you were able to verify that the new update feeding into \"ride_sharing\" contains a bug generating both complete and incomplete duplicated rows for some values of the \"ride_id\" column, with occasional discrepant values for the \"user_birth_year\" and \"duration\" columns.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average duration, and the minimum user_birth_year for each set of incomplete duplicate rows.\n",
    "# *******************************************************************************************************************\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Drop complete duplicates in \"ride_sharing\" and store the results in \"ride_dup\".\n",
    "#    Create the statistics dictionary which holds minimum aggregation for \"user_birth_year\" and mean aggregation for \"duration\".\n",
    "#    Drop incomplete duplicates by grouping by \"ride_id\" and applying the aggregation in statistics.\n",
    "    Find duplicates again and run the assert statement to verify de-duplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973867c5-89d4-478a-abb8-d88a0e0c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_dup = ride_sharing.drop_duplicates()  # No need set \"inplace=\" arg to True as we passing it top a varianble\n",
    "\n",
    "\n",
    "summaries = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(summaries).reset_index()\n",
    "\n",
    "\n",
    "duplicates = ride_unique.duplicated(subset='ride_id', keep=False)\n",
    "\n",
    "duplicates_rides = ride_unique[duplicates]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daf0a4-c4a3-4adb-8c8c-bdf23d1e1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ____.____()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': ____, 'duration': ____}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.____('____').____(____).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.____(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0  ##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa1f01-de87-460c-99a4-13cff9de7c5b",
   "metadata": {},
   "source": [
    "## Membership constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Good work on chapter 1.  We are now equipped to treat more complex, and specific data cleaning problems.  In this chapter, we're going to take a look at common data problems with text and categorical data.  So lets get started.  \n",
    "\n",
    "In this lesson, we'll focus on categorical variables.  As discussed early in chapter 1, categorical data represent variables that represent predefined finite set of categories.  Examples of this range from marriage status, household income categories, loan status and others.  \n",
    "\n",
    "# To run machine learning models on categorical data, they are often coded as numbers.  \n",
    "Since categorical data represent a predefined set of categories, they can't have values that go beyond these predefined categories.  We can have inconsistencies in our categorical data for a variety of reasons.  This could be due to data entry issues with free text vs dropdown fields, data parsing errors and other types of errors.  \n",
    "\n",
    "There's a variety of ways we can treat these, with increasingly specific solutions for different types of inconsistencies.  Most simply, we can drop the rows with incorrect categories.  We can attempt remapping incorrect categories to correct ones, and more.  We'll see a variety of ways of dealing with this throughout the chapter and the course, but for now we'll just focus on dropping data.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets first look at an example. Here is a DataFrame named study_data containing a list of first names, birth dates, and blood types.  Additional, a DataFrame named categories, containing the correct possible categories for the bloodtype column has been created as well.  Notice the inconsistency here?  There's definitely no bloodtype named Z+.  Luckily, the categories DataFrame will help us systematically spot all rows with these inconsistencies.  \n",
    "\n",
    "# Its always good practice to keep a log of all possible values of your categorical data, as it will make dealing with these types of inconsistencies way easier.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Now before moving on to dealing with these inconsistent values, lets have a brief reminder on joins.  The two main types of joins we care about here are anti-joins and inner-joins.  We join DataFrames on common columns between them. \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The anti-joins take in 2 DataFrames A and B, and return data from 1 DataFrame that is not contained in another. \n",
    "Imagine a example we performing a left anti-joins of DF A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on.  \n",
    "\n",
    "Inner-joins returns only the data that is contained DataFrames.  For example, an inner-join of A and B would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "In our example, an left anti-join essentially returns all the data in study data with inconsistent bloodtypes, and an inner-join returns all the rows containing consistent bloodtypes.  Now lets see how to do that in Python.  We first gety all inconsistent categories in the blood_type column of the study_data DataFrame.  We do that by creating a set of the blood_type column which stores its unique values, and use the \".difference()\" method which takes in as argument the blood_type column from the categories DataFrame.  This returns all the categories in blood_type that are not in categories.  We then find the inconsistent rows by finding all the rows of the blood_type columns that are equal to inconsistent categories by using the \".isin()\" method, this returns a series of boolean values that are True for inconsistent rows and False for consistent ones.  We then subset the study_data DF based on the boolean indexing, and viola we have our inconsistent data.  \n",
    "\n",
    "To drop inconsistent rows and keep ones that are only consistent.  We just use the tilde symbol (~) while subsetting which rturns everything except inconsistent rows.  \n",
    "\n",
    "\n",
    "Now thatwe know about treating categorical data, lets practice.  \n",
    "\n",
    "\n",
    "\n",
    "# Read study data and print in\n",
    "study_data = pd.read_csv('study.csv')\n",
    "print(stydy_data)\n",
    "\n",
    "-------------------------------------------------\n",
    "    name       | birthday     | bloodtype\n",
    "1   Beth       | 2019-10-20   | B-\n",
    "2   Ignatius   | 2020-07-08   | A-\n",
    "3   Paul       | 2019-08-12   | O+\n",
    "4   Helen      | 2019-03-17   | O-\n",
    "5   Jennifer   | 2019-12-17   | Z+   <---\n",
    "6   Kennedy    | 2020-04-27   | A+\n",
    "7   Keityh     | 2019-04-19   | AB+\n",
    "\n",
    "\n",
    "# Correct possible blood types\n",
    "print(categories)\n",
    "\n",
    "-----------------------\n",
    "    bloodtype\n",
    "1   O-\n",
    "2   O+\n",
    "3   A-\n",
    "4   A+\n",
    "5   B+\n",
    "6   B-\n",
    "7   AB+\n",
    "8   AB-\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "inconsistent_categories = set(study_data['bloodtype']).difference(categories['bloodtype'])  #########################\n",
    "print(inconsistent_categories)\n",
    "\n",
    "{'z'}\n",
    "\n",
    "# Get and print rows with inconsistent categories\n",
    "inconsistens_rows = study_data['bloodtype'].isin(inconsistent_categories)\n",
    "\n",
    "inconsistene_data = study_data[inconsistent_rows]\n",
    "\n",
    "# Drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4afd28-8a3b-41ab-8105-1aa1a61934b8",
   "metadata": {},
   "source": [
    "## Members only\n",
    "\n",
    "Throughout the course so far, you've been exposed to some common problems that you may encounter with your data, from data type constraints, data range constrains, uniqueness constraints, and now membership constraints for categorical values.\n",
    "\n",
    "In this exercise, you will map hypothetical problems to their respective categories.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map the data problem observed with the correct type of data problem.\n",
    "\n",
    "Hint\n",
    "\n",
    "#    Remember, a membership constraint is when a categorical column has values that are not in the predefined set of categories of your column.\n",
    "\n",
    "Other Constraint: \n",
    "     A \"revenue\" column represented as a string\n",
    "     A \"birthdate\" column with value in the future\n",
    "     An \"age\" column with value above \"130\"\n",
    "\n",
    "\n",
    "Membership Constraint:\n",
    "     A \"month\" column with the value \"14\"\n",
    "     A \"day_of_week\" column with the value \"Suntermonday\"\n",
    "     A \"GPA\" column containing a \"z-\" grade\n",
    "     A \"has_loan\" column with the value \"12\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28556e-d74e-41e3-8c6b-62f9d05f4478",
   "metadata": {},
   "source": [
    "## Finding consistency\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this exercise and throughout this chapter, you'll be working with the airlines DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named categories was created, containing all correct possible values for the survey columns.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise. The pandas package has been imported as pd, and the airlines and categories DataFrames are in your environment.\n",
    "Instructions 1/4\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "#    Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.\n",
    "#    Print the unique values of the survey columns in airlines using the \".unique()\" method.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Print the unique values of the survey columns in airlines using the .unique() method.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Find rows of airlines with a cleanliness value not in categories and print the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d166ab-1eb1-4535-a087-7e5489cbf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_val_lis = {'col_key1': ['val_11', 'val_12', 'val_13', 'val_14'],\n",
    "                'col_key2': ['val_21', 'val_22', 'val_23', 'val_24'],\n",
    "                'col_key3': ['val_31', 'val_32', 'val_33', 'val_34']}\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "list_dict = [{'country': 'Brazil', 'capital': 'Brasilia', 'area': 8.516, 'population': 200.4}, \n",
    "             {'country':'Russia', 'capital':'Moscow', 'area':17.10, 'population':143.5}, \n",
    "             {'country':'India', 'capital':'New Delhi', 'area':3.286, 'population':1252}, \n",
    "             {'country':'China', 'capital':'Beijing', 'area':9.597, 'population':1357}, \n",
    "             {'country':'South Africa','capital':'Pretoria','area':1.221, 'population':52.98}]\n",
    "\n",
    "import pandas as pd\n",
    "putcome = pd.DataFrame(list_dict)\n",
    "print(putcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7e8e25d-7700-4e54-b309-99f3250c0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        East US       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  \n",
      "0      Very satisfied  \n",
      "1      Very satisfied  \n",
      "2             Neutral  \n",
      "3  Somewhat satsified  \n",
      "4  Somewhat satsified   \n",
      "\n",
      "          id       day        airline destination dest_region dest_size  \\\n",
      "2805  2222.0  Thursday      SOUTHWEST     PHOENIX     West US       Hub   \n",
      "2806  2684.0    Friday         UNITED     ORLANDO     East US       Hub   \n",
      "2807  2549.0   Tuesday        JETBLUE  LONG BEACH     West US     Small   \n",
      "2808  2162.0  Saturday  CHINA EASTERN     QINGDAO        Asia     Large   \n",
      "0        NaN       NaN            NaN         NaN         NaN       NaN   \n",
      "\n",
      "     boarding_area   dept_time  wait_min cleanliness         safety  \\\n",
      "2805   Gates 20-39  2018-12-31     165.0       Clean      Very safe   \n",
      "2806   Gates 70-90  2018-12-31      92.0       Clean      Very safe   \n",
      "2807    Gates 1-12  2018-12-31      95.0       Clean  Somewhat safe   \n",
      "2808    Gates 1-12  2018-12-31     220.0       Clean      Very safe   \n",
      "0              NaN         NaN       NaN        cool   good quality   \n",
      "\n",
      "            satisfaction  \n",
      "2805      Very satisfied  \n",
      "2806      Very satisfied  \n",
      "2807      Very satisfied  \n",
      "2808  Somewhat satsified  \n",
      "0              best ever   \n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty' 'cool']\n",
      "['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'\n",
      " 'good quality']\n",
      "['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied' 'best ever'] \n",
      "\n",
      "      cleanliness           safety          satisfaction\n",
      "0           Clean        Very safe        Very satisfied\n",
      "1  Somewhat clean    Somewhat safe    Somewhat satsified\n",
      "2         Average          Neutral               Neutral\n",
      "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
      "4           Dirty  Somewhat unsafe      Very unsatisfied \n",
      "\n",
      "{'cool'}\n",
      "2804    False\n",
      "2805    False\n",
      "2806    False\n",
      "2807    False\n",
      "2808    False\n",
      "0        True\n",
      "Name: cleanliness, dtype: bool \n",
      "\n",
      "          id       day        airline   destination dest_region dest_size  \\\n",
      "2804  1475.0   Tuesday         ALASKA  NEW YORK-JFK     East US       Hub   \n",
      "2805  2222.0  Thursday      SOUTHWEST       PHOENIX     West US       Hub   \n",
      "2806  2684.0    Friday         UNITED       ORLANDO     East US       Hub   \n",
      "2807  2549.0   Tuesday        JETBLUE    LONG BEACH     West US     Small   \n",
      "2808  2162.0  Saturday  CHINA EASTERN       QINGDAO        Asia     Large   \n",
      "\n",
      "     boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "2804   Gates 50-59  2018-12-31     280.0  Somewhat clean        Neutral   \n",
      "2805   Gates 20-39  2018-12-31     165.0           Clean      Very safe   \n",
      "2806   Gates 70-90  2018-12-31      92.0           Clean      Very safe   \n",
      "2807    Gates 1-12  2018-12-31      95.0           Clean  Somewhat safe   \n",
      "2808    Gates 1-12  2018-12-31     220.0           Clean      Very safe   \n",
      "\n",
      "            satisfaction  \n",
      "2804  Somewhat satsified  \n",
      "2805      Very satisfied  \n",
      "2806      Very satisfied  \n",
      "2807      Very satisfied  \n",
      "2808  Somewhat satsified  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv', index_col=0)\n",
    "print(airlines.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "dirty_airlines = pd.DataFrame({'cleanliness': ['cool'], 'safety': ['good quality'], 'satisfaction': ['best ever']})\n",
    "\n",
    "airlines = pd.concat([airlines, dirty_airlines])#, ignore_index=True)\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "print(airlines.tail(), \"\\n\")\n",
    "\n",
    "\n",
    "print(type(airlines['cleanliness'].unique()))\n",
    "print(airlines['cleanliness'].unique())\n",
    "print(airlines['safety'].unique())\n",
    "print(airlines['satisfaction'].unique(), \"\\n\")\n",
    "\n",
    "\n",
    "categories = pd.DataFrame({'cleanliness': ['Clean', 'Somewhat clean', 'Average', 'Somewhat dirty', 'Dirty'], \n",
    "                          'safety': ['Very safe', 'Somewhat safe', 'Neutral', 'Very unsafe', 'Somewhat unsafe'], \n",
    "                          'satisfaction': ['Very satisfied', 'Somewhat satsified', 'Neutral', 'Somewhat unsatisfied', 'Very unsatisfied']})\n",
    "\n",
    "print(categories, '\\n')\n",
    "\n",
    "#print(airlines[['cleanliness', 'safety', 'satisfaction']].unique())\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# So lets image we'll drop every row when a column has inconsistent value\n",
    "# *******************************************************************************************************************\n",
    "bad_cleanliness = set(airlines['cleanliness']).difference(categories['cleanliness'])  ###############################\n",
    "print(bad_cleanliness)\n",
    "\n",
    "inconsistens_cleanliness_rows = airlines['cleanliness'].isin(bad_cleanliness)\n",
    "\n",
    "print(inconsistens_cleanliness_rows[-6:], '\\n')\n",
    "\n",
    "good_cleanliness = airlines[~inconsistens_cleanliness_rows]\n",
    "print(good_cleanliness.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8799349-f287-42eb-9709-6419ba8ff593",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Find the cleanliness category in airlines not in categories\n",
    "    cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "    # Find rows with that category\n",
    "    cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "    # Print rows with inconsistent category\n",
    "    print(airlines[cat_clean_rows])\n",
    "\n",
    "    -> airlines의 cleanliness 컬럼의 중복을 제거한 리스트로 만들어, 차집합을 통해 겹치지 않는 값을\n",
    "    cat_clean에 저장한다. 이때 겹치는 값을 제외한 Unacceptable만 저장된다.\n",
    "\n",
    "    -> 그리고 cleanliness 컬럼의 값이 cat_clean에 포함되어있는지를 불리언의 형태로\n",
    "    cat_clean_rows에 저장하였다.\n",
    "\n",
    "    -> 마지막으로 이 값들이 포함되어있는 행들을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f99ad-2e03-44ee-9acf-22e63ee6f779",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Awesome work on the last lesson.  Now lets discuss other types of problems that could affect categorical variables.  In the last lesson, we saw how categorical data has a value membership constraint, where columns need to have a predefined set of values.  However this is not the only set of problems we may encounter.  \n",
    "\n",
    "When cleaning categorical data, some of the problems we may encounter include value inconsistency, the presence of too many categories that could be collapsed into one, and making sure data is of the right type.  Lets start with making sure our categorical data is consistent.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# A common categorical data problem is having values that slightly differ because of capitalizations.  \n",
    "Not treating this could lead tyo misleading results when we dicide to analyze the data, for example, lets assume we're working with a demographics dataset, and we have a marriage status column with inconsistent capitalization.  Below is what counting the number of married people in the marriage_status Series would look like.  \n",
    "\n",
    "Note that \".value_counts()\" method works on Series only.  For a DF, we can \"df.groupby()\" the column and use the \".count()\" method.  \n",
    "\n",
    "To deal with this, we can either capitalize or lowercase the marriage_status column.  This can be done with the \"str.upper()\" or \"str.lower()\" function respectively.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Another common problem with categorical values are leading or trailing spaces.  \n",
    "For example, imagine the same demographics DF containing values with leading spaces.  Below is what the counts of married vs unmarried people would look like.  Note that there is a married category with a trailing space on the right, which makes it hard to sport on the output, as opposed to unmarried.  \n",
    "\n",
    "To remove leading and trailing spaces, we can use the \"str.strip()\" method, which when giving no input, strips all leading and trailing white spaces.  \n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Sometimes, we may want to create categories out of our data, such as creating household income groups from income data.  \n",
    "To create categories out of data, lets use the example of creating an income group column in the demographics DataFrame.  (do you remember how we did this using \"pd.cut()\" & \"pd.qcut()\")  \n",
    "\n",
    "# \"pd.qcut(df['column'], q=, labels=)\"\n",
    "We can do this in 2 ways.  The first method utilizes the \"pd.qcut()\" function from Pandas, which automatically divides our data based on its distribution into the number of categories we set in the \"q=\" argument, we created the category names in the group_names list and fed it to \"labels=\" argument, returning the following.  \n",
    "\n",
    "Notice the first row actually misrepresents the actual income of the income group, as we didn't instruct qcut where our ranges actually lie.  \n",
    "\n",
    "# \"pd.cut(df['column'], bins=, labels=)\"\n",
    "We can do this with \"pd.cut()\" function instead, which lets us define category cutoff ranges with the bins argument.  It takes in a list of cutoff points for each category, with the final one being infinity represented with \"np.inf()\" from NumPy.  From the output, we can we can see that this is much more correct.  \n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Sometimes, we may want to reduce the amount of categories we have in our data.  Lets move on to mapping categories to fewer ones.  \n",
    "\n",
    "For example, assume we have a column containing the operating system of different devices, and contains these unique values: Microsoft, MacOS, IOS, Android, Linux.  Say we want to collapse these categories into 2, DesktopOS, and MobileOS.  \n",
    "\n",
    "# We can do this by using the \".replace()\" method.  \n",
    "It takes in a dictionary that maps each existing category to the category name you desire.  In this case, this is a mapping dictionary.  A quick print of the unique values of operating system shows the mapping has been complete.  \n",
    "\n",
    "\n",
    "Now that we know about treating categorical data, lets practice.  \n",
    "\n",
    "\n",
    "\n",
    " (1) Value inconsistency\n",
    "     Inconsistent fields: 'married', 'Maried', 'UNMARRIED', 'not married' ...\n",
    "     Trailing white spaces: 'married', ' married '\n",
    "(11) Collapsing too many categories to few\n",
    "     Creating new groups: \"0-20k\", \"20-40k\" categories from continuous household income data\n",
    "     Mapping groups to new ones: Mapping household income categories to \"rich\", \"poor\"\n",
    "\n",
    "\n",
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "\n",
    "marriage_status.value_counts()\n",
    "\n",
    "unmarried   352\n",
    "married     268\n",
    "MARRIED     204\n",
    "UNMARRIED   176\n",
    "dtype: int64\n",
    "\n",
    "\n",
    " unmarried   352\n",
    "married      268\n",
    "MARRIED      204\n",
    "UNMARRIED    176\n",
    "dtype: int64\n",
    "\n",
    "\n",
    "\n",
    "# Using  \"pd.qcut()\"\n",
    "group_names = ['0-200k', '200-500k', '500k+']\n",
    "\n",
    "demographics[\"income_group\"] = pd.qcut(demographics['household_income'], q=3, labels=group_names)\n",
    "\n",
    "# Print income_group column\n",
    "demographics[['income_group', 'household_income']]\n",
    "\n",
    "-----------------------------------------\n",
    "    income_group  | household_income\n",
    "    200-500k      | 189243\n",
    "    500k+         | 778533\n",
    "    \n",
    "\n",
    "\n",
    "# Using \"pd.cut()\" - creat category ranges and names\n",
    "ranges = [0, 200000, 500000, np.inf]\n",
    "\n",
    "group_names = ['0-200k', '200-500k', '500k+']\n",
    "\n",
    "# Create income group column\n",
    "demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
    "\n",
    "-----------------------------------------\n",
    "    income_group  | household_income\n",
    "    0-200k        | 189243\n",
    "    500k+         | 778533\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a mapping dictionary and replace\n",
    "mapping = {\"Microsoft\": \"DesktopOS\", \"MacOS\": \"DesktopOS\", \"Linux\": \"DesktopOS\", \"IOS\": \"MobileOS\", \"Android\": \"MobileOS\"}\n",
    "\n",
    "devices[\"operating_system\"] = devices[\"operating_system\"].replace(mapping) ##########################################\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "devices[\"operating_system\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bcecf-9e21-4829-94ff-14e12af6a85c",
   "metadata": {},
   "source": [
    "## Categories of errors\n",
    "\n",
    "In the video exercise, you saw how to address common problems affecting categorical variables in your data, including white spaces and inconsistencies in your categories, and the problem of creating new categories and mapping existing ones to new ones.\n",
    "\n",
    "To get a better idea of the toolkit at your disposal, you will be mapping functions and methods from pandas and Python used to address each type of problem.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map each function/method to the categorical data problem it solves.\n",
    "\n",
    "White spaces and inconsistency:\n",
    "     .str.upper()\n",
    "     .str.lower()\n",
    "     .str.strip()\n",
    "\n",
    "Creating or remapping categories:\n",
    "     pd.cut()\n",
    "     .replace()\n",
    "     pd.qcut()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee7ece-abbd-48cb-a60f-c28674df7348",
   "metadata": {},
   "source": [
    "##  Inconsistent categories\n",
    "\n",
    "In this exercise, you'll be revisiting the airlines DataFrame from the previous lesson.\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport.\n",
    "\n",
    "In this exercise, you will examine two categorical columns from this DataFrame, dest_region and dest_size respectively, assess how to address them and make sure that they are cleaned and ready for analysis. The pandas package has been imported as pd, and the airlines DataFrame is in your environment.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the unique values in \"dest_region\" and \"dest_size\" respectively.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    From looking at the output, what do you think is the problem with these columns?\n",
    "    Possible Answers\n",
    "    The \"dest_region\" column has only inconsistent values due to capitalization.\n",
    "#    The \"dest_region\" column has inconsistent values due to capitalization and has one value that needs to be remapped.    yes, the 'euro' should be mapped to 'Europe'\n",
    "    The dest_size column has only inconsistent values due to leading and trailing spaces.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Change the capitalization of all values of \"dest_region\" to lowercase.\n",
    "#    Replace the 'eur' with 'europe' in dest_region using the \".replace()\" method.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "#    Strip white spaces from the \"dest_size\" column using the \".strip()\" method.\n",
    "#    Verify that the changes have been into effect by printing the unique values of the columns using .unique() .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1a7758-e6a6-4ce1-8f43-63364abd6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    id        day      airline        destination    dest_region  \\\n",
      "0           0  1351    Tuesday  UNITED INTL             KANSAI           Asia   \n",
      "1           1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
      "2           2  2820   Thursday        DELTA        LOS ANGELES        West US   \n",
      "3           3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US   \n",
      "4           4  2992  Wednesday     AMERICAN              MIAMI        East US   \n",
      "\n",
      "  dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
      "0       Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
      "1     Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
      "2       Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
      "3       Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
      "4       Hub   Gates 50-59  2018-12-31     559.0  Somewhat clean   \n",
      "\n",
      "          safety        satisfaction  \n",
      "0        Neutral      Very satisfied  \n",
      "1      Very safe      Very satisfied  \n",
      "2  Somewhat safe             Neutral  \n",
      "3      Very safe  Somewhat satsified  \n",
      "4      Very safe  Somewhat satsified   \n",
      "\n",
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east'] \n",
      "\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     '] \n",
      "\n",
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv')\n",
    "print(airlines.head(), '\\n')\n",
    "\n",
    "\n",
    "print(airlines['dest_region'].unique(), '\\n')\n",
    "\n",
    "print(airlines['dest_size'].unique(), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "\n",
    "\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur': 'europe'})  #######################################\n",
    "\n",
    "\n",
    "print(airlines['dest_region'].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bdb0e-323e-412f-8f38-b9f6821bb9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b33e9-e9d2-4109-bc9f-096205e7e7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2a496-58d1-4a3e-97b4-70c8635ab3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea39437-9f47-4c18-9595-38ad09592728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7cd2b-ca7b-423d-9d52-e42c741e8e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68014c2-68eb-4660-b479-9d36855a0b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b5aae-c927-4c0d-b80b-06704323ef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3daf1b8-4b85-4545-a07d-8876d6f25dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a8957-dae6-4313-ba01-0f89a6434d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a84a05-ee49-4a27-a737-bc3636168454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094aabf3-dbb9-4891-bc84-215be59c11ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bf200-a450-421c-888e-2fe940c1da4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a61272-f84d-45b7-8df3-0d3fa27a2d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2cb7ea-3871-4a30-a9e4-f92d75beb668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82287eff-1652-47e7-acaa-b8cf573c3038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c4692-e79f-4288-8dc3-33d4b53ec65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21b64a-60d4-43e4-a5bf-6f4d441b371e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d297b-8ea3-4c11-b672-fbcb3f4aebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088555d4-2b5d-4704-a1b7-4fd7274c8eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf0a26-1290-4e3d-ac7b-205450c6d253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca095c-134e-4bce-99af-809fd645e325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494659c2-bc5e-44da-80c2-e48245ca814a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777b8f0-d110-41bd-a000-d1d30f2e89da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779a711-a27f-4d8e-9531-717508754c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628da8c-6fac-47f3-9316-290b59e27721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9b3ad-1738-4f84-b990-87d2629c6d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a03198-ec16-49c0-b67e-9be584dac693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f182d-c4b1-4d68-b49c-0bae6f75baa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b968c2-3eac-40c4-85f3-cb85469c0acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca2fd7-2c2c-4d10-8985-a4926c586488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1a7ea-fb05-4f0a-b4e2-3bae2be84b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e3110-ff97-473a-bc08-341156ed22c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119e39-5965-407b-8278-b4d4ccb017de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f894985-7dab-4dcd-b945-c6085eea9630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c8bd6-1e7c-4e97-851d-f20f5a6cd4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c1f7f-14a8-400f-859e-0a1d7c00062f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212483f-04a3-4a41-b09a-2ebf8e83bf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73bd84-821f-42cf-a953-1c8d5663676b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0c4de-ef37-4e8c-ac52-331ecf20d20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abc35c-15f2-4b53-8d30-95a365f3d4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163c4f9-0c73-4f02-b2a3-72f4c0d5a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b925f9a-51e1-4339-90c5-15f7276f75a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d50508-2483-41fd-95aa-7c51dcddb852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf82d1-7be0-4ec1-bd7a-e4c022d767c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6853a29-2b7c-4033-980f-440b9116db18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d33e7-9a26-45a5-928a-4f9137977e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051bff-4379-4701-98d0-0f3dbf613040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a6391-c35c-430f-9203-6caec9024446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b43593-9a0b-4fcc-8b05-8c1eb1892b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b26f54-248b-453a-aa99-c5804e119c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db9e3f-5fed-41fb-bd6c-63a64fb644c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d7963-0b1b-4576-97bf-caebae057ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72ec58-1544-434e-8c7c-e88e40615894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c5de3-dcc2-4c0b-9ec0-21946e3dc558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3e77f-98af-459f-b2ed-ab5c3bddf34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db7f19-7f56-4cbd-97be-8597992bd68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777d141-90c8-459c-ba5b-f6539ff59088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaeabee-e0c6-41a0-b377-b74fe8b1e7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff3cd0-37ac-426d-8a45-562daa94bdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013140f-8985-4d92-902a-c2383849979c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999515a-bfd3-4fd6-a31a-2d1a6982f51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860fbe6-7026-4b49-9518-d1fbd6ced516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e4ea8-0183-43cc-86f6-1c9fcc14e4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b487ff0-887d-4c9d-8a4d-c4af744db139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db13c8-52d9-43d5-8963-cd6539510604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d5121-c7ea-4f24-87f8-172bbcc108c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020aad0-e3fa-41e0-a94a-4dcffda750f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a121467-deb3-49d9-bd6e-542174ec69e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a058cae-530f-45e7-8587-bf7b9b2f3a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcc880-cab3-4d1b-b81d-5000d8c6d23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c0733-5df6-40a5-93af-1d2778e3d9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc24ab-5204-4993-b63e-3d4b57590586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055cd25-e46b-47a4-95e7-2b53140307d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7f94a-0af1-436f-8742-350c3b3046e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cada37-4ed1-4754-ac80-658422f980e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326b982-02fb-44af-b33f-1d929149d879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b1a1e-8236-4225-a5ad-e061d1a5c478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e6382-a26a-4913-9fde-3a3bf275773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac01ea-72c2-4816-b086-18db3f58a9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30226e6f-4314-4dd5-8cb1-5b629ee1bcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88e088-53c6-4ac8-a6b0-d0e683629b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51eae4-451e-407a-9189-bc0ca9e6a412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c926df5-24ba-46f6-9c88-25df7a76b16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052eaea-8c07-4458-a688-9d69ac88978d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
