{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdb6c41-92b9-47b2-b660-7e6a4b1e24c2",
   "metadata": {},
   "source": [
    "## Cleaning Data in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055a7c9-8970-4132-829c-2b27d95c641c",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "[It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it]. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions. Data cleaning is an essential task in data science. Without properly cleaned data, the results of any data analysis or machine learning model could be inaccurate. In this course, you will learn how to identify, diagnose, and treat a variety of data cleaning problems in Python, ranging from simple to advanced. You will deal with improper data types, check that your data is in the correct range, handle missing data, perform record linkage, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef6cb7-259a-431e-90c6-1daf990df19d",
   "metadata": {},
   "source": [
    "##  Common data problems\n",
    "Free\n",
    "0%\n",
    "\n",
    "In this chapter, you'll learn how to overcome some of [the most common dirty data problems]. You'll convert data types, apply range constraints to remove future data points, and remove duplicated data points to avoid double-counting.\n",
    "\n",
    "[    Data type] constraints    50 xp\n",
    "    Common data types    100 xp\n",
    "    Numeric data or ... ?    100 xp\n",
    "    Summing strings and concatenating numbers    100 xp\n",
    "[    Data range] constraints    50 xp\n",
    "    Tire size constraints    100 xp\n",
    "    Back to the future    100 xp\n",
    "    Uniqueness constraints    50 xp\n",
    "    How big is your subset?    50 xp\n",
    "[    Finding duplicates]   100 xp\n",
    "    Treating duplicates    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781858e-0a46-4241-98dd-6547f3c63dc5",
   "metadata": {},
   "source": [
    "##  Text and categorical data problems\n",
    "0%\n",
    "\n",
    "Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, you’ll learn how to [fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency].\n",
    "\n",
    "    Membership constraints    50 xp\n",
    "    Members only    100 xp\n",
    "    Finding consistency    100 xp\n",
    "    Categorical variables    50 xp\n",
    "    Categories of errors    100 xp\n",
    "    Inconsistent categories    100 xp\n",
    "[    Remapping categories]    100 xp\n",
    "[    Cleaning text data]    50 xp\n",
    "    Removing titles and taking names    100 xp\n",
    "    Keeping it descriptive    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf42f6f-f930-4e9a-9fd7-7aebf5c266a3",
   "metadata": {},
   "source": [
    "##  Advanced data problems\n",
    "0%\n",
    "\n",
    "In this chapter, you’ll dive into more advanced data cleaning problems, such as [ensuring that weights are all written in kilograms instead of pounds]. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses.\n",
    "\n",
    "[    Uniformity]    50 xp\n",
    "[    Ambiguous dates]    50 xp\n",
    "    Uniform currencies    100 xp\n",
    "    Uniform dates    100 xp\n",
    "[    Cross field validation]    50 xp\n",
    "    Cross field or no cross field?    100 xp\n",
    "    How's our data integrity?    100 xp\n",
    "    Completeness    50 xp\n",
    "    Is this missing at random?    50 xp\n",
    "    Missing investors    100 xp\n",
    "    Follow the money    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a59e4-6b35-408c-8fb3-1bf98ad897de",
   "metadata": {},
   "source": [
    "##  Record linkage\n",
    "0%\n",
    "\n",
    "Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to [link records by calculating the similarity between strings]—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset.\n",
    "\n",
    "    Comparing strings    50 xp\n",
    "[    Minimum edit distance]    50 xp\n",
    "    The cutoff point    100 xp\n",
    "    Remapping categories II    100 xp\n",
    "    Generating pairs    50 xp\n",
    "    To link or not to link?    100 xp\n",
    "    Pairs of restaurants    100 xp\n",
    "    Similar restaurants    100 xp\n",
    "    Linking DataFrames    50 xp\n",
    "    Getting the right index    50 xp\n",
    "[    Linking them together!]    100 xp\n",
    "    Congratulations!    50 xp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094813d4-e269-4a00-8837-ebe1ea5fb63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa37edc2-e0fe-4481-a3d7-c2f29bf3dbee",
   "metadata": {},
   "source": [
    "## Data type constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The institutor's name is Adel, he'll be our host as we learn how to clean data in Python.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this course, we're going to understand how to diagnose different problems in our data and how they can come up during out workflow.  We will also understand the side effects of not treating our data correctly.  And various ways to address different types of dirty data.  \n",
    "\n",
    "\n",
    "In this chapter, we're going to discuss the most common data problems you may encounter and how to address them.  \n",
    "\n",
    "# To understand why we need to clean data, lets remind ourselves of the data science workflow.  In a typical data science workflow, we usually access our raw data, explore and process it, develop insights using visualizations or predictive models, and finally report these insights with dashboards or reports.  \n",
    "\n",
    "[Access Data --> Explore and Process Data --> Extract Insights --> Report Insights]\n",
    "\n",
    "Dirty data can appear because of [duplicate values], [miss-spellings], [data type parsing errors] and [legacy systems].  \n",
    "Without making sure that data is properly clearned in the exploration and processing phase, we will surely compromise the insights and reports subsequently generated.  As the old adage says, garbage in garbage out.  \n",
    "\n",
    "\n",
    "When working with data, there are various types that we may encounter along the way.  We could be working with text data, integers, decimals, dates, zip codes, and others.  Luckily, Python has specific data type objects for various data types that you're probably familiar with by now.  This makes it much easier to manipulate these various data types in Python.  \n",
    "    \n",
    "    Text data, Integers, Decimals, Binary, Dates, Categories\n",
    "    str, int, float, bool, datetime, cateory\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# As such, before preparing to analyze and extract insights from our data, we need to make sure our variables have the correct data types, other wise we risk compromising our anaysis.  Lets take a look at the following example.  \n",
    "\n",
    "\n",
    "Below is a head od a DF containing revenue generated and quantity of items sold for a sales order.  We want to calculate the total revenue generated by all sales orders.  As you can see, the Revenue column has the dollar ($) sign on the right-hand side.  A close inspection of the DF columns' data types using the [.dtypes] attribute returns [object] for the Revenue column, which is what Pandas uses to store strings.  We can also check the data types as well as the number of missing values per column in a DataFrame, by using the [.info()] method.  \n",
    "\n",
    "Since the Revenue column is a string, summing across all sales order returns one large concatenated string containing each row's string.  To fix this, we need to first remove the $ sign from the string so that Pandas is able to convert the strings into numbers without error (how about we define it at the first place? need to do some test).  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To do this with the [sales['Revenue'].str.strip('$')] method, while specifying the string we want to strip as an argument which is in this case the dollar sign.  \n",
    "Since our dollar value do not contain decimals, we then convert the Revenue column to an integer by using the [.astype('int')] method , specifying the desired data type as argument.  Had our revenue value been decimal, we would have converted the Revenue column to float.  \n",
    "We can make sure that the Revenue column is now an integer by using the [assert] statement, which takes in a condition as input, and returns nothing if that condition is met, and ar error is it is not.  \n",
    "You can test almost anything you can image of by using [assert], and we'll see more ways to utilize it as we go along the course.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "# A common type of data seems numeric but actually represents cateories with a finite set of possible categories.  This is called categorical data.  \n",
    "We will look more closely at categorical data in Chapter 2, but lets take a look at this example.  Here we have a marriage status column, which is represented by [0] for never married, [1] for married, [2] for separated, and [3] for divorced.  \n",
    "\n",
    "However it will be imported of type integer, which could lead to misleading results when trying to extract some statistical summaries.  We can solve this by using the same [.astype()] method seen earlier, but this time specifying the [\"category\"] data type.  When applying the [.describe()] method again, we see that the summary statistics are much more aligned with that of a categorical variable, discussinng the number of observations, number of unique values, most frequent category instead of mean and standard deviation.  \n",
    "\n",
    "\n",
    "Now that we have solid understanding of data type constrains - lets get to practice.  \n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Remove $ from Revenue column\n",
    "sales['Revenue'] = sales['Revenue'].str.strip('$')  #################################################################\n",
    "sales['Revenue'] = sales['Revenue'].astype('int')\n",
    "\n",
    "# Verify that Revenue is now an integer\n",
    "assert sales['Revenue'].dtypes == 'int'  ############################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Convert to categorical\n",
    "df['marriage_status'] = df['marriage_status'].astype('category')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53a2fc-1b2d-48da-9110-a923c4faea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales.csv')\n",
    "\n",
    "print(sales.head())\n",
    "\n",
    "\n",
    "---------------------------------------------\n",
    "  SalesOrderId  | Revenue     | Quantity\n",
    "0        43659  |     23153$  |       12\n",
    "1        43660  |      1457$  |        2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c075397b-8c4e-45ec-b130-394c52927157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23153\n",
      "<class 'str'>\n",
      "23153\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "price = '23153$'\n",
    "\n",
    "\n",
    "n_price = price.strip('$')\n",
    "print(n_price)\n",
    "print(type(n_price))\n",
    "\n",
    "print(int(n_price))\n",
    "print(type(int(n_price)))\n",
    "\n",
    "\n",
    "#assert int(n_price).dtype() == 'int'\n",
    "#assert type(int(n_price)) == 'int'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9797795-53fc-4193-95bc-156f95347465",
   "metadata": {},
   "source": [
    "## Common data types\n",
    "\n",
    "Manipulating and analyzing data with incorrect data types could lead to compromised analysis as you go along the data science workflow.\n",
    "\n",
    "When working with new data, you should always check the data types of your columns using the .dtypes attribute or the .info() method which you'll see in the next exercise. Often times, you'll run into columns that should be converted to different data types before starting any analysis.\n",
    "\n",
    "In this exercise, you'll first identify different types of data and correctly map them to their respective types.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Assign each card to what type of data you think it is.\n",
    "\n",
    "Numeric data type:   Salary earned monthly, \n",
    "                     Number of items bought in a basket, \n",
    "                     Number of points on customer loyalty card\n",
    "  \n",
    "Text:                City of residence, \n",
    "                     First name\n",
    "                     Shipping address of a customer\n",
    "  \n",
    "Dates:               Birthdates of clients, \n",
    "                     Order date of a product, \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0971b53-b4fd-42ea-930a-01603d66afd4",
   "metadata": {},
   "source": [
    "## Numeric data or ... ?\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called \"ride_sharing\". It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.\n",
    "\n",
    "The \"user_type\" column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "\n",
    "    1 for free riders.\n",
    "    2 for pay per ride.\n",
    "    3 for monthly subscribers.\n",
    "\n",
    "In this instance, you will print the information of ride_sharing using .info() and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset. The pandas package is imported as pd.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the information of ride_sharing.\n",
    "[    Use .describe() to print the summary statistics of the user_type column from ride_sharing.]\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    By looking at the summary statistics - they don't really seem to offer much description on how users are distributed along their purchase type, why do you think that is?\n",
    "Possible Answers\n",
    "    -The user_type column is not of the correct type, it should be converted to str.\n",
    "    -The user_type column has an infinite set of possible values, it should be converted to category.\n",
    "[    -The user_type column has an finite set of possible values that represent groupings of data, it should be converted to category].     Also other 'int64' values, station_A_id, bike_id, user_birth_year, user_gender\n",
    "\n",
    "\n",
    "    Question 3\n",
    "    Convert \"user_type\" into categorical by assigning it the 'category' data type and store it in the \"user_type_cat\" column.\n",
    "    Make sure you converted \"user_type_cat\" correctly by using an \"assert\" statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcee990f-9595-4f4e-ab9c-66e9528957f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration           object\n",
      "station_A_id        int64\n",
      "station_A_name     object\n",
      "station_B_id        int64\n",
      "station_B_name     object\n",
      "bike_id             int64\n",
      "user_type           int64\n",
      "user_birth_year     int64\n",
      "user_gender        object\n",
      "dtype: object \n",
      "\n",
      "     duration  station_A_id  \\\n",
      "0  12 minutes            81   \n",
      "1  24 minutes             3   \n",
      "2   8 minutes            67   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "\n",
      "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480          2             1959   \n",
      "1  Eureka Valley Recreation Center     5193          2             1965   \n",
      "2    The Embarcadero at Steuart St     3652          3             1993   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male   \n",
      "\n",
      "user_type is:  category \n",
      "\n",
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type, dtype: int64 \n",
      "\n",
      "12972\n",
      "0        2\n",
      "1        2\n",
      "4        2\n",
      "5        2\n",
      "6        2\n",
      "        ..\n",
      "25752    2\n",
      "25753    2\n",
      "25756    2\n",
      "25757    2\n",
      "25758    2\n",
      "Name: user_type, Length: 12972, dtype: category\n",
      "Categories (3, int64): [1, 2, 3]\n",
      "0        2\n",
      "1        2\n",
      "4        2\n",
      "5        2\n",
      "6        2\n",
      "        ..\n",
      "25752    2\n",
      "25753    2\n",
      "25756    2\n",
      "25757    2\n",
      "25758    2\n",
      "Name: user_type, Length: 12972, dtype: category\n",
      "Categories (3, int64): [1, 2, 3] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  object  \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(4), object(4)\n",
      "memory usage: 1.8+ MB\n",
      "None\n",
      "       station_A_id  station_B_id       bike_id  user_birth_year\n",
      "count  25760.000000  25760.000000  25760.000000     25760.000000\n",
      "mean      31.023602     89.558579   4107.621467      1983.054969\n",
      "std       26.409263    105.144103   1576.315767        10.010992\n",
      "min        3.000000      3.000000     11.000000      1901.000000\n",
      "25%       15.000000     21.000000   3106.000000      1978.000000\n",
      "50%       21.000000     58.000000   4821.000000      1985.000000\n",
      "75%       67.000000     93.000000   5257.000000      1990.000000\n",
      "max       81.000000    383.000000   6638.000000      2001.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.dtypes, '\\n')\n",
    "\n",
    "print(df.head(3), '\\n')\n",
    "\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "print('user_type is: ',  df['user_type'].dtypes, '\\n')\n",
    "assert df['user_type'].dtype == 'category' # ------------------------------------------------------------------------\n",
    "\n",
    "print(df['user_type'].describe(), '\\n')\n",
    "\n",
    "\n",
    "print(len(df[df['user_type']==2]))\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# filt_list = movies_genres.loc[movies_genres['_merge']=='both', 'title'].unique()\n",
    "print(df.loc[df['user_type']==2, 'user_type'])\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "print(df[df['user_type']==2]['user_type'].copy(), '\\n')\n",
    "\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48055e6c-63c2-47b3-a701-ada0b5216392",
   "metadata": {},
   "source": [
    "## Summing strings and concatenating numbers\n",
    "\n",
    "In the previous exercise, you were able to identify that category is the correct data type for user_type and convert it [in order to extract relevant statistical summaries] that shed light on the distribution of user_type. (cause 1 for single, 2 for married, 3 for having kids, summaries like average and std is the bad to summarize)\n",
    "\n",
    "# Another common data type problem is importing what should be numerical values as strings, \n",
    "as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.\n",
    "\n",
    "In this exercise, you'll be converting the string column duration to the type int. Before that however, you will need to make sure to strip \"minutes\" from the column in order to make sure pandas reads it as numerical. The pandas package has been imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "[    Use the \".strip()\" method to strip duration of \"minutes\" and store it in the \"duration_trim\" column].\n",
    "    Convert \"duration_trim\" to int and store it in the \"duration_time\" column.\n",
    "    Write an \"assert\" statement that checks if duration_time's data type is now an int.\n",
    "    Print the average ride duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "738ded22-e604-4f3a-b0ec-f93a3d873df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration           object\n",
      "station_A_id        int64\n",
      "station_A_name     object\n",
      "station_B_id        int64\n",
      "station_B_name     object\n",
      "bike_id             int64\n",
      "user_type           int64\n",
      "user_birth_year     int64\n",
      "user_gender        object\n",
      "dtype: object \n",
      "\n",
      "0    12 minutes\n",
      "1    24 minutes\n",
      "2     8 minutes\n",
      "Name: duration, dtype: object \n",
      "\n",
      "int64 \n",
      "\n",
      "df['duration'].mean():  11.389052795031056 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  int64   \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(5), object(3)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.dtypes, '\\n')\n",
    "\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "print(df['duration'].head(3), '\\n')\n",
    "\n",
    "\n",
    "df['duration'] = df['duration'].str.strip('minutes')\n",
    "\n",
    "df['duration'] = df['duration'].astype('int')\n",
    "#print(df['duration'].head())\n",
    "print(df['duration'].dtype, '\\n')\n",
    "\n",
    "\n",
    "print(\"df['duration'].mean(): \", df['duration'].mean(), '\\n')\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261ae62-1f89-472f-8ee3-e8315793665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].____.____()\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ____\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['____'].____ == '____'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f7a79-1ec0-460d-8b95-01691e25bbe2",
   "metadata": {},
   "source": [
    "## Data range constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Hi and welcome back.  In this lesson, we're going to discuss [data that should fall within a range]\n",
    ".  Lets first start off with some motivation.  \n",
    "\n",
    "Imagine we have a dataset of movies with their respective average rating from a streaming service.  The rating can be any integer between 1 and 5.  After creating a histogram with Matplotlib, we see that there are a few movies with an average rating of 6, which is well above the allowable range.  [This is most likely an error in data collection or parsing, where a variable is well beyond its range] and treating it is essential to have accurate analysis.  \n",
    "\n",
    "Here is another example, where we see subscription dates in the future for a service.  Inherently this doesn't make any sence, as well cannot sign up for a service in the future, but [these errors exist either due to technical or human error].  We use the datetime package's \".date.today()\" function to get today's date, and we filter the dateset by any subscription date higher than today's date.  \n",
    "\n",
    "\n",
    "\n",
    "# We need to pay attention to the range of our data.  There's a variety of options to deal with out of range data.  \n",
    "\n",
    "(1) The simplest option is to [drop the data].  \n",
    "However, depending on the size of your out of range data, you could be loosing out on essential information.  As a rule of thumb, only drop data when a small proportion of your dataset is affected by out of range values, however you really need to understand your dataset before deciding to drop values.  \n",
    "(2) Another option would be [setting custom minimums or maximums] to your columns.  \n",
    "(3) We could also [set the data to be missing, and impute it], but we'll take a look at how to deal with missing data in Chapter 3.  \n",
    "(4) We could also, dependent on the business assumptions behind our data, [assign a custom value] for any values of our data that go beyond a certain range.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take a look at the movies example mentioned earlier.  We first isolate the movies with ratings higher than 5.  Now if these values are affect a small set of our data, we can drop them.  We can drop them is 2 ways - we can either create a new filtered movies DF where we only keep values of \"avg_rating\" lower than or equal to 5.  Or drop the values by using the [.drop()] method.  The \".drop()\" method takes in as argument the row indices of movies for which the \"avg_rating\" is higher than 5.  We set the \"inplace=\" argument to True so that values are dropped in place and we don't have to create a new column.  We can make sure this is set in place using assert statement that checks if the maximum of avg_rating is lower lan or equal to 5.  \n",
    "\n",
    "Depending on the assumptions behind our data, we can also change the out of range values to a hard limit.  For example, here we're setting any value of the avg_rating column to 5 if it goes beyond it.  We can do this using the \".loc[]\" method, which returns all cells that fit a custom row and column index.  It takes as first argument the row index, or here all instaces of avg_rating above 5, and second argument the column index, which is here the avg_rating column.  Aagin, we can make sure that this change was done using an assert statement.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take another look at the date range example mentioned earlier, where we have subscriptions happening in the future.  We first look at the datetypes (.dtype) of the column with the [.dtype] attribute.  We can confirm that the subscription_date column is an \"object\" and not a \"datetime\" object.  Datetime objects allow much earsier manipulation of date data, so lets convert it to that.  We do so with [pd.to_datetime()] function from Pandas, which takes in as argument the column we want to convert.  We can then test the data type conversion by asserting that the subscription date's column is equal to \"datetime64[ns]\", which is how the data type is represented in Pandas.  \n",
    "\n",
    "Now that the column is in datetime, we can treat it in a variety of ways.  We first create a today_date variable using the datetime function \".date.today()\", which allows us to store today's date.  We can then either drop the rows with exceeding dates similar to how we did in the average rating example, or replace exceeding values with todays date.  \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(movies['avg_rating'])\n",
    "plt.title('Average rating of movies (1-5)')\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "today_date = datetime.date.today()\n",
    "user_signups[user_signups['subscription_date'] > today_date]\n",
    "\n",
    "\n",
    "\n",
    "# Drop values using filtering\n",
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "\n",
    "# Drop values using .drop()\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True)  ################################################\n",
    "\n",
    "assert movies['avg_rating'].max() == 5\n",
    "\n",
    "\n",
    "# Convert avg_rating > 5 to value 5\n",
    "movies.loc[movies['avg_rating']>5, 'avg_rating'] = 5   ##############################################################\n",
    "\n",
    "\n",
    "# Convert to DataTime\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date'])  ##############################\n",
    "\n",
    "assers user_signups['subscription_date'].dtype == 'datetime64[ns]'  #################################################\n",
    "\n",
    "\n",
    "\n",
    "<1> Drop the data\n",
    "today_date = datetime.date.today()\n",
    "\n",
    "# Drop values using filtering\n",
    "user_signups = user_signups[user_signups['subscription_date'] < today_date]\n",
    "# Drop values using .drop()\n",
    "user_signups.drop(user_signups[user_signups['subscription_date'] < today_date].index, inplace = True)\n",
    "\n",
    "\n",
    "<2> Hardcode dates with upper limit\n",
    "\n",
    "# Replace values using filtering\n",
    "user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date\n",
    "\n",
    "assert user_signups['subscription_date'].max().date() <= today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc5690b8-861f-4968-a047-36524ae5bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "{'Search': [{'Title': 'Batman: The Killing Joke', 'Year': '2016', 'imdbID': 'tt4853102', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BMTdjZTliODYtNWExMi00NjQ1LWIzN2MtN2Q5NTg5NTk3NzliL2ltYWdlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Mask of the Phantasm', 'Year': '1993', 'imdbID': 'tt0106364', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTRiMWM3MGItNjAxZC00M2E3LThhODgtM2QwOGNmZGU4OWZhXkEyXkFqcGdeQXVyNjExODE1MDc@._V1_SX300.jpg'}, {'Title': 'Batman: The Dark Knight Returns, Part 2', 'Year': '2013', 'imdbID': 'tt2166834', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTEzMmE0ZDYtYWNmYi00ZWM4LWJjOTUtYTE0ZmQyYWM3ZjA0XkEyXkFqcGdeQXVyNTA4NzY1MzY@._V1_SX300.jpg'}, {'Title': 'Batman: Year One', 'Year': '2011', 'imdbID': 'tt1672723', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BNTJjMmVkZjctNjNjMS00ZmI2LTlmYWEtOWNiYmQxYjY0YWVhXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Assault on Arkham', 'Year': '2014', 'imdbID': 'tt3139086', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BZDU1ZGRiY2YtYmZjMi00ZDQwLWJjMWMtNzUwNDMwYjQ4ZTVhXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman', 'Year': '1966', 'imdbID': 'tt0060153', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BMmM1OGIzM2UtNThhZS00ZGNlLWI4NzEtZjlhOTNhNmYxZGQ0XkEyXkFqcGdeQXVyNTkxMzEwMzU@._V1_SX300.jpg'}, {'Title': 'Batman: Gotham Knight', 'Year': '2008', 'imdbID': 'tt1117563', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BM2I0YTFjOTUtMWYzNC00ZTgyLTk2NWEtMmE3N2VlYjEwN2JlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Arkham City', 'Year': '2011', 'imdbID': 'tt1568322', 'Type': 'game', 'Poster': 'https://m.media-amazon.com/images/M/MV5BZDE2ZDFhMDAtMDAzZC00ZmY3LThlMTItMGFjMzRlYzExOGE1XkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman Beyond', 'Year': '1999–2001', 'imdbID': 'tt0147746', 'Type': 'series', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTBiZjFlZDQtZjc1MS00YzllLWE5ZTQtMmM5OTkyNjZjMWI3XkEyXkFqcGdeQXVyMTA1OTEwNjE@._V1_SX300.jpg'}, {'Title': 'Son of Batman', 'Year': '2014', 'imdbID': 'tt3139072', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYjdkZWFhNzctYmNhNy00NGM5LTg0Y2YtZWM4NmU2MWQ3ODVkXkEyXkFqcGdeQXVyNTA0OTU0OTQ@._V1_SX300.jpg'}], 'totalResults': '497', 'Response': 'True'}\n",
      "['Batman: The Killing Joke', 'Batman: Mask of the Phantasm', 'Batman: The Dark Knight Returns, Part 2', 'Batman: Year One', 'Batman: Assault on Arkham', 'Batman', 'Batman: Gotham Knight', 'Batman: Arkham City', 'Batman Beyond', 'Son of Batman']\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.omdbapi.com/?t=hackers'\n",
    "url = 'http://www.omdbapi.com/?i=tt3896198&apikey=8c241015'\n",
    "url = 'http://www.omdbapi.com/?s=Batman&page=1&apikey=8c241015'\n",
    "url = 'http://www.omdbapi.com/?s=Batman&page=2&apikey=8c241015'\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "r = requests.get(url)\n",
    "print(type(r))\n",
    "\n",
    "j_data = r.json()\n",
    "\n",
    "print(j_data)\n",
    "\n",
    "\n",
    "\n",
    "titles = []\n",
    "with requests.get(url) as r:\n",
    "    j_data = r.json()\n",
    "    content = j_data['Search']\n",
    "    for i in content:\n",
    "        titles.append(i['Title'])\n",
    "    \n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33c05df2-cb75-42de-89e9-69dccb9d1d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     duration  station_A_id  \\\n",
      "0  12 minutes            81   \n",
      "1  24 minutes             3   \n",
      "2   8 minutes            67   \n",
      "3   4 minutes            16   \n",
      "4  11 minutes            22   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "3                            Steuart St at Market St            28   \n",
      "4                              Howard St at Beale St           350   \n",
      "\n",
      "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480          2             1959   \n",
      "1  Eureka Valley Recreation Center     5193          2             1965   \n",
      "2    The Embarcadero at Steuart St     3652          3             1993   \n",
      "3     The Embarcadero at Bryant St     1883          1             1979   \n",
      "4             8th St at Brannan St     4626          2             1994   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "3        Male  \n",
      "4        Male  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  object  \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(4), object(4)\n",
      "memory usage: 1.8+ MB\n",
      "None\n",
      "       station_A_id  station_B_id       bike_id  user_birth_year\n",
      "count  25760.000000  25760.000000  25760.000000     25760.000000\n",
      "mean      31.023602     89.558579   4107.621467      1983.054969\n",
      "std       26.409263    105.144103   1576.315767        10.010992\n",
      "min        3.000000      3.000000     11.000000      1901.000000\n",
      "25%       15.000000     21.000000   3106.000000      1978.000000\n",
      "50%       21.000000     58.000000   4821.000000      1985.000000\n",
      "75%       67.000000     93.000000   5257.000000      1990.000000\n",
      "max       81.000000    383.000000   6638.000000      2001.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "#help(df.drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d396a30-5313-49b7-8f24-51dd411433b5",
   "metadata": {},
   "source": [
    "## Tire size constraints\n",
    "\n",
    "In this lesson, you're going to build on top of the work you've been doing with the ride_sharing DataFrame. You'll be working with the \"tire_sizes\" column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.\n",
    "\n",
    "In this exercise, you will make sure the tire_sizes column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Convert the \"tire_sizes\" column from category to 'int'.\n",
    "    Use \".loc[]\" method to set all values of tire_sizes above 27 to 27.\n",
    "    Reconvert back tire_sizes to 'category' from int.\n",
    "    Print the description of the tire_sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62f6c902-1baf-4688-8bd5-21b7cd884a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    duration  station_A_id  \\\n",
      "0           0  12 minutes            81   \n",
      "1           1  24 minutes             3   \n",
      "2           2   8 minutes            67   \n",
      "3           3   4 minutes            16   \n",
      "4           4  11 minutes            22   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "3                            Steuart St at Market St            28   \n",
      "4                              Howard St at Beale St           350   \n",
      "\n",
      "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480          2             1959   \n",
      "1  Eureka Valley Recreation Center     5193          2             1965   \n",
      "2    The Embarcadero at Steuart St     3652          3             1993   \n",
      "3     The Embarcadero at Bryant St     1883          1             1979   \n",
      "4             8th St at Brannan St     4626          2             1994   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "3        Male  \n",
      "4        Male  \n",
      "   Unnamed: 0 duration  station_A_id  \\\n",
      "0           0      12             81   \n",
      "1           1      24              3   \n",
      "2           2       8             67   \n",
      "3           3       4             16   \n",
      "4           4      11             22   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "3                            Steuart St at Market St            28   \n",
      "4                              Howard St at Beale St           350   \n",
      "\n",
      "                    station_B_name  bike_id user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480         2             1959   \n",
      "1  Eureka Valley Recreation Center     5193         2             1965   \n",
      "2    The Embarcadero at Steuart St     3652         3             1993   \n",
      "3     The Embarcadero at Bryant St     1883         1             1979   \n",
      "4             8th St at Brannan St     4626         2             1994   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "3        Male  \n",
      "4        Male  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tire_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.virtual_environments/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtual_environments/py39/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.virtual_environments/py39/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tire_size'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2873/24797610.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Convert tire_sizes to integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mride_sharing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tire_sizes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mride_sharing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tire_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Set all values above 27 to 27\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtual_environments/py39/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3506\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3508\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtual_environments/py39/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tire_size'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ride_sharing = pd.read_csv('ride_sharing_new.csv')\n",
    "print(ride_sharing.head())\n",
    "\n",
    "\n",
    "ride_sharing['duration'] = ride_sharing['duration'].str.strip('minutes')\n",
    "ride_sharing['duration'].astype('int')\n",
    "\n",
    "ride_sharing['user_type'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "print(ride_sharing.head())\n",
    "\n",
    "\n",
    "\n",
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_size'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_size'] > 27, 'tire_size'] = 27  # ***********************************************\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')  # ***************************************\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0113d-fa1f-4406-8656-45de4d583eed",
   "metadata": {},
   "source": [
    "## Back to the future\n",
    "\n",
    "[A new update to the data pipeline feeding into the \"ride_sharing\" DataFrame has been updated to register each ride's date]. This information is stored in the \"ride_date\" column of the type \"object\", which represents strings in pandas.\n",
    "\n",
    "A bug was discovered which was relaying rides taken today as taken next year (think other conditions, last year?).  To fix this, you will find all instances of the \"ride_date\" column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert \"ride_date\" to a datetime object.\n",
    "\n",
    "The datetime package has been imported as dt, alongside all the packages you've been using till now.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Convert ride_date to a datetime object and store it in ride_dt column using to_datetime().\n",
    "    Create the variable today, which stores today's date by using the \"dt.date.today()\" function.\n",
    "#    For all instances of \"ride_dt\" in the future, set them to today's date.\n",
    "    Print the maximum date in the ride_dt column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb53826-f7e6-4948-bd7b-d8c71fca039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "today_date = datetime.date.today()\n",
    "\n",
    "ride_sharing['ride_date'] = ride_sharing['ride_date'].astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "ride_sharing['ride_date'] = X  ride_sharing.loc[ride_sharing['ride_date'] > today_date, 'ride_date'] = today_date\n",
    "ride_sharing['ride_date'] = X  ride_sharing[ride_sharing['ride_date']>today_date]['ride_date'] = today_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d47f4-a445-4b81-aec4-a470efe94816",
   "metadata": {},
   "source": [
    "# Your thinking makes you smarter, not stareing at the answer, you learn nothing in looking at it, but learn a lot in figuring it out.  Choose the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae977680-7059-4d50-ab84-7e48225d9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to datetime\n",
    "ride_sharing['ride_dt'] = pd.____(____['____'])\n",
    "\n",
    "# Save today's date\n",
    "today = ____\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.____[____['____'] > ____, '____'] = ____\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].____())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5865b6c-ef93-4273-a400-e04e00372d6e",
   "metadata": {},
   "source": [
    "## Uniqueness constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Hi and welcome to the final lesson of this chapter.  Lets discuss another common data clearning problem, the [duplicate values].  Duplicate values can be diagnosed when we have the same exact information repeated across multiple row, for some or all column of our DF.  \n",
    "\n",
    "In this example DF containing the names, address, height, and weight of individuals, the rows represented have identical values across all columns.  In this one there are duplicate values for all columns except the height column, which leads us to think its more likely a data entry error than an actural other person.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Apart from data entry and human errors alluded to in the previous slide, duplicate data can also arise because of bugs and design errors whether in business processes or data pipelines.  However, they often most arise form the necessary act of joining and consolidating data from various resources, which could retain duplicate values.  \n",
    "\n",
    "\n",
    "# Lets first see how to find duplicate values.  \n",
    "[df.duplicated()] method --> boolean indexing\n",
    "In this example we're working with a bigger version of the height and weight data seen earlier in this video.  We can find duplicates in a DF by using [df.duplicated()] method.  It returns a Series of boolean values that are True for duplicated values, and False for non-duplicated values.  We can see exactly which rows are affected by using boolean indexing (df[df.duplicated()]).  However using \"df.duplicated()\" without playing around with the arguments of the method can lead to misleading results, as all of the columns are required to have duplicated values by default, with all duplicated values being marked as True except for the first occurence.  \n",
    "[Here, can I recall how we do the DF merger and concatnate operations?  Think and think]\n",
    "\n",
    "This limit our ability to properly diagnose what type of duplication we have, how to effectively treat it.  To properly calibrate how we find duplicates, we will use 2 arguments from the [df.duplicated(sueset=, keep=)] method.  The [subset=] argument lets us set a list of column names to check for duplication.  For example, it can allows us to find duplicates for the first and last name columns only.  The [keep=] argument lets us keep the first occurrence of a duplicate value by setting it to the tring \"first\", \"last\", or keep all occurances of duplicated values by setting it to \"False\".  In below example, we're checking for duplicates across the first name, last name and address columns\n",
    "and we choosing to keep all duplicates.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "We see the following results, to get a better bird's eye view of the duplicates, we sort the duplicated rows using [df.sort_values(by='first_name')] method, choosing \"first_name\" to sort by.  We find that there are 4 sets of duplicated rows, the first 2 being complete duplicates of each other across all columns.  The other 2 being incomplete duplicates of each other with discrepancies across height and weight respectively.  \n",
    "\n",
    "# [.drop_duplicates()] method\n",
    "The complete duplicates can be treated easily.  All that required is keep one of them only and discard the others.  This can be down with the [.drop_duplicates()] method, which also takes in the same \"subset=\" and \"keep=\" argument asin the \".duplicated()\" method, as well as the [inplace=] argument which drops the duplicated values directly inside the height_weight DF.  Here we are droping complete duplicates only, so its not necessary nor adviable to set a [subset=], and since the [keep=\"first\"] argument takes in [\"first\"] as default, we can keep it as such.  Note that we can also set it as [\"last\"], but not as [\"False\"] as it would keep all duplicates.  \n",
    "\n",
    "This leaves us with the other 2 sets of duplicates discussed earlier, which are the same for first_name, last_name and address, but contain discrepancies in height and weight.  \n",
    "\n",
    "\n",
    "Apart from droping rows with really small discrepancies, we can use a statistical measure to combine each set of duplicated values.  For example, we can combine these 2 rows into 1 by computing the average mean between them, or the maximum, or other statistical measures, this is highly dependent on a common sense understanding of our data, and what type of data we have.  We can do this easily using the [.groupby()] method, which when chained with [.agg()] method, let you group by a set of common columns and return statistical values for specific columns when the aggregation is being performed. \n",
    "\n",
    "For example here, we created a dictionary called summaries, which instructs groupby to return the maximum of duplicated rows for the height column, and the mean duplicated rows for the weight column.  \n",
    "We then \".groupby()\" height_weight by the column names defined earlier, and chained it with the \"agg()\" method, which takes in the summaries dictionary we created.  \n",
    "We chain this entire line with the \".reset_index()\" method, so that we can have numbered indices in the final output. \n",
    "We can verify that there are no more duplicate values by running the \".duplicated()\" method again, and use brackets to output duplicate rows.  \n",
    "\n",
    "\n",
    "Now that we have a solid grasp of dupliccation, lets practice.  \n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------\n",
    "first_name   | last_name   | address                                    | height   | weight\n",
    "Justin       | Saddlemyer  | Boulevard du Jardin Botainque 3, Bruxelles | 193 cm   | 87 kg\n",
    "Justin       | Saddlemyer  | Boulevard du Jardin Botainque 3, Bruxelles | 194 cm   | 87 kg\n",
    "\n",
    "\n",
    "\n",
    "Data Entry & Human Error,   Bugs and design errors,   Join or merge Errors\n",
    "\n",
    "\n",
    "\n",
    "# Column names to check for duplication\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)  #############################################\n",
    "\n",
    "print(height_weight[duplicates])\n",
    "\n",
    "----------------------------------------------------- Image what will be the output of it\n",
    "\n",
    "\n",
    "\n",
    "# Output duplicate values\n",
    "height_weight[duplicates].sort_values(by='first_name')   # Think why choosing f_name, ll_name, address set duplicate \n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "     first_name  | last_name  | address                              | height  | weight\n",
    " 22        Cole  |    Palmer  |                     8366 At, Street  |    178  | 91\n",
    "102        Cole  |    Palmer  |                     8366 At, Street  |    178  | 91\n",
    " 28     Desirae  |   Shannon  | P.O. Box 643, 5251 Consectetuer, Rd. |    195  | 83\n",
    "103     Desirae  |   Shannon  | P.O. Box 643, 5251 Consectetuer, Rd. |    196  | 83\n",
    "  1        Ivor  |    Pierce  |                   102-3364 Non. Road |    168  | 66\n",
    "101        Ivor  |    Pierce  |                   102-3364 Non. Road |    168  | 88\n",
    " 37        Mary  |     Colon  |                         4674 Ut Rd.  |    179  | 75\n",
    "100        Mary  |     Colon  |                         4674 Ut Rd.  |    179  | 75\n",
    "         \n",
    "         \n",
    "# Drop duplicates\n",
    "height_weight.drop_duplicates(inplace=True)  ########################################################################\n",
    "\n",
    "\n",
    "height_weight.groupby(['first_name', 'last_name', 'address']).agg(np.mean)  ??\n",
    "\n",
    "# Groupby column names and produce statistical summaries\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "summaries = {'height': 'max', 'weight': 'mean'}    ##################################################################\n",
    "height_weight = height_weight.groupby(by=column_names).agg(summaries).reset_index()  ################################\n",
    "\n",
    "# Make sure aggregation is done\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)\n",
    "height_weight[duplicates].sort_values(by='first_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c53b81-edc2-42a3-b12c-1ed622b2a063",
   "metadata": {},
   "source": [
    "## How big is your subset?\n",
    "\n",
    "You have the following loans DataFrame which contains loan and credit score data for consumers, and some metadata such as their first and last names. You want to find both complete and incomplete duplicates using \".duplicated()\" method.\n",
    "first_name \tlast_name \t    credit_score \thas_loan\n",
    "Justin \t    Saddlemeyer \t600 \t        1\n",
    "Hadrien \tLacroix \t    450 \t        0\n",
    "\n",
    "Choose the correct usage of [.duplicated()] below:\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    loans.duplicated()\n",
    "      Because the default method returns both complete and incomplete duplicates.  X Full duplicates in every col\n",
    "    1\n",
    "    loans.duplicated(subset = 'first_name')\n",
    "      Because constraining the duplicate rows to the first name lets me find incomplete duplicates as well.  X\n",
    "    2\n",
    "    loans.duplicated(subset = ['first_name', 'last_name'], keep = False)\n",
    "      Because subsetting on consumer metadata and not discarding any duplicate returns all duplicated rows.  X set \"keep=\" arg to False will keep all duplicates\n",
    "    3\n",
    "#    loans.duplicated(subset = ['first_name', 'last_name'], keep = 'first')\n",
    "      Because this drops all duplicates.\n",
    "    4\n",
    "    \n",
    "# The value we set for \"keep=\" arg is for \"mark\" boolean, not keep the records\n",
    "# *******************************************************************************************************************\n",
    "    keep : {'first', 'last', False}, default 'first'\n",
    "        Determines which duplicates (if any) to mark.\n",
    "    \n",
    "        - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n",
    "        - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n",
    "        - False : Mark all duplicates as ``True``.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97a93835-bfd1-41c9-b581-403a0c6f59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(pd.DataFrame.duplicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686092f6-607d-4d0e-926d-8ec9f3ace435",
   "metadata": {},
   "source": [
    "## Finding duplicates\n",
    "\n",
    "A new update to the data pipeline feeding into \"ride_sharing\" has added the \"ride_id\" column, which represents a unique identifier for each ride.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future.  Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the \"ride_sharing\" DataFrame.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "In this exercise, you will confirm this suspicion by finding those duplicates. A sample of \"ride_sharing\" is in your environment, as well as all the packages you've been working with thus far.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Find duplicated rows of \"ride_id\"  in the \"ride_sharing\" DataFrame while setting \"keep=\" arg to False.\n",
    "    Subset \"ride_sharing\" on duplicates and sort by \"ride_id\" and assign the results to \"duplicated_rides\".\n",
    "    Print the \"ride_id\", \"duration\" and \"user_birth_year\" columns of \"duplicated_rides\" in that order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de25e1-3624-4ec0-bec2-b5aa1a3005fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)   ################################################\n",
    "\n",
    "\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "\n",
    "print(duplicated_rides[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a2ecb-7a3a-49f8-ab98-8fd1fa5288ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ____.____(____, ____)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[____].____('____')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['____','____','____']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbab60-bff4-4752-a62a-fe4a09ff1493",
   "metadata": {},
   "source": [
    "## Treating duplicates\n",
    "\n",
    "In the last exercise, you were able to verify that the new update feeding into \"ride_sharing\" contains a bug generating both complete and incomplete duplicated rows for some values of the \"ride_id\" column, with occasional discrepant values for the \"user_birth_year\" and \"duration\" columns.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average duration, and the minimum user_birth_year for each set of incomplete duplicate rows.\n",
    "# *******************************************************************************************************************\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Drop complete duplicates in \"ride_sharing\" and store the results in \"ride_dup\".\n",
    "#    Create the statistics dictionary which holds minimum aggregation for \"user_birth_year\" and mean aggregation for \"duration\".\n",
    "#    Drop incomplete duplicates by grouping by \"ride_id\" and applying the aggregation in statistics.\n",
    "    Find duplicates again and run the assert statement to verify de-duplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973867c5-89d4-478a-abb8-d88a0e0c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_dup = ride_sharing.drop_duplicates()  # No need set \"inplace=\" arg to True as we passing it top a varianble\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "summaries = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(summaries).reset_index()\n",
    "\n",
    "\n",
    "duplicates = ride_unique.duplicated(subset='ride_id', keep=False)\n",
    "\n",
    "\n",
    "duplicates_rides = ride_unique[duplicates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ee783d-b71e-44a6-a648-b046c9eb0009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dteday  season  yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
      "0  2011-01-01       1   0     1        0        6           0           2   \n",
      "1  2011-01-02       1   0     1        0        0           0           2   \n",
      "2  2011-01-03       1   0     1        0        1           1           1   \n",
      "3  2011-01-04       1   0     1        0        2           1           1   \n",
      "4  2011-01-05       1   0     1        0        3           1           1   \n",
      "\n",
      "       temp     atemp       hum  windspeed  casual  registered  total_rentals  \n",
      "0  0.344167  0.363625  0.805833   0.160446     331         654            985  \n",
      "1  0.363478  0.353739  0.696087   0.248539     131         670            801  \n",
      "2  0.196364  0.189405  0.437273   0.248309     120        1229           1349  \n",
      "3  0.200000  0.212122  0.590435   0.160296     108        1454           1562  \n",
      "4  0.226957  0.229270  0.436957   0.186900      82        1518           1600  \n"
     ]
    }
   ],
   "source": [
    "'''__groupby with agg & pivittable with aggfunc__'''\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "jhu@debian:~/.virtual_environments$ for i in \"DataCamp*\"; do grep \"pivot_table\" $i; done  ###########################\n",
    "DataCamp Cleaning Data in Python.ipynb:    \"#print(help(df.pivot_table))\\n\",\n",
    "DataCamp Data Manipulation with Pandas.ipynb:    \"# We can do the same thing using the \\\".pivot_table()\\\" method.  \\n\",\n",
    "DataCamp Data Manipulation with Pandas.ipynb:    \"By default, .pivot_table() takes the mean value for each group.  If we want a different summary statistics, we can use the aggfunc= argument and pass it a function.  Here we take the median for each dog color using NumPy's .median() function.  \\n\",\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"bike_share.csv\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "#print(dir(df))\n",
    "\n",
    "#print(help(df.pivot_table))\n",
    "#values=None, index=None, columns=None, aggfunc='mean', fill_value=None   ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daf0a4-c4a3-4adb-8c8c-bdf23d1e1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ____.____()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': ____, 'duration': ____}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.____('____').____(____).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.____(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0  ##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa1f01-de87-460c-99a4-13cff9de7c5b",
   "metadata": {},
   "source": [
    "## Membership constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Good work on chapter 1.  We are now equipped to treat more complex, and specific data cleaning problems.  In this chapter, we're going to take a look at common data problems with text and categorical data.  So lets get started.  \n",
    "\n",
    "In this lesson, we'll focus on categorical variables.  As discussed early in chapter 1, categorical data represent variables that represent predefined finite set of categories.  Examples of this range from marriage status, household income categories, loan status and others.  \n",
    "\n",
    "# To run machine learning models on categorical data, they are often coded as numbers.  \n",
    "Since categorical data represent a predefined set of categories, they can't have values that go beyond these predefined categories.  We can have inconsistencies in our categorical data for a variety of reasons.  This could be due to data entry issues with free text vs dropdown fields, data parsing errors and other types of errors.  \n",
    "\n",
    "There's a variety of ways we can treat these, with increasingly specific solutions for different types of inconsistencies.  Most simply, we can drop the rows with incorrect categories.  We can attempt remapping incorrect categories to correct ones, and more.  We'll see a variety of ways of dealing with this throughout the chapter and the course, but for now we'll just focus on dropping data.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets first look at an example. Here is a DataFrame named study_data containing a list of first names, birth dates, and blood types.  Additional, a DataFrame named categories, containing the correct possible categories for the bloodtype column has been created as well.  Notice the inconsistency here?  There's definitely no bloodtype named Z+.  Luckily, the categories DataFrame will help us systematically spot all rows with these inconsistencies.  \n",
    "\n",
    "# Its always good practice to keep a log of all possible values of your categorical data, as it will make dealing with these types of inconsistencies way easier.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Now before moving on to dealing with these inconsistent values, lets have a brief reminder on joins.  The two main types of joins we care about here are anti-joins and inner-joins.  We join DataFrames on common columns between them. \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The anti-joins take in 2 DataFrames A and B, and return data from 1 DataFrame that is not contained in another. \n",
    "Imagine a example we performing a left anti-joins of DF A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on.  \n",
    "\n",
    "Inner-joins returns only the data that is contained DataFrames.  For example, an inner-join of A and B would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "In our example, an left anti-join essentially returns all the data in study data with inconsistent bloodtypes, and an inner-join returns all the rows containing consistent bloodtypes.  Now lets see how to do that in Python.  We first gety all inconsistent categories in the blood_type column of the study_data DataFrame.  We do that by creating a set of the blood_type column which stores its unique values, and use the \".difference()\" method which takes in as argument the blood_type column from the categories DataFrame.  This returns all the categories in blood_type that are not in categories.  We then find the inconsistent rows by finding all the rows of the blood_type columns that are equal to inconsistent categories by using the \".isin()\" method, this returns a series of boolean values that are True for inconsistent rows and False for consistent ones.  We then subset the study_data DF based on the boolean indexing, and viola we have our inconsistent data.  \n",
    "\n",
    "To drop inconsistent rows and keep ones that are only consistent.  We just use the tilde symbol (~) while subsetting which rturns everything except inconsistent rows.  \n",
    "\n",
    "\n",
    "Now that we know about treating categorical data, lets practice.  \n",
    "\n",
    "\n",
    "\n",
    "# Read study data and print in\n",
    "study_data = pd.read_csv('study.csv')\n",
    "print(stydy_data)\n",
    "\n",
    "-------------------------------------------------\n",
    "    name       | birthday     | bloodtype\n",
    "1   Beth       | 2019-10-20   | B-\n",
    "2   Ignatius   | 2020-07-08   | A-\n",
    "3   Paul       | 2019-08-12   | O+\n",
    "4   Helen      | 2019-03-17   | O-\n",
    "5   Jennifer   | 2019-12-17   | Z+   <---\n",
    "6   Kennedy    | 2020-04-27   | A+\n",
    "7   Keityh     | 2019-04-19   | AB+\n",
    "\n",
    "\n",
    "# Correct possible blood types\n",
    "print(categories)\n",
    "\n",
    "-----------------------\n",
    "    bloodtype\n",
    "1   O-\n",
    "2   O+\n",
    "3   A-\n",
    "4   A+\n",
    "5   B+\n",
    "6   B-\n",
    "7   AB+\n",
    "8   AB-\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "inconsistent_categories = set(study_data['bloodtype']).difference(categories['bloodtype'])  #########################\n",
    "print(inconsistent_categories)\n",
    "\n",
    "{'z'}\n",
    "\n",
    "# Get and print rows with inconsistent categories\n",
    "inconsistens_rows = study_data['bloodtype'].isin(inconsistent_categories)\n",
    "\n",
    "inconsistene_data = study_data[inconsistent_rows]\n",
    "\n",
    "# Drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4afd28-8a3b-41ab-8105-1aa1a61934b8",
   "metadata": {},
   "source": [
    "## Members only\n",
    "\n",
    "Throughout the course so far, you've been exposed to some common problems that you may encounter with your data, from data type constraints, data range constrains, uniqueness constraints, and now membership constraints for categorical values.\n",
    "\n",
    "In this exercise, you will map hypothetical problems to their respective categories.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map the data problem observed with the correct type of data problem.\n",
    "\n",
    "Hint\n",
    "\n",
    "#    Remember, a membership constraint is when a categorical column has values that are not in the predefined set of categories of your column.\n",
    "\n",
    "Other Constraint: \n",
    "     A \"revenue\" column represented as a string\n",
    "     A \"birthdate\" column with value in the future\n",
    "     An \"age\" column with value above \"130\"\n",
    "\n",
    "\n",
    "Membership Constraint:\n",
    "     A \"month\" column with the value \"14\"\n",
    "     A \"day_of_week\" column with the value \"Suntermonday\"\n",
    "     A \"GPA\" column containing a \"z-\" grade\n",
    "     A \"has_loan\" column with the value \"12\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28556e-d74e-41e3-8c6b-62f9d05f4478",
   "metadata": {},
   "source": [
    "## Finding consistency\n",
    "\n",
    "In this exercise and throughout this chapter, you'll be working with the airlines DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named categories was created, containing all correct possible values for the survey columns.\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise. The pandas package has been imported as pd, and the airlines and categories DataFrames are in your environment.\n",
    "Instructions 1/4\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.\n",
    "[    Print the unique values of the survey columns in airlines using the \".unique()\" method].\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Print the unique values of the survey columns in airlines using the .unique() method.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Find rows of airlines with a cleanliness value not in categories and print the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1d166ab-1eb1-4535-a087-7e5489cbf7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        country    capital    area  population\n",
      "0        Brazil   Brasilia   8.516      200.40\n",
      "1        Russia     Moscow  17.100      143.50\n",
      "2         India  New Delhi   3.286     1252.00\n",
      "3         China    Beijing   9.597     1357.00\n",
      "4  South Africa   Pretoria   1.221       52.98\n"
     ]
    }
   ],
   "source": [
    "dict_val_lis = {'col_key1': ['val_11', 'val_12', 'val_13', 'val_14'],\n",
    "                'col_key2': ['val_21', 'val_22', 'val_23', 'val_24'],\n",
    "                'col_key3': ['val_31', 'val_32', 'val_33', 'val_34']}\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "list_dict = [{'country': 'Brazil', 'capital': 'Brasilia', 'area': 8.516, 'population': 200.4}, \n",
    "             {'country':'Russia', 'capital':'Moscow', 'area':17.10, 'population':143.5}, \n",
    "             {'country':'India', 'capital':'New Delhi', 'area':3.286, 'population':1252}, \n",
    "             {'country':'China', 'capital':'Beijing', 'area':9.597, 'population':1357}, \n",
    "             {'country':'South Africa','capital':'Pretoria','area':1.221, 'population':52.98}]\n",
    "\n",
    "import pandas as pd\n",
    "outcome = pd.DataFrame(list_dict)\n",
    "print(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7e8e25d-7700-4e54-b309-99f3250c0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        East US       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  \n",
      "0      Very satisfied  \n",
      "1      Very satisfied  \n",
      "2             Neutral  \n",
      "3  Somewhat satsified  \n",
      "4  Somewhat satsified   \n",
      "\n",
      "          id       day        airline destination dest_region dest_size  \\\n",
      "2805  2222.0  Thursday      SOUTHWEST     PHOENIX     West US       Hub   \n",
      "2806  2684.0    Friday         UNITED     ORLANDO     East US       Hub   \n",
      "2807  2549.0   Tuesday        JETBLUE  LONG BEACH     West US     Small   \n",
      "2808  2162.0  Saturday  CHINA EASTERN     QINGDAO        Asia     Large   \n",
      "0        NaN       NaN            NaN         NaN         NaN       NaN   \n",
      "\n",
      "     boarding_area   dept_time  wait_min cleanliness         safety  \\\n",
      "2805   Gates 20-39  2018-12-31     165.0       Clean      Very safe   \n",
      "2806   Gates 70-90  2018-12-31      92.0       Clean      Very safe   \n",
      "2807    Gates 1-12  2018-12-31      95.0       Clean  Somewhat safe   \n",
      "2808    Gates 1-12  2018-12-31     220.0       Clean      Very safe   \n",
      "0              NaN         NaN       NaN        cool   good quality   \n",
      "\n",
      "            satisfaction  \n",
      "2805      Very satisfied  \n",
      "2806      Very satisfied  \n",
      "2807      Very satisfied  \n",
      "2808  Somewhat satsified  \n",
      "0              best ever  \n",
      "\n",
      "\n",
      "<class 'str'> \n",
      "\n",
      "['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty' 'cool']\n",
      "['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'\n",
      " 'good quality']\n",
      "['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied' 'best ever'] \n",
      "\n",
      "      cleanliness           safety          satisfaction\n",
      "0           Clean        Very safe        Very satisfied\n",
      "1  Somewhat clean    Somewhat safe    Somewhat satsified\n",
      "2         Average          Neutral               Neutral\n",
      "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
      "4           Dirty  Somewhat unsafe      Very unsatisfied \n",
      "\n",
      "{'cool'} \n",
      "\n",
      "2804    False\n",
      "2805    False\n",
      "2806    False\n",
      "2807    False\n",
      "2808    False\n",
      "0        True\n",
      "Name: cleanliness, dtype: bool \n",
      "\n",
      "          id       day        airline   destination dest_region dest_size  \\\n",
      "2804  1475.0   Tuesday         ALASKA  NEW YORK-JFK     East US       Hub   \n",
      "2805  2222.0  Thursday      SOUTHWEST       PHOENIX     West US       Hub   \n",
      "2806  2684.0    Friday         UNITED       ORLANDO     East US       Hub   \n",
      "2807  2549.0   Tuesday        JETBLUE    LONG BEACH     West US     Small   \n",
      "2808  2162.0  Saturday  CHINA EASTERN       QINGDAO        Asia     Large   \n",
      "\n",
      "     boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "2804   Gates 50-59  2018-12-31     280.0  Somewhat clean        Neutral   \n",
      "2805   Gates 20-39  2018-12-31     165.0           Clean      Very safe   \n",
      "2806   Gates 70-90  2018-12-31      92.0           Clean      Very safe   \n",
      "2807    Gates 1-12  2018-12-31      95.0           Clean  Somewhat safe   \n",
      "2808    Gates 1-12  2018-12-31     220.0           Clean      Very safe   \n",
      "\n",
      "            satisfaction  \n",
      "2804  Somewhat satsified  \n",
      "2805      Very satisfied  \n",
      "2806      Very satisfied  \n",
      "2807      Very satisfied  \n",
      "2808  Somewhat satsified  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv', index_col=0)\n",
    "print(airlines.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "dirty_airlines = pd.DataFrame({'cleanliness': ['cool'], 'safety': ['good quality'], 'satisfaction': ['best ever']})\n",
    "\n",
    "airlines = pd.concat([airlines, dirty_airlines])#, ignore_index=True)  ##############################################\n",
    "#####################################################################################################################\n",
    "\n",
    "print(airlines.tail())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(type(airlines['cleanliness'].unique()[0]), '\\n')\n",
    "print(airlines['cleanliness'].unique())\n",
    "print(airlines['safety'].unique())\n",
    "print(airlines['satisfaction'].unique(), \"\\n\")\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "categories = pd.DataFrame(\n",
    "    {'cleanliness': ['Clean', 'Somewhat clean', 'Average', 'Somewhat dirty', 'Dirty'], \n",
    "     'safety': ['Very safe', 'Somewhat safe', 'Neutral', 'Very unsafe', 'Somewhat unsafe'], \n",
    "     'satisfaction': ['Very satisfied', 'Somewhat satsified', 'Neutral', 'Somewhat unsatisfied', 'Very unsatisfied']}\n",
    ")\n",
    "\n",
    "print(categories, '\\n')\n",
    "\n",
    "#print(airlines[['cleanliness', 'safety', 'satisfaction']].unique())\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "# So lets image we'll drop every row when a column has inconsistent value\n",
    "#####################################################################################################################\n",
    "bad_cleanliness = set(airlines['cleanliness']).difference(categories['cleanliness'])  ###############################\n",
    "print(bad_cleanliness, '\\n')\n",
    "\n",
    "inconsistens_cleanliness_rows = airlines['cleanliness'].isin(bad_cleanliness)\n",
    "\n",
    "print(inconsistens_cleanliness_rows[-6:], '\\n')\n",
    "\n",
    "good_cleanliness = airlines[~inconsistens_cleanliness_rows]\n",
    "print(good_cleanliness.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8799349-f287-42eb-9709-6419ba8ff593",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Find the cleanliness category in airlines not in categories\n",
    "    cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "    # Find rows with that category\n",
    "    cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "    # Print rows with inconsistent category\n",
    "    print(airlines[cat_clean_rows])\n",
    "\n",
    "    -> airlines의 cleanliness 컬럼의 중복을 제거한 리스트로 만들어, 차집합을 통해 겹치지 않는 값을\n",
    "    cat_clean에 저장한다. 이때 겹치는 값을 제외한 Unacceptable만 저장된다.\n",
    "\n",
    "    -> 그리고 cleanliness 컬럼의 값이 cat_clean에 포함되어있는지를 불리언의 형태로\n",
    "    cat_clean_rows에 저장하였다.\n",
    "\n",
    "    -> 마지막으로 이 값들이 포함되어있는 행들을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f99ad-2e03-44ee-9acf-22e63ee6f779",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Awesome work on the last lesson.  Now lets discuss other types of problems that could affect categorical variables.  In the last lesson, we saw how categorical data has a value membership constraint, where columns need to have a predefined set of values.  However this is not the only set of problems we may encounter.  \n",
    "\n",
    "When cleaning categorical data, some of the problems we may encounter include value inconsistency, the presence of too many categories that could be collapsed into one, and making sure data is of the right type.  Lets start with making sure our categorical data is consistent.  \n",
    "\n",
    "# A common categorical data problem is having values that slightly differ because of capitalizations.  \n",
    "Not treating this could lead tyo misleading results when we dicide to analyze the data, for example, lets assume we're working with a demographics dataset, and we have a marriage status column with inconsistent capitalization.  Below is what counting the number of married people in the marriage_status Series would look like.  \n",
    "\n",
    "Note that [.value_counts()] method works on Series only.  For a DF, we can [df.groupby()] the column and use the [.count()] method.  #################################################################################################\n",
    "\n",
    "To deal with this, we can either capitalize or lowercase the marriage_status column.  This can be done with the \"str.upper()\" or \"str.lower()\" function respectively.  \n",
    "\n",
    "\n",
    "# Another common problem with categorical values are leading or trailing spaces.  \n",
    "For example, imagine the same demographics DF containing values with leading spaces.  Below is what the counts of married vs unmarried people would look like.  Note that there is a married category with a trailing space on the right, which makes it hard to sport on the output, as opposed to unmarried.  \n",
    "\n",
    "To remove leading and trailing spaces, we can use the \"str.strip()\" method, which when giving no input, strips all leading and trailing white spaces.  \n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Sometimes, we may want to create categories out of our data, such as creating household income groups from income data.  \n",
    "To create categories out of data, lets use the example of creating an income group column in the demographics DataFrame.  (do you remember how we did this using \"pd.cut()\" & \"pd.qcut()\")  \n",
    "\n",
    "# \"pd.qcut(df['column'], q=, labels=)\"\n",
    "We can do this in 2 ways.  The first method utilizes the \"pd.qcut()\" function from Pandas, which automatically divides our data based on its distribution into the number of categories we set in the \"q=\" argument, we created the category names in the group_names list and fed it to \"labels=\" argument, returning the following.  \n",
    "\n",
    "Notice the first row actually misrepresents the actual income of the income group, as we didn't instruct qcut where our ranges actually lie.  \n",
    "\n",
    "# \"pd.cut(df['column'], bins=, labels=)\"\n",
    "We can do this with \"pd.cut()\" function instead, which lets us define category cutoff ranges with the bins argument.  It takes in a list of cutoff points for each category, with the final one being infinity represented with \"np.inf()\" from NumPy.  From the output, we can we can see that this is much more correct.  \n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Sometimes, we may want to reduce the amount of categories we have in our data.  Lets move on to mapping categories to fewer ones.  \n",
    "\n",
    "For example, assume we have a column containing the operating system of different devices, and contains these unique values: Microsoft, MacOS, IOS, Android, Linux.  Say we want to collapse these categories into 2, DesktopOS, and MobileOS.  \n",
    "\n",
    "# We can do this by using the \".replace()\" method.  \n",
    "It takes in a dictionary that maps each existing category to the category name you desire.  In this case, this is a mapping dictionary.  A quick print of the unique values of operating system shows the mapping has been complete.  \n",
    "\n",
    "\n",
    "Now that we know about treating categorical data, lets practice.  \n",
    "\n",
    "\n",
    "\n",
    " (1) Value inconsistency\n",
    "     Inconsistent fields: 'married', 'Maried', 'UNMARRIED', 'not married' ...\n",
    "     Trailing white spaces: 'married', ' married '\n",
    "(11) Collapsing too many categories to few\n",
    "     Creating new groups: \"0-20k\", \"20-40k\" categories from continuous household income data\n",
    "     Mapping groups to new ones: Mapping household income categories to \"rich\", \"poor\"\n",
    "\n",
    "\n",
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "\n",
    "marriage_status.value_counts()\n",
    "\n",
    "unmarried   352\n",
    "married     268\n",
    "MARRIED     204\n",
    "UNMARRIED   176\n",
    "dtype: int64\n",
    "\n",
    "\n",
    " unmarried   352\n",
    "married      268\n",
    "MARRIED      204\n",
    "UNMARRIED    176\n",
    "dtype: int64\n",
    "\n",
    "\n",
    "\n",
    "# Using  \"pd.qcut()\"\n",
    "group_names = ['0-200k', '200-500k', '500k+']  ######################################################################\n",
    "\n",
    "demographics[\"income_group\"] = pd.qcut(demographics['household_income'], q=3, labels=group_names)  ##################\n",
    "\n",
    "# Print income_group column\n",
    "demographics[['income_group', 'household_income']]\n",
    "\n",
    "-----------------------------------------\n",
    "    income_group  | household_income\n",
    "    200-500k      | 189243\n",
    "    500k+         | 778533\n",
    "    \n",
    "\n",
    "\n",
    "# Using \"pd.cut()\" - creat category ranges and names\n",
    "ranges = [0, 200000, 500000, np.inf]  ###############################################################################\n",
    "\n",
    "group_names = ['0-200k', '200-500k', '500k+']  ######################################################################\n",
    "\n",
    "# Create income group column\n",
    "demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
    "\n",
    "-----------------------------------------\n",
    "    income_group  | household_income\n",
    "    0-200k        | 189243\n",
    "    500k+         | 778533\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a mapping dictionary and replace\n",
    "mapping = {\"Microsoft\": \"DesktopOS\", \"MacOS\": \"DesktopOS\", \"Linux\": \"DesktopOS\", \"IOS\": \"MobileOS\", \"Android\": \"MobileOS\"}\n",
    "\n",
    "devices[\"operating_system\"] = devices[\"operating_system\"].replace(mapping) ##########################################\n",
    "#####################################################################################################################\n",
    "\n",
    "devices[\"operating_system\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bcecf-9e21-4829-94ff-14e12af6a85c",
   "metadata": {},
   "source": [
    "## Categories of errors\n",
    "\n",
    "In the video exercise, you saw how to address common problems affecting categorical variables in your data, including white spaces and inconsistencies in your categories, and the problem of creating new categories and mapping existing ones to new ones.\n",
    "\n",
    "To get a better idea of the toolkit at your disposal, you will be mapping functions and methods from pandas and Python used to address each type of problem.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map each function/method to the categorical data problem it solves.\n",
    "\n",
    "White spaces and inconsistency:\n",
    "     .str.upper()\n",
    "     .str.lower()\n",
    "     .str.strip()\n",
    "\n",
    "Creating or remapping categories:\n",
    "     pd.cut()\n",
    "     .replace()\n",
    "     pd.qcut()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee7ece-abbd-48cb-a60f-c28674df7348",
   "metadata": {},
   "source": [
    "##  Inconsistent categories\n",
    "\n",
    "In this exercise, you'll be revisiting the airlines DataFrame from the previous lesson.\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport.\n",
    "\n",
    "In this exercise, you will examine two categorical columns from this DataFrame, dest_region and dest_size respectively, assess how to address them and make sure that they are cleaned and ready for analysis. The pandas package has been imported as pd, and the airlines DataFrame is in your environment.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the unique values in \"dest_region\" and \"dest_size\" respectively.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    From looking at the output, what do you think is the problem with these columns?\n",
    "Possible Answers\n",
    "    The \"dest_region\" column has only inconsistent values due to capitalization.\n",
    "#    The \"dest_region\" column has inconsistent values due to capitalization and has one value that needs to be remapped.    yes, the 'euro' should be mapped to 'Europe'\n",
    "    The dest_size column has only inconsistent values due to leading and trailing spaces.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "[    Change the capitalization of all values of \"dest_region\" to lowercase].\n",
    "[    Replace the 'eur' with 'europe' in dest_region using the \".replace()\" method].\n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "[    Strip white spaces from the \"dest_size\" column using the \".strip()\" method].\n",
    "    Verify that the changes have been into effect by printing the unique values of the columns using .unique() .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc1a7758-e6a6-4ce1-8f43-63364abd6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        East US       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  \n",
      "0      Very satisfied  \n",
      "1      Very satisfied  \n",
      "2             Neutral  \n",
      "3  Somewhat satsified  \n",
      "4  Somewhat satsified   \n",
      "\n",
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east'] \n",
      "\n",
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand'] \n",
      "\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     '] \n",
      "\n",
      "['Hub', 'Small', 'Medium', 'Large']\n",
      "Categories (4, object): ['Hub', 'Large', 'Medium', 'Small'] \n",
      "\n",
      "category\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv', index_col=0)\n",
    "print(airlines.head(), '\\n')\n",
    "\n",
    "\n",
    "print(airlines['dest_region'].unique(), '\\n')\n",
    "\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur': 'europe'})  #######################################\n",
    "\n",
    "\n",
    "print(airlines['dest_region'].unique(), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "print(airlines['dest_size'].unique(), '\\n')\n",
    "\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()  ##########################################################\n",
    "\n",
    "airlines['dest_size'] = airlines['dest_size'].astype('category')  ###################################################\n",
    "\n",
    "print(airlines['dest_size'].unique(), '\\n')\n",
    "print(airlines['dest_size'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24f943-27c0-40ef-aed3-12d366c1f90f",
   "metadata": {},
   "source": [
    "## Remapping categories\n",
    "\n",
    "To better understand survey respondents from airlines, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The airlines DataFrame contains the \"day\" and \"wait_min\" columns, which are categorical and numerical respectively. The day column contains the exact day a flight took place, and wait_min contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "\n",
    "    wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+\n",
    "    day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend.\n",
    "\n",
    "The pandas and numpy packages have been imported as pd and np. Let's create some new categorical data!\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create the ranges and labels for the \"wait_type\" column mentioned in the description.\n",
    "[    Create the \"wait_type\" column by from wait_min by using \"pd.cut()\", while inputting \"label_ranges\" and \"label_names\" in the correct arguments].\n",
    "[    Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'].\n",
    "    Create the day_week column by using .replace().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a3b33e9-e9d2-4109-bc9f-096205e7e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  canada/mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        west us       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        west us       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        east us       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  \n",
      "0      Very satisfied  \n",
      "1      Very satisfied  \n",
      "2             Neutral  \n",
      "3  Somewhat satsified  \n",
      "4  Somewhat satsified   \n",
      "\n",
      "object \n",
      "\n",
      "['Tuesday' 'Friday' 'Thursday' 'Wednesday' 'Saturday' 'Sunday' 'Monday'] \n",
      "\n",
      "category \n",
      "\n",
      "['weekday', 'weekend']\n",
      "Categories (2, object): ['weekend', 'weekday'] \n",
      "\n",
      "['medium', 'long', 'short']\n",
      "Categories (3, object): ['short' < 'medium' < 'long'] \n",
      "\n",
      "        id        day         airline      destination  dest_region dest_size  \\\n",
      "1844  2676    Tuesday       SOUTHWEST           DENVER      west us       Hub   \n",
      "1241  1755     Monday       LUFTHANSA           MUNICH       europe       Hub   \n",
      "1664   733   Thursday  AIR FRANCE/KLM  PARIS-DE GAULLE       europe       Hub   \n",
      "2448  2370  Wednesday          UNITED           AUSTIN   midwest us    Medium   \n",
      "2226  9003    Tuesday          ALASKA         PORTLAND      west us    Medium   \n",
      "603   1666   Saturday        EMIRATES            DUBAI  middle east       Hub   \n",
      "2147   442     Sunday          UNITED         HONOLULU      west us    Medium   \n",
      "\n",
      "     boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "1844   Gates 20-39  2018-12-31     170.0           Clean      Very safe   \n",
      "1241  Gates 91-102  2018-12-31     235.0         Average  Somewhat safe   \n",
      "1664    Gates 1-12  2018-12-31     155.0         Average  Somewhat safe   \n",
      "2448   Gates 70-90  2018-12-31     210.0  Somewhat clean      Very safe   \n",
      "2226   Gates 50-59  2018-12-31     124.0  Somewhat clean      Very safe   \n",
      "603     Gates 1-12  2018-12-31     205.0  Somewhat clean        Neutral   \n",
      "2147   Gates 70-90  2018-12-31     167.0           Clean      Very safe   \n",
      "\n",
      "            satisfaction wait_type day_week  \n",
      "1844      Very satisfied    medium  weekday  \n",
      "1241             Neutral      long  weekday  \n",
      "1664             Neutral    medium  weekday  \n",
      "2448  Somewhat satsified      long  weekday  \n",
      "2226  Somewhat satsified    medium  weekday  \n",
      "603   Somewhat satsified      long  weekend  \n",
      "2147      Very satisfied    medium  weekend  \n"
     ]
    }
   ],
   "source": [
    "print(airlines.head(), '\\n')\n",
    "\n",
    "\n",
    "print(airlines['day'].dtypes, '\\n')\n",
    "\n",
    "print(airlines['day'].unique(), '\\n')\n",
    "\n",
    "airlines['day'] = airlines['day'].astype('category')\n",
    "print(airlines['day'].dtypes, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "wait_ranges = [0, 60, 180, np.inf]   ################################################################################\n",
    "wait_labels = ['short', 'medium', 'long']\n",
    "\n",
    "#####################################################################################################################\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], \n",
    "                               bins=wait_ranges, \n",
    "                               labels=wait_labels) \n",
    "\n",
    "\n",
    "\n",
    "day_mapping = {'Tuesday':'weekday', 'Friday':'weekday', 'Thursday':'weekday',  'Wednesday':'weekday',\n",
    "               'Saturday':'weekend', 'Sunday':'weekend', 'Monday':'weekday'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(day_mapping)  ########################################################\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "print(airlines['day_week'].unique(), '\\n')\n",
    "print(airlines['wait_type'].unique(), '\\n')\n",
    "\n",
    "print(airlines.sample(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2a496-58d1-4a3e-97b4-70c8635ab3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, ____, np.inf]\n",
    "label_names = ['short', ____, ____]\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.____(____, bins = ____, \n",
    "                                labels = ____)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'____', 'Wednesday': '____', \n",
    "            'Thursday': '____', '____': '____', \n",
    "            'Saturday': 'weekend', '____': '____'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].____(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d526058-b3e2-4c3c-8a9e-0baa214a71ee",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Good job on the previous lesson.  In the final lesson of this chapter, we'll talk about [text data and regular expressions].  Text data is on of the most common types of data types.  Examples of it range from names, phone numbers, addresses, emails and more.  \n",
    "\n",
    "Common text data problems include handling inconsistencies, making sure text data is of a certain length, typos and others.  Lets take a look at the following example.  Below is a DF named phones containing the full name and phone numbers of individuals.  Both are string columns.  Notice the phone number column.  We can see that there are phone number values, that begin with 00 or +.  We also see  that there are one of entry where the phone number is 4 digits, which is non-existent.  Furthermore, we can see that there are dashes across the phone number column.  \n",
    "\n",
    "If we wanted to feed these phone numbers into an automated call system, or create a report discussing the distribution of users by area code, we couldn't really do so without uniform phone numbers.  \n",
    "\n",
    "Ideally, we'd want to the phone number column as such.  Where all phone numbers are aligned to begin with 00, where any number below the 10 digit value is replaced with NaN to represent a missing value, and where all dashes have been removed.  Lets see how thats done.  \n",
    "\n",
    "[ \".str.replace()\" on string data and \".str.len()\" to subset & indexi damaged data to fill as NaN]\n",
    "# *******************************************************************************************************************\n",
    "Lets first begin by [replacing the plus sign with 00], to do this, we use the [.str.replace()] method with takes in 2 values, the string being replaced, which is in this case the plus (+) sign and the string to replace it with which is is this case 00.  We can see that the column has been updated.  We use the same exact technique to dashes, by replacing the dash symbol with an empty string.  Now finally, we're going to replace all phone numbers below 10 digits to NaN.  We can do this by chaining the Phone number column with the [.str.len()], which returns the string length of each row in the column.  We can he use the \".loc[]\" method, to index rows where digits is below 10, and replace the value of Phone number with NumPy's nan object.  \n",
    "# df.loc[df[col].str.len()<10, col]=np.nan  \n",
    "\n",
    "We can also write assert statement top test whether the Phone number column has a specific length, and whether it contains the symbols we removed.  The first assert statement tests that the minimum length of the strings in the Phone number column, found through str.len(), is bigger than or equal to 10.  In the second assert statement, we use the \"str.contains()\" methpd to test wheather the Phone number column contains a specific pattern.  It returns a series of booleans that are Ture for matches and False for non-matches.  We set the pattern \"+|-\", the bar (|) pipe here is basically an or statement, so we're trying to find matches for either symbols.  We chain it with the \"any()\" method which returns True if any element in the output of our \".str.contains()\" is True, and test whether it returns False.  \n",
    "\n",
    "# Regular Expressions gives us the ability to search for any pattern in text data\n",
    "But what about more complicated examples?  How can we clean a Phone number column that looks like below fopr example?  Where Phone numbers can contain a range of symbols from plus (+) signs, dashes (-), parenthesis (({) and maybe more.  This is where Regular Expressions come in.  Regular Expressions gives us the ability to search for any pattern in text data, like only digits for example.  They are likely control (+) find in your browser, but way more dynamic and robust.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take a look at this example.  Here we are attempting to only extract digits from the Phone number column.  To do this, we use the [.str.replace()] method with the pattern we want to replace with an empty string.  Notice the pattern fed into the method.  This is essentially us telling Pandas to replace anything that is not digt with nothing.  [.str.replace(r'\\D+', '']  We won't get into the specifics of [Regular Expressions], and how to construct them, but they are immensely useful for difficult string cleaning tasks, so make sure to check out DataCamp's course library on Regular Expressions.  \n",
    "\n",
    "Now that we know how to clean text data, lets get to practice.  \n",
    "\n",
    "\n",
    "\n",
    " (1) Data inconsistency\n",
    "     \"+96171679912\" or \"0096171679912\"\n",
    " (2) Fixed length violations:\n",
    "     Passwords needs to be at least 8 characters\n",
    " (3) Typos:\n",
    "     \"+961.71.679912\"\n",
    "\n",
    "\n",
    "phones = pd.read_csv('phones.csv')\n",
    "print(phones)\n",
    "\n",
    "--------------------------------------------\n",
    "             Full name |       Phone number\n",
    "       Noelani A. Gray |   001-702-397-5143\n",
    "        Myles Z. Gomez |   001-329-485-0540\n",
    "          Gil B. Silva |   001-195-492-2338\n",
    "    Prescott D. Hardin |    +1-297-996-4904\n",
    "    Benedict G. Valdaz |   001-969-820-3536\n",
    "      Reece M. Andrews |               4138\n",
    "        Harfa E. Keith |   001-536-175-8444\n",
    "\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('+', '00')  #############################################\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('-', '')\n",
    "\n",
    "phones['Phone number'] = phones.loc[len(phones['Phone number'])<10, 'Phone number'] = 'Nan'  ##### Take a try and see\n",
    "\n",
    "\n",
    "phones.loc[phones['Phone number'].str.len()<10, 'Phone number'] = np.nan  ###########################################\n",
    "\n",
    "assert phones['Phone number'].str.len.min() < 10\n",
    "\n",
    "\n",
    "\n",
    "# Assert all numbers do not have \"+\" or \"-\" symbol\n",
    "assert phpnes['Phone number'].str.contains('+|-').any() == False  ###################################################\n",
    "\n",
    "--------------------------------------------\n",
    "            Full name  |       Phone number\n",
    "      Noelani A. Gray  |      0017023975143\n",
    "        Myles Z. Gomez |      0013294850540\n",
    "          Gil B. Silva |      0011954922338\n",
    "    Prescott D. Hardin |      0012979964904\n",
    "    Benedict G. Valdaz |      0019698203536\n",
    "      Reece M. Andrews |                NaN\n",
    "        Harfa E. Keith |      0015361758444\n",
    "        \n",
    "\n",
    "--------------------------------------------\n",
    "             Full name |       Phone number\n",
    "       Noelani A. Gray |     +(01706)-25891\n",
    "        Myles Z. Gomez |       +0500-571437\n",
    "          Gil B. Silva |         +0800-1111\n",
    "    Prescott D. Hardin |      +07058-879063\n",
    "    Benedict G. Valdaz |     +(016799)-8424\n",
    "\n",
    "\n",
    "# Replace letters with nothing\n",
    "phones['Phone number'] = phones['Phone number'].str.replace(r'\\D+', '')  ############################################\n",
    "\n",
    "--------------------------------------------\n",
    "             Full name |       Phone number\n",
    "       Noelani A. Gray |         0170625891\n",
    "        Myles Z. Gomez |         0500571437\n",
    "          Gil B. Silva |           08001111\n",
    "    Prescott D. Hardin |        07058879063\n",
    "    Benedict G. Valdaz |         0167998424\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75e7cd2b-ca7b-423d-9d52-e42c741e8e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Full name      Phone number\n",
      "0     Noelani A. Gray  001-702-397-5143\n",
      "1      Myles Z. Gomez  001-329-485-0540\n",
      "2        Gil B. Silva  001-195-492-2338\n",
      "3  Prescott D. Hardin   +1-297-996-4904\n",
      "4  Benedict G. Valdaz  001-969-820-3536 \n",
      "\n",
      "object \n",
      "\n",
      "            Full name   Phone number  Phone\n",
      "0     Noelani A. Gray  0017023975143  False\n",
      "1      Myles Z. Gomez  0013294850540  False\n",
      "2        Gil B. Silva  0011954922338  False\n",
      "3  Prescott D. Hardin    12979964904  False\n",
      "4  Benedict G. Valdaz  0019698203536  False\n",
      "5    Reece M. Andrews           4138  False\n",
      "6      Harfa E. Keith  0015361758444  False \n",
      "\n",
      "object\n",
      "            Full name   Phone number  Phone\n",
      "0     Noelani A. Gray  0017023975143  False\n",
      "1      Myles Z. Gomez  0013294850540  False\n",
      "2        Gil B. Silva  0011954922338  False\n",
      "3  Prescott D. Hardin    12979964904  False\n",
      "4  Benedict G. Valdaz  0019698203536  False\n",
      "5    Reece M. Andrews            NaN  False\n",
      "6      Harfa E. Keith  0015361758444  False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "phones = pd.read_csv('phones.csv')\n",
    "print(phones.head(), '\\n')\n",
    "\n",
    "print(phones['Phone number'].dtypes, '\\n')\n",
    "\n",
    "\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('-', '', regex=True)\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('+', '', regex=True)  ### lets say we do it another way\n",
    "# FutureWarning: The default value of regex will change from True to False in a future version. \n",
    "# In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
    "\n",
    "\n",
    "\n",
    "phones['Phone'] = phones['Phone number'].str.contains('+|-', regex=False)#.any()  ###################################\n",
    "#phones.loc[phones['Phone number'].str.contains('+', regex=0)==True, 'Phone number']=111   ##########################\n",
    "print(phones, '\\n')\n",
    "# Return boolean Series or Index based on whether a given pattern or regex is contained in strings of Series / Index\n",
    "\n",
    "print(phones['Phone number'].dtypes)   # Object means string in Pandas DataFrame\n",
    "\n",
    "\n",
    "phones.loc[phones['Phone number'].str.len()<10, 'Phone number'] = np.nan  #------------------------------------------\n",
    "print(phones, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b68014c2-68eb-4660-b479-9d36855a0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__frozen', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_data', '_doc_args', '_freeze', '_get_series_list', '_index', '_inferred_dtype', '_is_categorical', '_is_string', '_name', '_orig', '_parent', '_validate', '_wrap_result', 'capitalize', 'casefold', 'cat', 'center', 'contains', 'count', 'decode', 'encode', 'endswith', 'extract', 'extractall', 'find', 'findall', 'fullmatch', 'get', 'get_dummies', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'islower', 'isnumeric', 'isspace', 'istitle', 'isupper', 'join', 'len', 'ljust', 'lower', 'lstrip', 'match', 'normalize', 'pad', 'partition', 'removeprefix', 'removesuffix', 'repeat', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'slice', 'slice_replace', 'split', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'wrap', 'zfill'] \n",
      "\n",
      "['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__func__', '__ge__', '__get__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__self__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__wrapped__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(phones['Phone number'].str), '\\n')\n",
    "\n",
    "#help(phones['Phone number'].str.contains)\n",
    "\n",
    "print(dir(phones['Phone number'].str.contains))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b265bb-c750-480f-962a-32eb6d7ef031",
   "metadata": {},
   "source": [
    "## Removing titles and taking names\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "While collecting survey respondent metadata in the airlines DataFrame, the full name of respondents was saved in the \"full_name\" column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as \"Dr.\", \"Mr.\", \"Ms.\" and \"Miss\".\n",
    "\n",
    "Your ultimate objective is to create two new columns named first_name and last_name, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "The airlines DataFrame is in your environment, alongside pandas as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Remove \"Dr.\", \"Mr.\", \"Miss\" and \"Ms.\" from full_name by replacing them with an empty string \"\" in that order.\n",
    "    Run the assert statement using .str.contains() that tests whether full_name still contains any of the honorifics.\n",
    "\n",
    "Hint\n",
    "\n",
    "    The .str.replace() method takes in the pattern to find, and the pattern to replace it by.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a518b0ac-37a2-4e74-a90c-e5c81b1303e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        East US       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  \n",
      "0      Very satisfied  \n",
      "1      Very satisfied  \n",
      "2             Neutral  \n",
      "3  Somewhat satsified  \n",
      "4  Somewhat satsified   \n",
      "\n",
      "['Tuesday' 'Friday' 'Thursday' 'Wednesday' 'Saturday' 'Sunday' 'Monday']\n",
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        East US       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  day_week  \n",
      "0      Very satisfied         1  \n",
      "1      Very satisfied         1  \n",
      "2             Neutral         1  \n",
      "3  Somewhat satsified         1  \n",
      "4  Somewhat satsified         1  \n"
     ]
    }
   ],
   "source": [
    "airlines = pd.read_csv('airlines_final.csv', index_col=0)\n",
    "print(airlines.head(), '\\n')\n",
    "\n",
    "print(airlines['day'].unique())\n",
    "\n",
    "\n",
    "# Now recall what should we do ?????\n",
    "#####################################################################################################################\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':1, 'Tuesday':1, 'Wednesday': 1, 'Thursday': 1, 'Friday': 1, 'Saturday': 0, 'Sunday': 0}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)\n",
    "\n",
    "\n",
    "print(airlines.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3daf1b8-4b85-4545-a07d-8876d6f25dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")  ################################################\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = ____\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "____\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "____\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False  #++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c25e0-1b97-4d29-af26-7f8b9dc209cc",
   "metadata": {},
   "source": [
    "## Keeping it descriptive\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the \"survey_response\" column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than 40 , and make sure your new DataFrame contains responses with 40 characters or more using an [assert] statement to check if the outcome are expected.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "The airlines DataFrame is in your environment, and pandas is imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "[    Using the \"airlines\" DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len()].\n",
    "    Isolate the rows of airlines with resp_length higher than 40.\n",
    "    Assert that the smallest survey_response length in airlines_survey is now bigger than 40.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a84a05-ee49-4a27-a737-bc3636168454",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.loc[airlines['survey_response'].str.len<40, 'survey_response'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094aabf3-dbb9-4891-bc84-215be59c11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = ____\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[____ > ____]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert ____.str.len().____ > _____   ################################################################################\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bf200-a450-421c-888e-2fe940c1da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85a61272-f84d-45b7-8df3-0d3fa27a2d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 5, 7, 9}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "def find_z(n):\n",
    "    \n",
    "    z = []\n",
    "    for item in n:\n",
    "        if item == 1:\n",
    "            z.append(item)\n",
    "        if item == 2:\n",
    "            z.append(item)\n",
    "            \n",
    "        for i in range(2, item):\n",
    "            for j in range (2, i):\n",
    "                if i % j == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    z.append(i)\n",
    "            \n",
    "    return set(z)\n",
    "\n",
    "find_z(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8b76d-567b-403e-943c-c6798a518e00",
   "metadata": {},
   "source": [
    "## Uniformity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Stellar work on chapter 2.  \n",
    "# You're now an expert at handling categorical and text variables.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this chapter, we are looking at more advanced data clearning problems, such as [uniformity], [cross fields validation] and dealing with [missing data].  \n",
    "\n",
    "In chapter 1, we saw how out of range values are a common problem when clearning data, and that when left untouched, can skew your analysis.  In this lesson, we're going to tackle a problem that could similarly skew our data, which is [unit uniformity].  For example, we can have [temperature data that have values in both Fahrenheit and Celsius], [weight data in Kilograms and in stones], dates in multiple formats, and so on.  Verifying uniformity is imperative to having accurate analysis.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "Here is a dataset with average temperature data throughout the month of March in New York City.  The dataset was collected from different sources with temperature data in Celsius and Fahrenheit merged together.  We can see that unless a major climate event occurred, the value \"62.6\" is most likely Fahrenheit not Celsius.  \n",
    "\n",
    "Lets confirm the presence of these values visually.  We can do so by plotting a scatter plot of our data.  We can do this using Matplotlib.pyplot imported as plt.  Use the [plt.scatter(data=, x=, y=)] function, which takes in what to plot on the x axis, the y axis, and which data source to use.  We set the title, axis labels with the helper function below, then show the plot.  Notice the outer data points?  They all must be Fahrenheit.  A simple Google search returns the formula for converting Fahrenheit to Celsius.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To convert our temperature data, we isolate all rows of temperature column where it is above 40 using the [.loc[]] method.  We choose 40 because its a common sense maximum for Celsius temperature in New York City.  We then convert these values to Celsius using the formula and resign them to their respective Fahrenheit values in temperatures.  We can make sure that our conversion was correct with assert statement, by making sure the maximum value of temperatrure is less tha 40.  df.loc[df['temp']>40, temp]=????????.   Think about it  ++++++++++++++++++++++++++++++++++++++++++++\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "Here is another common uniformity problem with date data.  This is a DataFrame called birthdays containing birth dates for a variety of individuals.  It has been collected from a variety of sources and merged into one.  Notice the format of each oeservation in Birthday column, and even contains error data.  We'll learn how to deal with those.  \n",
    "\n",
    "[  \"pd.to_datetime()\" function accepts different formats, but]\n",
    "We already discussed datetime objects.  Without getting too much into details, datetime accepts different formats that help you format your dates as pleased.  The Pandas [pd.to_datetime()] function automatically accepts most date formats, but could raise errors when certain formats are unrecognizable.  You don't have to memorize these formats, just know that they exist amnd are easily searchable.  \n",
    "\n",
    "[ this isn't enough and will most likely return an error in real-world multiple formats circumstance]\n",
    "You can treat these date inconsistencies easily by converting your date column to datetime.  We can do this in Pandas use the \"pd.to_datetime()\" function mentioned above.  However this isn't enough and will most likely return an error, since we have dates in multiple formats, especially the weird day/day/year format - the error one, which triggers an error with months.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Instead we set the \"infer_datetime_format=\" arg equal to True and set the \"errors=\" arg equal top \"coerce\".  \n",
    "This will infer the format and return missing value for dates that couldn't be identified and converted instead of a value error.  This returns the birthday column with alligned formats, with the initial ambiguous format of day-day-year, being set to NaT, which represents missing values in Pandas for datetime objects.  \n",
    "\n",
    "# We can also convert the format of a datetime column using the \".dt.strftime()\" method, \n",
    "Which accetps a datetime format of your choice.  For example, here we converthe Birthday colum to day-month-year, instead of year-month-day.  However a common problem is having ambiguous dates with vague formats.  For example is this \"2019-03-08\" date value set in March or August?  Unfortunately there is no clear cut way to soprt this inconsistency or to treat it.  Depending on the size of the dataset and suspected ambiguities, we can either convert these dates to NAs ad deal with them accordingly.  Or if you have additional context on the source of your data, you can probably infer the format.  If the majority of subsequent or previous data is of one format, you can probably infer the format as well.  All in all, it is essential to properly understand where your data coes from, before trying to treat it, as it will make making these decisions much easier.  \n",
    "\n",
    "New lets make our data uniform.  \n",
    "\n",
    "\n",
    "\n",
    "temperatures = pd.read_csv(temperature.csv)\n",
    "temperature.head()\n",
    "\n",
    "-------------------------------------------\n",
    "   Date      | Temperature\n",
    "   03.03.19  | 14.0\n",
    "   04.03.19  | 15.0\n",
    "   05.03.19  | 18.0\n",
    "   06.03.19  | 16.0\n",
    "   07.03.19  | 62.6\n",
    "   \n",
    "   \n",
    "plt.scatter(data=temperatures, x='Date', y='Temperature')  ##########################################################\n",
    "plt.title()\n",
    "plt.xlable()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "temp_fah = temperature.loc[temperature['Temperature']>40, 'Temperaature']   #########################################\n",
    "\n",
    "temperature.loc[temperature['Temperature']>40, 'Temperaature'] = (temp_fah - 32) * (5/9)  ###########################\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------\n",
    "   Birthday         | First name   | Last name\n",
    "   27/27/19         | Rowan        | Nunex\n",
    "   03-29-19         | Brynn        | Yang\n",
    "   March 3rd, 2019  | Sophia       | Reilly\n",
    "   24-03-19         | Deacon       | Prince\n",
    "   06-03-19         | Griffith     | Neal\n",
    "   \n",
    "\n",
    "-----------------------------------------\n",
    "Date                | datetime format\n",
    "25-12-2019          | %d-%m-%Y\n",
    "December 25th 2019  | %c\n",
    "12-25-2019          | %m-%d-%Y\n",
    "...                 | ...\n",
    "\n",
    "pd.to_datetime() can recognize most formats automatically\n",
    "\n",
    "# Converts to datetime - but won't work cause the error data day-day-year\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'])\n",
    "  ValueError: month must be in 1..12\n",
    "  \n",
    "# Will work  ########################################################################################################\n",
    "birthday['Birthday'] = pd.to_datetime(birthday['Birthday'],   #######################################################\n",
    "                                         # Attempt to infer format of each date\n",
    "                                      infer_datetime_format=True,   #++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "                                         # Return NA fpr rows where conversion failed\n",
    "                                      errors='coerce')   #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01799846-2006-4145-9beb-7d1bfd87e068",
   "metadata": {},
   "source": [
    "## Ambiguous dates\n",
    "\n",
    "You have a DataFrame containing a \"subscription_date\" column that was collected from various sources with different Date formats such as YYYY-mm-dd and YYYY-dd-mm. What is the best way to unify the formats for ambiguous values such as 2019-04-07?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    Set them to NA and drop them.   press   # If we wish take the easy way\n",
    "    1\n",
    "    Infer the format of the data in question by checking the format of subsequent and previous values.   press   # Could be typo too\n",
    "    2\n",
    "    Infer the format from the original data source.   press   # If its provided\n",
    "    3\n",
    "#    All of the above are possible, as long as we investigate where our data comes from, and understand the dynamics affecting it before cleaning it.\n",
    "    4\n",
    "    \n",
    "Hint\n",
    "\n",
    "    Ambiguous date formats represent a data cleaning challenge that requires a solid understanding of where your data comes from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7139047-fad7-450c-849a-b01b1c924723",
   "metadata": {},
   "source": [
    "## Uniform currencies\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the banking DataFrame. The dataset contains data on the amount of money stored in accounts (acct_amount), their currency (acct_cur), amount invested (inv_amount), account opening date (account_opened), and last transaction date (last_transaction) that were consolidated from American and European branches.\n",
    "\n",
    "# You are tasked with understanding the average account size and how investments vary by the size of account, \n",
    "however in order to produce this analysis accurately, you first need to unify the currency amount into dollars. The pandas package has been imported as pd, and the banking DataFrame is in your environment.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Find the rows of \"acct_cur\" in banking that are equal to 'euro' and store them in the variable \"acct_eu\".\n",
    "#    Find all the rows of \"acct_amount\" in banking that fit the \"acct_eu\" condition, and convert them to USD by multiplying them with 1.1.\n",
    "    Find all the rows of \"acct_cur\" in banking that fit the \"acct_eu\" condition, set them to 'dollar'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "135d297b-8ea3-4c11-b672-fbcb3f4aebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cust_id  birth_date  age  acct_amount acct_cur  inv_amount   fund_A  \\\n",
      "0  870A9281  1962-06-09   54     63523.31   dollar    35500.50  30105.0   \n",
      "1  166B05B0  1962-12-16   36     38175.46   dollar    81921.86   4995.0   \n",
      "2  BFC13E88  1990-09-12   49     59863.77   dollar    46412.27  10323.0   \n",
      "3  F2158F66  1985-11-03   56     84132.10     euro    76563.35   3908.0   \n",
      "4  7A73F334  1990-05-17   21    120512.00     euro         NaN  12158.4   \n",
      "5  472341F2  1980-02-23   47     83127.65   dollar    93552.69  12686.0   \n",
      "6  6B094617  1977-08-26   53     89855.98   dollar    70357.70   1796.0   \n",
      "7  80C0DAB3  1963-10-14   29     73951.45     euro    14429.59  15290.0   \n",
      "8  E52D4C7F  1975-06-05   58     61795.89   dollar    51297.32  12939.0   \n",
      "\n",
      "    fund_B   fund_C   fund_D    account_opened last_transaction  \n",
      "0   4138.0   1420.0  15632.0          02-09-18         22-02-19  \n",
      "1    938.0   6696.0   2421.0          28-02-19         31-10-18  \n",
      "2   4590.0   8469.0   1185.0  January 26, 2018         02-04-18  \n",
      "3    492.0   6482.0  12830.0          21-14-17         08-11-18  \n",
      "4  51281.0  13434.0  18383.0          14-05-18         19-07-18  \n",
      "5  19776.0  23707.0  11791.0          14-12-18         22-04-18  \n",
      "6    312.0  20610.0  11831.0          06-02-18         14-02-19  \n",
      "7   3991.0  36728.0   5640.0          03-04-17         21-09-19  \n",
      "8   7757.0  12569.0  16120.0          22-05-17         24-10-19   \n",
      "\n",
      "    cust_id  birth_date  age  acct_amount acct_cur  inv_amount   fund_A  \\\n",
      "0  870A9281  1962-06-09   54    63523.310   dollar    35500.50  30105.0   \n",
      "1  166B05B0  1962-12-16   36    38175.460   dollar    81921.86   4995.0   \n",
      "2  BFC13E88  1990-09-12   49    59863.770   dollar    46412.27  10323.0   \n",
      "3  F2158F66  1985-11-03   56    92545.310   dollar    76563.35   3908.0   \n",
      "4  7A73F334  1990-05-17   21   132563.200   dollar         NaN  12158.4   \n",
      "5  472341F2  1980-02-23   47    83127.650   dollar    93552.69  12686.0   \n",
      "6  6B094617  1977-08-26   53    89855.980   dollar    70357.70   1796.0   \n",
      "7  80C0DAB3  1963-10-14   29    81346.595   dollar    14429.59  15290.0   \n",
      "8  E52D4C7F  1975-06-05   58    61795.890   dollar    51297.32  12939.0   \n",
      "\n",
      "    fund_B   fund_C   fund_D account_opened last_transaction  \n",
      "0   4138.0   1420.0  15632.0     2018-02-09       2019-02-22  \n",
      "1    938.0   6696.0   2421.0     2019-02-28       2018-10-31  \n",
      "2   4590.0   8469.0   1185.0     2018-01-26       2018-02-04  \n",
      "3    492.0   6482.0  12830.0            NaT       2018-08-11  \n",
      "4  51281.0  13434.0  18383.0     2018-05-14       2018-07-19  \n",
      "5  19776.0  23707.0  11791.0     2018-12-14       2018-04-22  \n",
      "6    312.0  20610.0  11831.0     2018-06-02       2019-02-14  \n",
      "7   3991.0  36728.0   5640.0     2017-03-04       2019-09-21  \n",
      "8   7757.0  12569.0  16120.0     2017-05-22       2019-10-24  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "banking = pd.read_csv('banking.csv', index_col=0)\n",
    "print(banking.head(9), '\\n')\n",
    "\n",
    "\n",
    "#********************************************************************************************************************\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'], \n",
    "                                           infer_datetime_format=True, \n",
    "                                           errors='coerce')\n",
    "\n",
    "banking['last_transaction'] = pd.to_datetime(banking['last_transaction'], \n",
    "                                           infer_datetime_format=True, \n",
    "                                           errors='coerce')\n",
    "#********************************************************************************************************************\n",
    "\n",
    "\n",
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1   #++++++++++++++++++++++++++++++++++\n",
    "#********************************************************************************************************************\n",
    "#********************************************************************************************************************\n",
    "#banking['acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1  #***********************************************\n",
    "\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'  #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#********************************************************************************************************************\n",
    "\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'\n",
    "\n",
    "\n",
    "print(banking.head(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bfb03-7582-4579-b2fa-1d73c70445ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 \n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de27f63-c182-4936-bdee-898542b25921",
   "metadata": {},
   "source": [
    "## Uniform dates\n",
    "\n",
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The account_opened column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a datetime object, while making sure that the format is inferred and potentially incorrect formats are set to missing. The banking DataFrame is in your environment and pandas was imported as pd.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the header of \"account_opened\" from the \"banking\" DataFrame and take a look at the different results.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Take a look at the output. You tried converting the values to datetime  using  the  default \"to_datetime()\" function without changing any argument, however received the following error: ValueError: month must be in 1..12. Why do you think that is?\n",
    "    Possible Answers\n",
    "- The to_datetime() function needs to be explicitly told which date format each row is in.[X]\n",
    "- The to_datetime() function can only be applied on YY-mm-dd date formats.[X]\n",
    "# - The 21-14-17 entry is erroneous and leads to an error.[Correct]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Convert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return a missing value.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Extract the year from the amended \"account_opened\" column and assign it to the \"acct_year\" column.\n",
    "- Print the newly created \"acct_year\" column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf0a26-1290-4e3d-ac7b-205450c6d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banking['account_opened'].unique())\n",
    "\n",
    "print(banking)\n",
    "\n",
    "\n",
    "\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'], \n",
    "                                           infer_datetime_format=True, \n",
    "                                           errors='coerce')\n",
    "\n",
    "\n",
    "banking['acct_year'] = banking['account_opened'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77add242-2b79-46ff-aa5e-27e9a989f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
      "12  EEBD980F  1990-11-20   34     57838.49       50812  18314.0   1477.0   \n",
      "41  2EC1B555  1974-03-17   46     55976.78       51477   8303.0  24112.0   \n",
      "78  F7FC8F78  1974-05-07   46     88049.82       84430   4590.0  24786.0   \n",
      "74  904A19DD  1987-02-18   33     31981.36       13188   9599.0    858.0   \n",
      "33  B5D367B5  1981-02-12   39     44226.86       36571   1280.0   8191.0   \n",
      "64  5321D380  1982-04-30   38     59700.08        8143    117.0   1198.0   \n",
      "60  5AEA5AB8  1972-10-24   48    100266.99       89341     41.0  13870.0   \n",
      "2   BFC13E88  1990-09-12   34     59863.77       24567  10323.0   4590.0   \n",
      "35  078C654F  1993-10-17   27     87312.64       66529   3684.0  17635.0   \n",
      "\n",
      "      fund_C   fund_D account_opened last_transaction  \n",
      "12  29049.48   5539.0       08-12-18         04-01-20  \n",
      "41  15776.00   3286.0       05-12-17         21-10-19  \n",
      "78   3346.00  51708.0       28-02-18         30-04-18  \n",
      "74   1083.00   1648.0       28-01-19         23-06-19  \n",
      "33   3462.00  23638.0       16-09-17         03-04-19  \n",
      "64    409.00   6419.0       09-10-18         04-02-19  \n",
      "60  60112.00  15318.0       09-06-18         03-07-19  \n",
      "2    8469.00   1185.0       25-04-18         02-04-18  \n",
      "35  11717.00  33493.0       14-04-17         05-08-18  \n",
      "    cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
      "0  870A9281  1962-06-09   58     63523.31       51295  30105.0   4138.0   \n",
      "1  166B05B0  1962-12-16   58     38175.46       15050   4995.0    938.0   \n",
      "2  BFC13E88  1990-09-12   34     59863.77       24567  10323.0   4590.0   \n",
      "3  F2158F66  1985-11-03   35     84132.10       23712   3908.0    492.0   \n",
      "4  7A73F334  1990-05-17   30    120512.00       93230  12158.4  51281.0   \n",
      "\n",
      "    fund_C   fund_D account_opened last_transaction  year  \n",
      "0   1420.0  15632.0     2018-02-09         22-02-19  2018  \n",
      "1   6696.0   2421.0     2019-02-28         31-10-18  2019  \n",
      "2   8469.0   1185.0     2018-04-25         02-04-18  2018  \n",
      "3   6482.0  12830.0     2017-07-11         08-11-18  2017  \n",
      "4  13434.0  18383.0     2018-05-14         19-07-18  2018  \n",
      "datetime64[ns]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "banking = pd.read_csv('banking_dirty.csv', index_col=0)\n",
    "print(banking.sample(9))\n",
    "\n",
    "\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'], \n",
    "                                           infer_datetime_format=True, \n",
    "                                           errors='coerce')\n",
    "\n",
    "banking['year'] = banking['account_opened'].dt.year  #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "print(banking.head())\n",
    "\n",
    "print(banking['account_opened'].dtypes)\n",
    "print(banking['year'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca095c-134e-4bce-99af-809fd645e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthday['Birthday'] = pd.to_datetime(birthday['Birthday'], \n",
    "                                      # Attempt to infer format of each date\n",
    "                                      infer_datetime_format=True,   #++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "                                      # Return NA fpr rows where conversion failed\n",
    "                                      errors='coerce')   #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898df08-2d3b-410f-a746-2cfbd1075eb0",
   "metadata": {},
   "source": [
    "## Cross field validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Here is the second lesson of this chapter.  In this lesson, we'll talk about cross field validation for diagnosing dirty data.  Let take a look at the following dataset.  \n",
    "\n",
    "It contains flight statistics on the total number of passengers in economy, business and first class as well as the total passengers for each flight.  \n",
    "# *******************************************************************************************************************\n",
    "We know that these columns have been collected and merged from different data sources, and a common challenge when merging data from different sources is data integrity, or more broadly making sure that our data is correct.  This is where \"cross field validation\" comes in.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\"Cross Field Validation\" is the use of multiple fields in your dataset to sanity check the integrity of your data.  For example in our flights dataset, this could be summing economy, business and first class values and making sure they are equal to the total passengers on the plane.  This could be easily done in Pandas, by first subsetting on the columns to sum, then use the \".sum()\" method with the axis argument set to 1 to indicate row wise summing.  We then find the instances where the total passengers column is equal to the sum of the classes.  And find and filter out instances of inconsistent passenger amounts by subsetting on the equality we created with brackets and the tilde symbol.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "Here is another example containing user IDs, birthdays and age values for a set of users.  We can for example make sure that the age and birthday columns are correct by subtracting the number of years between today's date and each birthday.  We can do this by first making sure the Birthday column is converted to \"datetime64[ns]\" with the Pandas \"pd.to_datetime()\" function.  We then create an object to storing today's date using the datetime package's \"date.today()\" function.  We then calculate the difference in years between today's date's year and the year of each birthday by using the \".dt.year\" attribute of the users Birthdat column (for datetime object we have .year attribute).  We then find instances where the calculated ages are equal to the actual age column in the users DF.  We then find and filter out the instances where we have inconsistences using subsetting with brackets and tilde symbol on the equality we created.  \n",
    "\n",
    "\n",
    "# So what should be the course of action in case we spot inconsistencies with \"Cross Field Validation\"?  \n",
    "Just like other data cleaning problems, there is no one size fits all solution, as often the best solution requires an in depth understanding of our datasets.  \n",
    "\n",
    "We can decide either drop inconsistent data, set is to missing and impute it, or apply some rules due to domain knowledge.  All these routes and assumptions can be decided upon only when you have a good understanding of where your dataset comes from different sources feeding into it.  \n",
    "\n",
    "\n",
    "Now that you know \"Cross Field Validation\", lets get to practice.  \n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "   flight_number | economy_class | business_class | first_class | total_passengers\n",
    "           DL140 |           100 |             60 |          40 |              200\n",
    "           BA248 |           130 |            100 |          70 |              300\n",
    "          MEA124 |           100 |             50 |          50 |              200\n",
    "          AFR939 |           140 |             70 |          90 |              300\n",
    "           DL140 |           130 |            100 |          20 |              250\n",
    "\n",
    "sum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis=1)\n",
    "passenger_equ = sum_classes == flights['total_passengers']\n",
    "\n",
    "# Find and filter out rows with inconsistent passenger totals\n",
    "inconsistent_pass = flights[~passenger_equ]\n",
    "consistsnce_pass = flights[passenger_equ]\n",
    "\n",
    "\n",
    "import datetime\n",
    "# Convert to datetime and get today's date\n",
    "users['Birthday'] = pd.to_datetime(users['Birthday'])  # If we assume all observation in one format, no concatinate\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "# For each row in the Birthday column, calculate year difference\n",
    "age_manual = today.year - user['Birthday'].dt.year\n",
    "\n",
    "# Find instances where ages match\n",
    "age_equ = age_manual == users['age']\n",
    "\n",
    "# Find and filter out rows with inconsistent age\n",
    "inconsistent_age = users[~age_equ]\n",
    "consistent_age = age[age_equ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e901b-4417-4aa3-a5a5-5b6ecf93d42b",
   "metadata": {},
   "source": [
    "## Cross field or no cross field?\n",
    "\n",
    "Throughout this course, you've been immersed in a variety of data cleaning problems from range constraints, data type constraints, uniformity and more.\n",
    "\n",
    "In this lesson, you were introduced to cross field validation as a means to sanity check your data and making sure you have strong data integrity.\n",
    "\n",
    "Now, you will map different applicable concepts and techniques to their respective categories.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map different applicable concepts and techniques to their respective categories.\n",
    "\n",
    "\n",
    "Cross Field Validation:\n",
    "    Comfirming the Age provided by users by cross checking their birthdays\n",
    "    Row wise operations such as \".sum(axis=1)\"\n",
    "    \n",
    "Not Cross Field Validation:\n",
    "    Making sure that a \"revenue\" column is a numeric column\n",
    "    Making sure a \"subscription_date\" column has no values set in the future\n",
    "    The use the the \".astype()\" method.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f5846-61ca-49fb-bb6a-2c6e0b10bce0",
   "metadata": {},
   "source": [
    "## How's our data integrity?\n",
    "\n",
    "# New data has been merged into the \"banking\" DataFrame that contains details on how investments in the \"inv_amount\" column are allocated across four different funds A, B, C and D.\n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the \"age\" and \"birth_date\" columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of inv_amount and age against the amount invested in different funds and customers' birthdays. Both pandas and datetime have been imported as pd and dt respectively.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#        Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.\n",
    "        Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.\n",
    "\n",
    "    Question 2\n",
    "\n",
    "        Store today's date into today, and manually calculate customers' ages and store them in ages_manual.\n",
    "        Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5628da8c-6fac-47f3-9316-290b59e27721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
      "98  93E78DA3  1969-12-14   51     41942.23       29662   1758.0  11174.0   \n",
      "25  296A9395  1984-05-02   36     34679.60       28459   4676.0   7349.0   \n",
      "66  807465A4  1983-07-29   37     28827.59       14584   2485.0   1260.0   \n",
      "59  3B240FEF  1968-03-30   52     97856.46       64838  13521.0  13303.0   \n",
      "2   BFC13E88  1990-09-12   34     59863.77       24567  10323.0   4590.0   \n",
      "23  A1815565  1968-09-27   56     82996.04       30897  16092.0   5491.0   \n",
      "4   7A73F334  1990-05-17   30    120512.00       93230  12158.4  51281.0   \n",
      "39  014E0511  1972-03-13   48     70272.97       65968  23849.0  23025.0   \n",
      "80  FBAD3C91  1970-08-27   50     99141.90       13466    260.0  10152.0   \n",
      "\n",
      "     fund_C   fund_D account_opened last_transaction  \n",
      "98  11650.0   5080.0       09-10-17         15-04-18  \n",
      "25  10075.0   6359.0       24-12-17         19-02-19  \n",
      "66   1464.0   9375.0       20-04-17         31-07-18  \n",
      "59   8336.0  29678.0       23-05-18         11-10-18  \n",
      "2    8469.0   1185.0       25-04-18         02-04-18  \n",
      "23   5098.0   4216.0       07-11-17         30-09-19  \n",
      "4   13434.0  18383.0       14-05-18         19-07-18  \n",
      "39   2802.0  16292.0       09-02-19         22-05-19  \n",
      "80   2936.0    118.0       29-05-18         02-11-19  \n",
      "     cust_id  birth_date  Age  acct_amount  inv_amount    fund_A    fund_B  \\\n",
      "4   7A73F334  1990-05-17   30    120512.00       93230  12158.40  51281.00   \n",
      "12  EEBD980F  1990-11-20   34     57838.49       50812  18314.00   1477.00   \n",
      "22  96525DA6  1992-11-23   28     82511.24       33927   8206.00  15019.00   \n",
      "43  38B8CD9C  1970-06-25   50     28834.71       27531    314.00   6072.28   \n",
      "47  68C55974  1962-07-08   58     95038.14       66796  33764.00   5042.00   \n",
      "65  0A9BA907  1966-09-21   54     90469.53       70171  28615.00  21720.05   \n",
      "89  C580AE41  1968-06-01   52     96673.37       68466   8489.36  28592.00   \n",
      "92  A07D5C92  1990-09-20   30     99577.36       60407   6467.00  20861.00   \n",
      "\n",
      "      fund_C    fund_D account_opened last_transaction  \n",
      "4   13434.00  18383.00       14-05-18         19-07-18  \n",
      "12  29049.48   5539.00       08-12-18         04-01-20  \n",
      "22   5559.60   6182.00       23-07-18         07-08-18  \n",
      "43  14163.00   7908.00       17-09-18         05-02-20  \n",
      "47  10659.00  19237.41       03-04-18         25-09-18  \n",
      "65  11906.00  10763.00       15-06-18         28-08-18  \n",
      "89   2439.00  30419.00       28-09-18         17-09-18  \n",
      "92   9861.00  26004.16       17-11-17         16-01-20  \n"
     ]
    }
   ],
   "source": [
    "banking = pd.read_csv('banking_dirty.csv', index_col=0)\n",
    "print(banking.sample(9))\n",
    "\n",
    "\n",
    "inv_equ = banking['inv_amount'] == banking[['fund_A', 'fund_B', 'fund_C', 'fund_D']].sum(axis=1)  #++++++++++++++++++\n",
    "\n",
    "#print(inv_equ)\n",
    "\n",
    "inconsistence_inv = banking[~inv_equ]  #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "consistence_inv = banking[inv_equ]\n",
    "\n",
    "print(inconsistence_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9b3ad-1738-4f84-b990-87d2629c6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[____].____(____) == ____\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = ____[____]\n",
    "inconsistent_inv = ____[____]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b3377-7306-4dfb-bc5d-6f5d3247fea2",
   "metadata": {},
   "source": [
    "## Completeness\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome to the last lesson of this chapter.  \n",
    "# In this lesson, we're going to discuss the completeness and missing data.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Missing data is on eof the most common and most important data clearning problems.  Essentially, missing data is when no data value is stored for a variable in an observation.  \n",
    "\n",
    "Missing data is most commonly represented as NA or NaN, but can take on arbitrary values like 0 or dot.  Like a lot of the problems that we've seen thus far in the course, \n",
    "# its commonly due to technical or human errors.  \n",
    "Missing data can take on many forms, so lets take a look at an eaxmple.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take a look at the airquality dataset.  It contains temperature and CO2 measurements for different dates.  We can see that the CO2 value in one of rows is represented as \"NaN\".  We can find rows with missing values by using the \".isna()\" method, which returns True for missing values and False for complete values across all our rows and columns.  We can also chain the \".isna()\" method with the \".sum()\" method, which returns a breakdown of missing values per column in our dataframe.  We noticed that teh CO2 column is the only colun with misisng values.  \n",
    "\n",
    "# \"missingno\" package\n",
    "Lets find out why and dig further nature of this missingness by first visualizing our missing values.  The \"missingno\" package allows us to create useful visualization or our missing data.   Digging into its details is not part of the course, but you can also check out other courses on missing data in DataCamp's course library.  \n",
    "\n",
    "# \"msno.matrix()\" fumnction\n",
    "We visualize the missingness of the \"airquality\" DataFrame with the \"msno.matrix()\" fumnction, and show it with plt.show() function from Matplotlib, which returns the following image(horizontal white lines in a black painted box, repreents each NaN observation of each column black-box).  This matrix essentially shows how missing values are distributed across a column.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "This matrix essentially shows how missing values are distributed across a column.  We can see that missing CO2 values are randomly scattered throughout the coumn, but is that really the case?  Lets dig deeper.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "We first isolate the rows of airquality with misisng CO2 values in one DF, and complete CO2 values in another.  Then, lets use the \".describe()\" method on each of the created DFs.  \n",
    "\n",
    "We see that for all missing values of CO2, they occur at really low temperatures, with the mean temperature at minus 39 degrees and minimum and maximum of -49 and -30 respectively.  Lets confirm this visually with the \"missingno\" package.  We first sort the DF by the temperature column.  Then we input the sorted DF to the matrix function from msno.  This leaves us the matrix with all empty bar on the top of CO2 black-box.  Notice how all missing values are on the top?  Like a white bar on top of a black box.  This is because the values are sorted from smallest to largest by default.  The figure essentially confirms that CO2 measurements are lost for really low temperatures.  So it must be a sensor failure.  \n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "# That leads us to missingness types.  \n",
    "Without going too much into the details, there are a variety of types of missing data.  It could be missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR).  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Missing Completely At Random is when there is missing data completely due to randomness, and there is no relationship between missing data and remaining values, such as data entry errors.  \n",
    "Missing At Random data is when there is a relationship between missing data and other observed values, such as our CO2 data being missing for low temperature.  \n",
    "Missing Bot At Random meaning there is systematic relationship between the missing data and unobserved values.  For example, when its really hot outside, the thermometer might stop working, so we don't have temperature measurements for days with high temperatures.  However, we have no way to tell this just from looking at the data since we can't actually see what the missing temperatures are (???????).  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "There is a variety of ways of dealing with missing data, from dropping missing data, to imputing them with statistical measures such as mean, median or mode, or imputing them with more complicated algorithmic approaches or ones that require some machine learning.  Each missingness type requires a specific approach, and each type of approach has drawbacks and positives, so make sure to dig deeper in DataCamp's course library on dealing with missing data.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this lesson, we'll just explore the simple approaches to dealing with missing data.  So lets grab another look at the header of airquality.  We can drop missing values, by using the \".dropna()\" method, alongside the \"subset=\" arg equal to the column with missing values to drop.  We can also replace the missing values of CO2 with the mean value of CO2, by using the \".fillna()\" method, which is in this case 1.73.  The \".fillna()\" takes in a dictionary with columns as keys, and the imputed value as values.  We can even feed custom values into fillna pertaining to our missing data if we have enough domain knowledge about our dataset.  \n",
    "\n",
    "\n",
    "\n",
    "Now that you know how to tackle missing data, lets get started.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "airquality.sample(5)\n",
    "\n",
    "------------------------------------------------\n",
    "      Date       | Temperature  | CO2\n",
    "987   20/04/2004 | 16.8         | 0.0\n",
    "2119  07/06/2004 | 18.7         | 0.8\n",
    "2451  20/06/2004 | -40.0        | NaN\n",
    "1984  01/06/2004 | 19.6         | 1.8\n",
    "8299  19/02/2005 | 11.2         | 1.2\n",
    "\n",
    "\n",
    "import missingno as msno\n",
    "# Visualize the missingness \n",
    "msno.matrix(airquality)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Isolate missing and complete values aside\n",
    "missing = airquality[airquality['CO2'].isna()]   #-------------------------------------------------------------------\n",
    "complete = airquality[~airquality['CO2'].isna()]\n",
    "\n",
    "\n",
    "# Describe complete DF\n",
    "complete.describe()\n",
    "\n",
    "---------------------------------------------\n",
    "          Temperature         | CO2\n",
    "count     8991.000000         |  8991.000000\n",
    "mean        18.317829         |     1.739584\n",
    "std          8.832116         |     1.537580\n",
    "min         -1.900000         |     0.000000\n",
    "...         ...               |     ...\n",
    "max         44.600000         |    11.900000\n",
    "\n",
    "\n",
    "# Describe missing DF\n",
    "missing.describe()\n",
    "\n",
    "---------------------------------------------\n",
    "          Temperature         | CO2\n",
    "count     366.000000          | 0.0\n",
    "mean      -39.655738          | NaN\n",
    "std         5.988716          | NaN\n",
    "min       -49.000000          | NaN\n",
    "...       ...                 | ...\n",
    "max       -30.000000          | NaN\n",
    "\n",
    "\n",
    "\n",
    "sorted_airquality = airquality.sort_values(by='Temperature')  #------------------------------------------------------\n",
    "msno.matrix(aorted_airquality)  #------------------------------------------------------------------------------------\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drop missing values\n",
    "airquality_dropped = airquality.dropna(subset=['CO2'])  #------------------------------------------------------------\n",
    "airquality_dropped.head()\n",
    "\n",
    "\n",
    "\n",
    "co2_mean = airquality['CO2'].mean()\n",
    "airquality_imputed = airquality.fillna({'CO2': co2_mean})  #---------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f182d-c4b1-4d68-b49c-0bae6f75baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "airquality = pd.read_csv('airquality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e86710-3099-471c-9234-a61159aa8724",
   "metadata": {},
   "source": [
    "## Is this missing at random?\n",
    "\n",
    "You've seen in the video exercise how there are a variety of missingness types when observing missing data. As a reminder, missingness types can be described as the following:\n",
    "\n",
    "    Missing Completely at Random: No systematic relationship between a column's missing values and other or own values.\n",
    "    Missing at Random: There is a systematic relationship between a column's missing values and other observed values.\n",
    "    Missing not at Random: There is a systematic relationship between a column's missing values and unobserved values.\n",
    "\n",
    "You have a DataFrame containing customer satisfaction scores for a service. What type of missingness is the following?\n",
    "\n",
    "                              A customer satisfaction_score column with missing values for highly dissatisfied customers.\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    Missing completely at random.\n",
    "    press\n",
    "    1\n",
    "    Missing at random.    # Cause it might be related to other features, like terrible food, and customer HATE it\n",
    "    press\n",
    "    2\n",
    "#    Missing not at random.\n",
    "    press\n",
    "    3\n",
    "    \n",
    "Hint\n",
    "\n",
    "    This is definitely not a random occurrence of missing values, however is this related to other observed values or unobserved values?\n",
    "\n",
    "Incorrect\n",
    "\n",
    "    Are you sure about that one? This would be more applicable if missing satisfaction_score values were due to values in another column of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f24e9f-6148-4b70-8abf-2257eecbf223",
   "metadata": {},
   "source": [
    "## Missing investors\n",
    "\n",
    "Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.\n",
    "\n",
    "You just received a new version of the banking DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing inv_amount values.\n",
    "\n",
    "You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. The pandas, \"missingno\" and matplotlib.pyplot packages have been imported as pd, msno and plt respectively. The banking DataFrame is in your environment.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "#    Print the number of missing values by column in the banking DataFrame.\n",
    "#    Plot and show the missingness matrix of banking with the msno.matrix() function.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Isolate the values of banking missing values of \"inv_amount\" into \"missing_investors\"\n",
    "#    and with non-missing \"inv_amount\" values into \"investors\".   - 25 XP\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Now that you've isolated banking into investors and missing_investors, use the .describe() method on both of\n",
    "    these DataFrames in the console to understand whether there are structural differences between them.\n",
    "    What do you think is going on?\n",
    "    Possible Answers\n",
    "    The data is missing completely at random and there are no drivers behind the missingness.\n",
    "    The inv_amount is missing only for young customers, since the average age in missing_investors is 22 and the maximum age is 25.\n",
    "    The inv_amount is missing only for old customers, since the average age in missing_investors is 42 and the maximum age is 59.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    4 Sort the banking DataFrame by the age column and plot the missingness matrix of banking_sorted.  - 25 XP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5a1a7ea-fb05-4f0a-b4e2-3bae2be84b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cust_id  age  acct_amount  inv_amount account_opened last_transaction\n",
      "0  8C35540A   54     44244.71    35500.50       03-05-18         30-09-19\n",
      "1  D5536652   36     86506.85    81921.86       21-01-18         14-01-19\n",
      "2  A631984D   49     77799.33    46412.27       26-01-18         06-10-19\n",
      "3  93F2F951   56     93875.24    76563.35       21-08-17         10-07-19\n",
      "4  DE0A0882   21     99998.35         NaN       05-06-17         15-01-19 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97 entries, 0 to 96\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   cust_id           97 non-null     object \n",
      " 1   age               97 non-null     int64  \n",
      " 2   acct_amount       97 non-null     float64\n",
      " 3   inv_amount        84 non-null     float64\n",
      " 4   account_opened    97 non-null     object \n",
      " 5   last_transaction  97 non-null     object \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 5.3+ KB\n",
      "None \n",
      "\n",
      "     cust_id  age  acct_amount  inv_amount account_opened last_transaction\n",
      "35  904A19DD   20     31981.36         NaN     2019-01-28       2019-06-23\n",
      "17  D3287768   20     89961.77         NaN     2018-03-09       2018-10-19\n",
      "59  56D310A8   21     88660.40         NaN     2018-02-25       2018-07-29\n",
      "54  6B094617   21     89855.98         NaN     2018-06-02       2019-02-14\n",
      "18  FA01676F   21     66947.30         NaN     2018-10-08       2019-07-23\n",
      "4   DE0A0882   21     99998.35         NaN     2017-05-06       2019-01-15\n",
      "82  078C654F   21     87312.64         NaN     2017-04-14       2018-05-08\n",
      "14  E7389E60   22     86028.48         NaN     2017-04-06       2018-07-08\n",
      "40  93E78DA3   22     41942.23         NaN     2017-09-10       2018-04-15\n",
      "cust_id              0\n",
      "age                  0\n",
      "acct_amount          0\n",
      "inv_amount          13\n",
      "account_opened       0\n",
      "last_transaction     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cust_id  age  acct_amount  inv_amount account_opened last_transaction\n",
      "35  904A19DD   20     31981.36         NaN     2019-01-28       2019-06-23\n",
      "17  D3287768   20     89961.77         NaN     2018-03-09       2018-10-19\n",
      "59  56D310A8   21     88660.40         NaN     2018-02-25       2018-07-29\n",
      "54  6B094617   21     89855.98         NaN     2018-06-02       2019-02-14\n",
      "18  FA01676F   21     66947.30         NaN     2018-10-08       2019-07-23\n",
      "4   DE0A0882   21     99998.35         NaN     2017-05-06       2019-01-15\n",
      "82  078C654F   21     87312.64         NaN     2017-04-14       2018-05-08\n",
      "14  E7389E60   22     86028.48         NaN     2017-04-06       2018-07-08\n",
      "40  93E78DA3   22     41942.23         NaN     2017-09-10       2018-04-15\n",
      "88  33CA2B76   23     75508.61         NaN     2017-11-16       2019-03-03\n",
      "93  4C7F8638   23     21942.37         NaN     2018-07-14       2019-02-02\n",
      "94  A81D31B3   24     74010.15         NaN     2018-02-06       2018-12-09\n",
      "46  3B240FEF   25     97856.46         NaN     2018-05-23       2018-11-10\n",
      "69  13770971   26     92750.87    27963.45     2017-08-16       2019-04-24\n",
      "28  F7FC8F78   26     88049.82    84432.03     2018-02-28       2018-04-30 \n",
      "\n",
      "     cust_id  age  acct_amount  inv_amount account_opened last_transaction\n",
      "77  D4C7E817   56     26585.87    20441.92     2018-06-05       2018-08-12\n",
      "57  0F0884F6   56     84505.81    47826.51     2018-08-03       2019-08-24\n",
      "50  46351200   56     97595.30    82251.59     2018-08-18       2020-02-20\n",
      "61  5AEA5AB8   56    100266.99    89342.43     2018-09-06       2019-03-07\n",
      "64  87FDF627   56     95275.46    55888.87     2019-02-26       2018-06-25\n",
      "56  B25B3B8D   57     99193.98    83364.21     2018-04-28       2019-04-07\n",
      "34  AC2AEAC4   57     88440.54    63332.90     2018-12-03       2019-01-08\n",
      "67  5F6A2443   57     98047.16    76216.88     2018-12-15       2019-12-11\n",
      "96  E961CA44   57     27907.16    10967.69     2017-10-23       2019-11-07\n",
      "66  BFC13E88   58     59863.77    24569.47     2018-04-25       2018-02-04\n",
      "32  5321D380   58     59700.08     8145.24     2018-09-10       2019-04-02\n",
      "8   870A9281   58     63523.31    51297.32     2018-02-09       2019-02-22\n",
      "89  EEBD980F   59     57838.49    50814.83     2018-08-12       2020-04-01\n",
      "11  2AB6539A   59     53796.13    12401.32     2019-03-01       2018-11-17\n",
      "84  8D08495A   59     89138.52    60798.23     2018-08-08       2019-05-02 \n",
      "\n",
      "             age   acct_amount  inv_amount\n",
      "count  13.000000     13.000000         0.0\n",
      "mean   21.846154  73231.238462         NaN\n",
      "std     1.519109  25553.327176         NaN\n",
      "min    20.000000  21942.370000         NaN\n",
      "25%    21.000000  66947.300000         NaN\n",
      "50%    21.000000  86028.480000         NaN\n",
      "75%    23.000000  89855.980000         NaN\n",
      "max    25.000000  99998.350000         NaN\n",
      "             age    acct_amount    inv_amount\n",
      "count  84.000000      84.000000     84.000000\n",
      "mean   43.559524   75095.273214  44717.885476\n",
      "std    10.411244   32414.506022  26031.246094\n",
      "min    26.000000   12209.840000   3216.720000\n",
      "25%    34.000000   57373.062500  22736.037500\n",
      "50%    45.000000   83061.845000  44498.460000\n",
      "75%    53.000000   94165.965000  66176.802500\n",
      "max    59.000000  250046.760000  93552.690000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "banking = pd.read_csv('banking_missing.csv', index_col = \"Unnamed: 0\")\n",
    "\n",
    "print(banking.head(), '\\n')\n",
    "print(banking.info(), '\\n')  #***************************************************************************************\n",
    "\n",
    "\n",
    "# Convert to DataTime  #---------------------------------------------------------------------------------------------\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'])\n",
    "banking['last_transaction'] = pd.to_datetime(banking['last_transaction'])\n",
    "\n",
    "\n",
    "#banking.sort_values(by='account_opened')    # So we cant find relationship using \".describe()\" with NaN and non-NaN\n",
    "#banking.sort_values(by='last_transaction')  # And other features seems got no relationship with NaNs, Think\n",
    "print(banking.sort_values(by='age').head(9))\n",
    "\n",
    "\n",
    "# Print the number of missing values by column in the banking DataFrame\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Plot and show the missingness matrix of banking with the msno.matrix() function   #++++++++++++++++++++++++++++++++\n",
    "msno.matrix(banking.sort_values(by='age'))   #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(banking.sort_values('age').head(15), '\\n')  #******************************************************************\n",
    "print(banking.sort_values('age').tail(15), '\\n')  #******************************************************************\n",
    "# So young kids knwos nothing about investment, just keep they money in a saving account  #**************************\n",
    "\n",
    "\n",
    "no_invest = banking[banking['inv_amount'].isna()]\n",
    "yes_invest = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "\n",
    "print(no_invest.describe())\n",
    "print(yes_invest.describe(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119e39-5965-407b-8278-b4d4ccb017de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values(by = 'age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9565f58-ceb0-4b6d-9642-277d76df48bb",
   "metadata": {},
   "source": [
    "## Follow the money\n",
    "\n",
    "In this exercise, you're working with another version of the banking DataFrame that contains missing values for both the \"cust_id\" column and the \"acct_amount\" column.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You know that rows with missing \"cust_id\" don't really help you, and that on average \"acct_amount\" is usually 5 times the amount of \"inv_amount\".\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "In this exercise, you will drop rows of banking with missing cust_ids, and impute missing values of acct_amount with some domain knowledge.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Use \".dropna()\" to drop missing values of the \"cust_id\" column in banking and store the results in \"banking_fullid\".\n",
    "#    Use \"inv_amount\" to compute the estimated account amounts for \"banking_fullid\" by setting the amounts equal to \"inv_amount\" * 5, and assign the results to acct_imp.\n",
    "#    Impute the missing values of \"acct_amount\" in \"banking_fullid\" with the newly created \"acct_imp\" using \".fillna()\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "417c8bd6-1e7c-4e97-851d-f20f5a6cd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_id              0\n",
      "age                  0\n",
      "acct_amount          0\n",
      "inv_amount          13\n",
      "account_opened       0\n",
      "last_transaction     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(banking.isna().sum())\n",
    "\n",
    "\n",
    "banking_fullid = banking.dropna(subset=['cust_id'])\n",
    "\n",
    "\n",
    "acct_imp = banking_fullid['inv_amount']*5\n",
    "\n",
    "\n",
    "banking_imputed = banking.fillna({'acct_amount', acct_imp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c1f7f-14a8-400f-859e-0a1d7c00062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.____(subset = ['____'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = ____\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.____({'____':____})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0d171-e6b6-4329-bf8e-59770f03d0b6",
   "metadata": {},
   "source": [
    "## Comparing strings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Awesome work on Chapter 3.  Welcome to the final Chapter of this course, where we'll discover the world of record linkage.  But before we get deep dive into record linkage, lets sharpen our understanding of string similarity and minimum edit distance.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "Minimum Edit Distance is a systematic way to identify how close 2 strings are.  For example, lets take a look at the following two words: \"intention\", and \"execution\".  The Minimum Edit Distance between them is the least possible amount of steps, that could get us from the world \"intention\" to \"execution\", with the available operations being.  Inserting new characters, deleting them, substituting them, and transposing consecutive characters.  \n",
    "\n",
    "To get from \"intention\" to \"execution\", We first start off by deleting \"I\" from \"intension\" and adding \"C\" between \"E\" and \"N\".  Our minimum edit distance so far is 2, since these are two operations.  We then substitute the first \"N\" with \"E\", \"T\" with \"X\", and \"N\" with \"U\", leading us to \"EXECUTION\".  With the minimum edit distance being 5.  \n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "The lower the edit distance, the closer two words are.  For example, the two different typos of reading having a minimum edit distance of 1 between them are reading.  There is a variety of algorithms based on edit distance that differ on which operations they use, how much weight attributed to each operation, which type of string they're suited for and more, with a variety of packages to get each similarity.  \n",
    "\n",
    "For this lesson, we'll be comparing strings using the \"Levenshtein distance\", since its the most general form of string matched by using the \"fuzzywuzzy\" package.  The \"fuzzywuzzy\" is a simple to use package to perform string comparison.  We first import \"fuzz\" from \"fuzzywuzzy\" package, which allow us to compare between single strings.  Below we use the \"fuzz.WRatio()\" function to compare the similarity between reading and it typo, inputting each string as an argument.  For any comparison function using \"fuzzywuzzy\", our output is a score from 0 to 100 with 0 being not similar at all, 100 being en extract match.  \n",
    "\n",
    "# Do not confuse this with the minimum eddit distance score earlier, where a lower minimum edit distance means a closer match.  \n",
    "\n",
    "\n",
    "The \"fuzz.WRatio()\" function is hightly robust against partial string comparison with different orderings.  For example here we compare the string \"Houston Rockets\" and \"Rockets\", and still receive a high similarity score.  The same can be said for the strings \"Houston Rockets vs Los Angeles Lakers\" and \"Lakers vs Rockets\", where the team names are only partial and they are differently ordered.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# \"process.extract()\" function from the \"process\" module from \"fuzzywuzzy\" package\n",
    "We can also compare a string with an array of strings by using the \"process.extract()\" function from the \"process\" module from \"fuzzywuzzy\" package.  The \"process.extract()\" takes in a string, an array of strings, and the number of possible matches to return ranked from highest to lowest.  It returns a list of tuples with 3 elements, the first one being the matching string being returned, the second one being its similarity score, and the third one being its index in the array.  (here I used the NumPy array, instead of Pandas Series)\n",
    "\n",
    "\n",
    "In chapter 2, we learned that collapsing data into categories is an essential aspect of working with categorical and text data, and we saw how to manually replace categories in a column of a DF.  But what is we had so many inconsistent categories that a manual replacement is simply not feasible?  We can easily do that with string similarity.  \n",
    "\n",
    "    Use \".replace()\" to collapse \"eur\" into \"Europe\"\n",
    "\n",
    "    What if there are too many variables?\n",
    "    \"EU\", \"eur\", \"Europ\", \"Europa\", \"Erope\", \"Evropa\", ...\n",
    "    \n",
    "# *******************************************************************************************************************\n",
    "Say we have a DF named survey containing answers from respondents from the state of New York and California asking them how likely are you to move on a scale of 0 to 5.  The \"state\" field was free text and contains hundreds of typos.  Remapping them manually would take a huge amount of time.  Instead, we'll use string similarity.  We also have a category DF containing the correct categories for each state.  Lets collapse the incorrect categories with string matching.  (Just cant figure out how to put above mentioned toolkits together and achieve our needs)\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "We first create a for loop, iterate over each correctly typed state in the categories DF.  For each state, we find its matches in the state column of the survery DF, returning all possible matches by setting the \"limit=\" argument of extract to the length of the survery DF.  Then we iterate over each potential match, isolating the ones with the similarity score higher or equal to 80 with an if statement.  Then for each of those returned strings, we replace it with the correct state using the loc[] method.  \n",
    "\n",
    "\n",
    "Record linkage attempts to join sources that have similarly fuzzy duplicate values, so that we end up with a final DF with no duplicates by using string similarity.  \n",
    "\n",
    "We'll cover record linkage in more detailn the next couple of lessons.  \n",
    "\n",
    "\n",
    "\n",
    "\"intention\"    I N T E * N T I O N\n",
    "               . | |   . |\n",
    "\"execution\"    * E X E C U T I O N\n",
    "\n",
    "\n",
    "\"intention\"    I N T E N * T I O N\n",
    "               . | |   | .\n",
    "\"execution\"    * E X E C U T I O N\n",
    "\n",
    "\n",
    "\"reading\":  R E E D I N G\n",
    "                |\n",
    "\"reading\":  R E A D I N G\n",
    "\n",
    "\n",
    "\"reading\":  R E D-A I N G\n",
    "                 |\n",
    "\"reading\":  R E A D I N G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c73bd84-821f-42cf-a953-1c8d5663676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "90\n",
      "86\n",
      "[('Houston Rockets VS Los Angeles', 95), ('Rockets vs Lakers', 86), ('Lakers vs Rockets', 86), ('Houston vs Los Angeles', 86)]\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "# Compare reeding vs reading\n",
    "print(fuzz.WRatio('reeding', 'reading'))\n",
    "\n",
    "\n",
    "print(fuzz.WRatio(\"Houston Rockets\", \"Rockets\"))  #------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print(fuzz.WRatio(\"Houston Rockets vs Los Angeles Lakers\", \"Lakers vs Rockets\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "string = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "choices = np.array([\"Rockets vs Lakers\", \"Lakers vs Rockets\", \"Houston vs Los Angeles\", \"Heat vs Bulls\", \n",
    "                    \"Houston VS Los Angeles\", \"Rockets VS Lakers\", \"Houston Rockets VS Los Angeles\"])\n",
    "\n",
    "print(process.extract(string, choices, limit=4))  #******************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c6c0c4de-ef37-4e8c-ac52-331ecf20d20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           state  score\n",
      "0     California      1\n",
      "1           Cali      2\n",
      "2     Calefornia      5\n",
      "3     Calefornie      3\n",
      "4     Californie      2\n",
      "5      Calfornia      5\n",
      "6     Calefernia      5\n",
      "7       New York      3\n",
      "8  New York City      4\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   state   9 non-null      object\n",
      " 1   score   9 non-null      int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 272.0+ bytes\n",
      "None \n",
      "\n",
      "['California' 'Cali' 'Calefornia' 'Calefornie' 'Californie' 'Calfornia'\n",
      " 'Calefernia' 'New York' 'New York City']\n",
      "[('California', 100, 0), ('Calfornia', 95, 5), ('Cali', 90, 1), ('Calefornia', 90, 2)] \n",
      "\n",
      "['California' 'New York'] \n",
      "\n",
      "        state  score\n",
      "0  California      1\n",
      "1  California      2\n",
      "2  California      5\n",
      "3  California      3\n",
      "4  California      2\n",
      "5  California      5\n",
      "6  California      5\n",
      "7    New York      3\n",
      "8    New York      4\n"
     ]
    }
   ],
   "source": [
    "survery = pd.read_csv('survery.csv')\n",
    "print(survery)\n",
    "\n",
    "print(survery.info(), '\\n')\n",
    "print(survery['state'].unique())\n",
    "\n",
    "\n",
    "categories = [\"California\", \"New York\"]\n",
    "\n",
    "\n",
    "print(process.extract(categories[0], survery['state'], limit=4), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "for i in categories:                                  ######################-----------------------------------------\n",
    "    test_score = process.extract(i, survery['state'], limit=survery.shape[0])\n",
    "    for j in test_score:\n",
    "        if j[1] > 40:                              # Keep Thinking\n",
    "            survery['state'] = survery['state'].replace(j[0], i)  #**************************************************\n",
    "        \n",
    "print(survery['state'].unique(), '\\n')\n",
    "\n",
    "\n",
    "print(survery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2abc35c-15f2-4b53-8d30-95a365f3d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           state  score\n",
      "0     California      1\n",
      "1           Cali      2\n",
      "2     Calefornia      5\n",
      "3     Calefornie      3\n",
      "4     Californie      2\n",
      "5      Calfornia      5\n",
      "6     Calefernia      5\n",
      "7       New York      3\n",
      "8  New York City      4 \n",
      "\n",
      "['California' 'Cali' 'Calefornia' 'Calefornie' 'Californie' 'Calfornia'\n",
      " 'Calefernia' 'New York' 'New York City']\n",
      "[('California', 100, 0), ('Calfornia', 95, 5), ('Cali', 90, 1), ('Calefornia', 90, 2), ('Californie', 90, 4), ('Calefornie', 80, 3), ('Calefernia', 80, 6), ('New York City', 33, 8), ('New York', 22, 7)] \n",
      "\n",
      "[('New York', 100, 7), ('New York City', 90, 8), ('Calefornia', 33, 2), ('Calefornie', 33, 3), ('Calfornia', 24, 5), ('California', 22, 0), ('Californie', 22, 4), ('Calefernia', 11, 6), ('Cali', 0, 1)] \n",
      "\n",
      "[('California', 100, 0), ('Calfornia', 95, 5), ('Cali', 90, 1), ('Calefornia', 90, 2), ('Californie', 90, 4), ('Calefornie', 80, 3), ('Calefernia', 80, 6), ('New York City', 33, 8), ('New York', 22, 7)]\n",
      "[('New York', 100, 7), ('New York City', 90, 8), ('California', 22, 0), ('California', 22, 1), ('California', 22, 2), ('California', 22, 3), ('California', 22, 4), ('California', 22, 5), ('California', 22, 6)]\n",
      "\n",
      "\n",
      "['California' 'New York'] \n",
      "\n",
      "        state  score\n",
      "0  California      1\n",
      "1  California      2\n",
      "2  California      5\n",
      "3  California      3\n",
      "4  California      2\n",
      "5  California      5\n",
      "6  California      5\n",
      "7    New York      3\n",
      "8    New York      4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "\n",
    "survery = pd.read_csv('survery.csv')\n",
    "\n",
    "print(survery, '\\n')\n",
    "print(survery['state'].unique())\n",
    "\n",
    "\n",
    "categories = [\"California\", \"New York\"]\n",
    "\n",
    "\n",
    "print(process.extract(categories[0], survery['state'], limit=survery.shape[0]), '\\n')\n",
    "print(process.extract(categories[1], survery['state'], limit=survery.shape[0]), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "for i in categories:\n",
    "    test_score = process.extract(i, survery['state'], limit=survery.shape[0])\n",
    "    print(test_score)\n",
    "    for j in test_score:\n",
    "        if j[1] > 40:\n",
    "            survery.loc[survery['state']==j[0], 'state'] = i  #******************************************************\n",
    "            #survery['state'] = survery['state'].replace(j[0], i)  #*************************************************\n",
    "\n",
    "print('\\n')\n",
    "print(survery['state'].unique(), '\\n')\n",
    "\n",
    "print(survery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7307e64-a635-433c-8504-fd9f86bc92ac",
   "metadata": {},
   "source": [
    "## Minimum edit distance\n",
    "\n",
    "In the video exercise, you saw how minimum edit distance is used to identify how similar two strings are. As a reminder, minimum edit distance is the minimum number of steps needed to reach from String A to String B, with the operations available being:\n",
    "\n",
    "    Insertion of a new character.\n",
    "    Deletion of an existing character.\n",
    "    Substitution of an existing character.\n",
    "    Transposition of two existing consecutive characters.\n",
    "\n",
    "\n",
    "                    What is the minimum edit distance from 'sign' to 'sing', and which operation(s) gets you there?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    2 by substituting 'g' with 'n' and 'n' with 'g'.\n",
    "    press\n",
    "    1\n",
    "#    1 by transposing 'g' with 'n'.\n",
    "    press\n",
    "    2\n",
    "    1 by substituting 'g' with 'n'.\n",
    "    press\n",
    "    3\n",
    "    2 by deleting 'g' and inserting a new 'g' at the end.\n",
    "    press\n",
    "    4\n",
    "    \n",
    "S I G-N\n",
    "     | \n",
    "S I N G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb40f0-05b5-447a-ae2a-607c1197e012",
   "metadata": {},
   "source": [
    "## The cutoff point\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with the restaurants DataFrame which has data on various restaurants. \n",
    "# Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "This version of restaurants has been collected from many sources, where the \"cuisine_type\" column is riddled with typos, and should contain only \"italian\", \"american\" and \"asian\" cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "Before doing so, you want to establish the cutoff point for the similarity score using the fuzzywuzzy's \"process.extract()\" function by finding the similarity score of the most distant typo of each category.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Import process from fuzzywuzzy.\n",
    "    Store the unique \"cuisine_types\" into \"unique_types\" variable.\n",
    "    Calculate the similarity of 'asian', 'american', and 'italian' to all possible cuisine_types using process.extract(), while returning all possible matches.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Take a look at the output, what do you think should be the similarity cutoff point when remapping categories?\n",
    "\n",
    "■ \\blacksquare ■ 80\n",
    "\n",
    "# □ \\square □ 70\n",
    "\n",
    "□ \\square □ 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d50508-2483-41fd-95aa-7c51dcddb852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name                       addr         city  \\\n",
      "0  arnie morton's of chicago   435 s. la cienega blv .   los angeles   \n",
      "1         art's delicatessen       12224 ventura blvd.   studio city   \n",
      "2                  campanile       624 s. la brea ave.   los angeles   \n",
      "3                      fenix    8358 sunset blvd. west     hollywood   \n",
      "4         grill on the alley           9560 dayton way   los angeles   \n",
      "\n",
      "        phone cuisine_type  \n",
      "0  3102461501      america  \n",
      "1  8187621221      merican  \n",
      "2  2139381447     amurican  \n",
      "3  2138486677     americen  \n",
      "4  3102760615    americann  \n",
      "['america' 'merican' 'amurican' 'americen' 'americann' 'asiane' 'itali'\n",
      " 'asiann' 'murican' 'italien' 'italian' 'asiat' 'american' 'americano'\n",
      " 'italiann' 'ameerican' 'asianne' 'italiano' 'americin' 'ammericann'\n",
      " 'amerycan' 'aamerican' 'ameriican' 'italiaan' 'asiian' 'asiaan'\n",
      " 'amerrican' 'ameerrican' 'ammereican' 'asian' 'italianne' 'italiian']\n",
      "[('italian', 100), ('asian', 67), ('american', 40)]\n",
      "[('american', 100), ('asian', 62), ('italian', 53)]\n",
      "[('asian', 100), ('italian', 67), ('american', 62)]\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz, process\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('restaurants.csv', index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "unique_types = df['cuisine_type'].unique()\n",
    "print(unique_types)\n",
    "\n",
    "\n",
    "\n",
    "categories = ['italian', 'american', 'asian']\n",
    "\n",
    "for i in categories:\n",
    "    print(process.extract(i, categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf82d1-7be0-4ec1-bd7a-e4c022d767c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "____\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = ____\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.____('____', ____, limit = len(____)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(____('____', ____, ____))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ec044b3e-f052-4054-b6d0-437c97e2d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('italian', 67), ('american', 62), ('mexican', 50), ('cajun', 40), ('southwestern', 36), ('southern', 31), ('coffeebar', 26), ('steakhouses', 25)]\n",
      "[('american', 100), ('mexican', 80), ('cajun', 68), ('asian', 62), ('italian', 53), ('southwestern', 41), ('southern', 38), ('coffeebar', 24), ('steakhouses', 21)]\n",
      "[('italian', 100), ('asian', 67), ('mexican', 43), ('american', 40), ('cajun', 33), ('southern', 27), ('southwestern', 26), ('steakhouses', 26), ('coffeebar', 12)]\n"
     ]
    }
   ],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = df['type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf8c0b-b1a6-4ce1-a832-b608d844e545",
   "metadata": {},
   "source": [
    "## Remapping categories II\n",
    "\n",
    "In the last exercise, you determined that the distance cutoff point for remapping typos of 'american', 'asian', and 'italian' cuisine types stored in the cuisine_type column should be 80.  __????????????? Why not 70__\n",
    "\n",
    "In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by using fuzywuzzy.process's extract() function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a string with an array of strings using \"process.extract()\", the output is a list of tuples where each is formatted like:\n",
    "\n",
    "(closest match, similarity score, index of match)\n",
    "\n",
    "The restaurants DataFrame is in your environment, and you have access to a categories list containing the correct cuisine types ('italian', 'asian', and 'american').\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Return all of the unique values in the \"cuisine_type\" column of restaurants.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Okay! Looks like you will need to use some string matching to correct these misspellings!     # check it before\n",
    "#    As a first step, create a list of matches, comparing 'italian' with the restaurant types listed in the \"cuisine_type\" column.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Now you're getting somewhere! Now you can iterate through matches to reassign similar entries.\n",
    "#    Within the for loop, use an if statement to check whether the similarity score in each match is greater than or equal to 80.\n",
    "#    If it is, use .loc to select rows where cuisine_type in restaurants is equal to the current match (which is the first element of match), and reassign them to be 'italian'.\n",
    "# *******************************************************************************************************************\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Finally, you'll adapt your code to work with every restaurant type in categories.\n",
    "#    Using the variable cuisine to iterate through categories, embed your code from the previous step in an outer for loop.\n",
    "    Inspect the final result. This has been done for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "409d33e7-9a26-45a5-928a-4f9137977e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  name  \\\n",
      "77                         le colonial   \n",
      "164                            halcyon   \n",
      "259  la grotta at ravinia dunwoody rd.   \n",
      "307            il fornaio levi's plaza   \n",
      "81                     nate   n   al's   \n",
      "83                        ocean avenue   \n",
      "58                             il nido   \n",
      "268                 south city kitchen   \n",
      "\n",
      "                                               addr           city  \\\n",
      "77                              8783 beverly blvd.     los angeles   \n",
      "164       151 w. 54th st. in the rihga royal hotel        new york   \n",
      "259   holiday inn/crowne plaza at ravinia dunwoody         atlanta   \n",
      "307                               1265 battery st.   san francisco   \n",
      "81                              414 n. beverly dr.     los angeles   \n",
      "83                                 1401 ocean ave.    santa monica   \n",
      "58                                 251 e. 53rd st.        new york   \n",
      "268                             1144 crescent ave.         atlanta   \n",
      "\n",
      "          phone      type  \n",
      "77   3102890660     asian  \n",
      "164  2124688888  american  \n",
      "259  7703959925   italian  \n",
      "307  4159860100   italian  \n",
      "81   3102740101  american  \n",
      "83   3103945669  american  \n",
      "58   2127538450   italian  \n",
      "268  4048737358  southern   \n",
      "\n",
      "(336, 5) \n",
      "\n",
      "['american' 'asian' 'italian' 'coffeebar' 'mexican' 'southwestern'\n",
      " 'steakhouses' 'southern' 'cajun'] \n",
      "\n",
      "american        137\n",
      "italian          78\n",
      "asian            72\n",
      "coffeebar        25\n",
      "mexican           9\n",
      "southern          8\n",
      "steakhouses       5\n",
      "southwestern      1\n",
      "cajun             1\n",
      "Name: type, dtype: int64 \n",
      "\n",
      "['american' 'asian' 'italian' 'coffeebar' 'mexican' 'southwestern'\n",
      " 'steakhouses' 'southern' 'cajun'] \n",
      "\n",
      "american        137\n",
      "italian          78\n",
      "asian            72\n",
      "coffeebar        25\n",
      "mexican           9\n",
      "southern          8\n",
      "steakhouses       5\n",
      "southwestern      1\n",
      "cajun             1\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('restaurants_L2.csv', index_col=0)\n",
    "\n",
    "print(df.sample(8), '\\n')\n",
    "print(df.shape, '\\n')\n",
    "print(df['type'].unique(), '\\n')\n",
    "\n",
    "print(df['type'].value_counts(), '\\n')\n",
    "\n",
    "\n",
    "categories = ['italian', 'american', 'asian']\n",
    "\n",
    "for i in categories:\n",
    "    scores = process.extract(i, df['type'], limit=df.shape[0])   # returns a list of tuples\n",
    "    for j in scores:\n",
    "        if j[1] > 80:\n",
    "            df.loc[df['type']==j[0], 'type'] = i\n",
    "\n",
    "            #survery.loc[survery['state']==j[0], 'state'] = i  #*****************************************************\n",
    "            #survery['state'] = survery['state'].replace(j[0], i)  #*************************************************\n",
    "\n",
    "\n",
    "\n",
    "print(df['type'].unique(), '\\n')\n",
    "\n",
    "print(df['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a63b1f8-e409-41b6-965d-6f12bb32f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           name  \\\n",
      "5              restaurant katsu   \n",
      "75   il fornaio cucina italiana   \n",
      "26                   mesa grill   \n",
      "159                 gallagher's   \n",
      "293               buca giovanni   \n",
      "149                   firehouse   \n",
      "86                       paty's   \n",
      "93                     swingers   \n",
      "\n",
      "                                                addr           city  \\\n",
      "5                            1972 n. hillhurst ave.     los angeles   \n",
      "75                               301 n. beverly dr.     los angeles   \n",
      "26         102 5th ave. between 15th and 16th sts .        new york   \n",
      "159                                 228 w. 52nd st.        new york   \n",
      "293                               800 greenwich st.   san francisco   \n",
      "149   522 columbus ave. between 85th and 86th sts .        new york   \n",
      "86                              10001 riverside dr.     toluca lake   \n",
      "93                               8020 beverly blvd.     los angeles   \n",
      "\n",
      "          phone cuisine_type  \n",
      "5    2136651891       asiane  \n",
      "75   3105508330     american  \n",
      "26   2128077400    ameerican  \n",
      "159  2122455336     american  \n",
      "293  4157767766      italian  \n",
      "149  2125953139     american  \n",
      "86   8187619126      italian  \n",
      "93   2136535858        asian   \n",
      "\n",
      "(336, 5) \n",
      "\n",
      "['america' 'merican' 'amurican' 'americen' 'americann' 'asiane' 'itali'\n",
      " 'asiann' 'murican' 'italien' 'italian' 'asiat' 'american' 'americano'\n",
      " 'italiann' 'ameerican' 'asianne' 'italiano' 'americin' 'ammericann'\n",
      " 'amerycan' 'aamerican' 'ameriican' 'italiaan' 'asiian' 'asiaan'\n",
      " 'amerrican' 'ameerrican' 'ammereican' 'asian' 'italianne' 'italiian'] \n",
      "\n",
      "american      110\n",
      "italian        79\n",
      "asian          73\n",
      "americann       9\n",
      "asianne         4\n",
      "ameerican       4\n",
      "asiat           3\n",
      "italiano        3\n",
      "amerycan        3\n",
      "italiann        3\n",
      "americano       3\n",
      "italien         3\n",
      "ammericann      3\n",
      "murican         3\n",
      "asiann          3\n",
      "itali           3\n",
      "aamerican       3\n",
      "italiaan        3\n",
      "americin        3\n",
      "america         2\n",
      "merican         2\n",
      "asiane          2\n",
      "americen        2\n",
      "amurican        2\n",
      "ameriican       1\n",
      "asiian          1\n",
      "asiaan          1\n",
      "amerrican       1\n",
      "ameerrican      1\n",
      "ammereican      1\n",
      "italianne       1\n",
      "italiian        1\n",
      "Name: cuisine_type, dtype: int64 \n",
      "\n",
      "['american' 'asian' 'italian'] \n",
      "\n",
      "american    153\n",
      "italian      96\n",
      "asian        87\n",
      "Name: cuisine_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "\n",
    "restaurant = pd.read_csv('restaurants.csv', index_col=0)\n",
    "\n",
    "print(restaurant.sample(8), '\\n')\n",
    "print(restaurant.shape, '\\n')\n",
    "\n",
    "\n",
    "print(restaurant['cuisine_type'].unique(), '\\n')\n",
    "print(restaurant['cuisine_type'].value_counts(), '\\n')\n",
    "\n",
    "\n",
    "categories = ['italian', 'american', 'asian']\n",
    "\n",
    "for i in categories:\n",
    "    scores = process.extract(i, restaurant['cuisine_type'], limit=restaurant.shape[0])   # returns a list of tuples\n",
    "    for j in scores:\n",
    "        if j[1] > 70:\n",
    "            restaurant.loc[restaurant['cuisine_type']==j[0], 'cuisine_type'] = i\n",
    "\n",
    "            #survery.loc[survery['state']==j[0], 'state'] = i  #*****************************************************\n",
    "            #survery['state'] = survery['state'].replace(j[0], i)  #*************************************************\n",
    "\n",
    "\n",
    "\n",
    "print(restaurant['cuisine_type'].unique(), '\\n')\n",
    "\n",
    "print(restaurant['cuisine_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051bff-4379-4701-98d0-0f3dbf613040",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['italian', 'asian', 'american']\n",
    "\n",
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "    \n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b1796-617f-45ab-9017-14bb24e3e2cb",
   "metadata": {},
   "source": [
    "## Generating pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Great work with lesson 1 - you now have solid understanding on how to calculate string similarity.  \n",
    "\n",
    "At the end of the last video exercise, we saw how record linkage attempts to join data sources with fuzzy duplicate values.  For example, here are two DFs containing NBA games and their schedules.  They've both been scraped from different sites and we would want to merge them together and have one DataFrame containing all unique games.  We see that there are duplicated values in both DFs with different naming and non duplicate values.  \n",
    "\n",
    "Since there are games happening at the same time, no common unique identifer between the DFs, and the events are differently named, a regular join or merge will not work.  This is where record linkage comes in.  \n",
    "\n",
    "\n",
    "# Record Linkage is the act of linking data from different sources regarding the same entity.  \n",
    "Generally, we clean two or more DFs, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them.  All of these steps can be achieved with the \"recordlinkage\" package, lets find how.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Here we have two DFs, census_A and census_B, containing data on individuala throughout the states.  We want to merger them while avoiding duplicaton using record linkage, since they are collected manually and are prone to typos, there are no consistent IDs between them.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "We first want to generate pairs between both DFs.  Ideally, we want to generate all possible pairs between our DFs, but what if we had big DFs and ended up having to generate millions if not billions of pairs?  It wouldn't prove scalable and could seriously hamper development time.  This is where we apply what we call blocking, which creates pairs based on a matching column, which is in this case, the state column, reducing the number of possible pairs.  \n",
    "\n",
    "\n",
    "To do this, we first start off by importing \"recordlinkage\" package.  \n",
    "# We then use the \"recordlinkage.Index()\" function, to create an indexing object.  \n",
    "This essentially is an object we can use to generate pairs from our DFs.  To generate pairs blocked on \"state\", we use the \".block()\" method, inputting the \"state\" column as input.  Once the indexer object has been initialized, we generate our pairs using the \".index()\" method, which takes in the two DFs.  \n",
    "\n",
    "The resulting object, is a Pandas multi index object containing pairs of row indices from both DFs, which is a fancy way to say: \n",
    "# it is an array containing possible pairs of indices that makes it much easier to subset DFs on.  \n",
    "\n",
    "\n",
    "Since we already generated our pairs, its time to find potential matches.  \n",
    "# We first start by creating a comparison object using the \"recordlinkage.Compare()\" function.  \n",
    "This is similar to the indexing object we created while generating pairs, but this one is responsible for assigning different comparison procedures for pairs.  \n",
    "\n",
    "# Lets say there are columns for which we want exact matches between the pairs.  \n",
    "To do that, we use the \".exact()\" method.  It takes in the column name in question for each DF, which is in this case the \"date_of_birth\" and \"state\", and a \"lable=\" argument which let us set the column name in the resulting DF.  \n",
    "\n",
    "Now in order to compare string similarities between pairs of rows for dolumns that have fuzzy values, we use the \".string()\" method, which also takes in the column names in question, the similarity cutoff point in the \"threshold=\" argument, which takes in the value between 0 and 1.  \n",
    "\n",
    "# And finally to compute the matches, we use the \".compute()\" function, which takes in the possible pairs, and the two DFs in question.  N\n",
    "ote that you need to always have the same order of DFs when inserting them as arguments when generating pairs, comparing between columns, and computing comparisons.  The output is a multi index DF, where the first index is the row index from the first DF, or census_a, and the second index is a list of all row indices in census_b.  The columns are the columns being compared, with values being 1 for a match, and 0 for not a match.  \n",
    "\n",
    "# To find potential matches, we just filter for rows where the sum of row values is higher than a certain threshold. \n",
    "Which is in this case higher or equal to 2.  But we'll dig deeper into these matches and see how to use them to link our census DFs in the next lesson.  \n",
    "\n",
    "\n",
    "For now, lets generate pairs.  \n",
    "\n",
    "\n",
    "--------------------------------------------------------\n",
    "Event                              | Time\n",
    "Houston Rockets vs Chicago Bulls   | 19:00      &\n",
    "Miami Heat vs Los Angeles Lakers   | 19:00      $\n",
    "Brooklyn Nets vs Orlando Magic     | 20:00      %\n",
    "Denver Nuggets vs Miami Heat       | 21:00\n",
    "San Antonio Spurs vs Atlanta Hawks | 21:00\n",
    "\n",
    "------------------------------------------\n",
    "Event                   | Time\n",
    "NBA: Nets vs Magic      | 8pm     %\n",
    "NBA: Bulls vs Rockets   | 9pm     &  should we take this as same? cause the order was switched, and time is different\n",
    "NBA: Heat vs Lakers     | 7pm     $\n",
    "NBA: Grizzlies vs Heat  | 10pm\n",
    "NBA: Heat vs Cavaliers  | 9pm\n",
    "\n",
    "\n",
    "\n",
    "Data A  \\\n",
    "          Generate pairs --> Compare pairs --> Score pairs --> Link data\n",
    "Data B  /\n",
    "\n",
    "\n",
    "\n",
    "census_A\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "              | given_name  | surname   | date_of_birth  | suburb         | state   | address_1\n",
    "rec_id        |             |           |                |                |         |  \n",
    "rec-1070-org  | michaela    | neumann   | 19151111       | winston hills  | cal     | stanley street\n",
    "rec-1016-org  | courtney    | painter   | 19161214       | richlands      | txs     | pinkerton circuit\n",
    "...\n",
    "\n",
    "\n",
    "census_B\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "               | given_name  | surname   | date_of_birth  | suburb         | state   | address_1\n",
    "rec_id         |             |           |                |                |         |  \n",
    "rec-561-dup-0  | elton       | NaN       | 19651013       | windermere     | ny      | light street\n",
    "rec-2642-dup-0 | mitchell    | maxon     | 19390212       | north ryde     | cal     | edkins circuit\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Import recordlinkage package for later use\n",
    "import recordlinkage\n",
    "\n",
    "\n",
    "# Create indexing object\n",
    "indexer = recordlinkage.Index()  #***********************************************************************************\n",
    "\n",
    "# Generate pairs blocked on state\n",
    "indexer.block('state')                      #************************************************************************\n",
    "pairs = indexer.index(census_A, cesus_B)    #************************************************************************\n",
    "\n",
    "print(pairs)\n",
    "    MultiIndex(levels=[['rec-1007-org', 'rec-1016-org', 'rec-1054-org', 'rec-1066-org', ... ,57 , 37, 70, 94]],                      names=['rec_id_1', 'rec_id_2'])\n",
    "\n",
    "\n",
    "# Create a Compare object\n",
    "compare_col = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches for pairs of \"date_of_birth\" and \"state\"     ***************************************************\n",
    "compare_col.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "compare_col.exact('state', 'state', label='state')\n",
    "\n",
    "\n",
    "# Find similar matches for pairs of surname and address_1 using string similarity   *********************************\n",
    "compare_col.string('surname', 'surname', threshold=0.85, label='surname')\n",
    "compare_col.string('address_1', 'sddress_1', threshold=0.85, label='address_1')\n",
    "\n",
    "\n",
    "# Find matches\n",
    "potential_matches = compare_col.compute(pairs, census_a, census_b)  #************************************************\n",
    "\n",
    "\n",
    "print(potential_matches)\n",
    "\n",
    "---------------------------------------------------------------------------------------\n",
    "              |                | date_of_birth | state | surname   | address_a\n",
    "rec_id_1      | rec_id_2       |               |       |           |\n",
    "rec-1070-org  | rec-561-dup-0  | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-2642-dup-0 | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-608-dup-0  | 0             | 1     | 0.0       | 0.0\n",
    "              ...\n",
    "rec-1631-org  | rec-4070-dup-0 | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-4862-dup-0 | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-629-dup-0  | 0             | 1     | 0.0       | 0.0\n",
    "\n",
    "\n",
    "\n",
    "potential_matches[potential_matches.sum(axis=1) >= 2]   #************************************************************\n",
    "\n",
    "---------------------------------------------------------------------------------------\n",
    "              |                | date_of_birth | state | surname   | address_a\n",
    "rec_id_1      | rec_id_2       |               |       |           |\n",
    "rec-4878-org  | rec-4878-dup-0 | 1             | 1     | 1.0       | 0.0\n",
    "rec-417-org   | rec-2867-dup-0 | 0             | 1     | 0.0       | 1.0\n",
    "rec-3964-org  | rec-397-dup-0  | 0             | 1     | 1.0       | 0.0\n",
    "rec-1373-org  | rec-4051-dup-0 | 0             | 1     | 1.0       | 0.0\n",
    "              | rec-802-dup-0  | 0             | 1     | 1.0       | 0.0\n",
    "rec-3540-org  | rec-470-dup-0  | 0             | 1     | 1.0       | 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c9613-411c-4957-9ce1-ed8785cd38e9",
   "metadata": {},
   "source": [
    "## To link or not to link?\n",
    "\n",
    "Similar to joins, record linkage is the act of linking data from different sources regarding the same entity. But unlike joins, record linkage does not require exact matches between different pairs of data, and instead can find close matches using string similarity. This is why record linkage is effective when there are no common unique keys between the data sources you can rely upon when linking data sources such as a unique identifier.\n",
    "\n",
    "In this exercise, you will classify each card whether it is a traditional join problem, or a record linkage one.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Classify each card into a problem that requires record linkage or regular joins.\n",
    "\n",
    "\n",
    "__Record linkage:__ \n",
    "Merging two basketball DFs, with columns team_A, team_B, and time and different formatted team names between each DF\n",
    "Two customer DFs containing names and address, one with a unique identifier per customer, one without\n",
    "Use an address column to join two DFs, with the address in each DF being formatted slightly differently\n",
    "    \n",
    "\n",
    "__Regular joins:__\n",
    "Two basketball DFs with a common unique identifier per game\n",
    "Consolidating two DFs containing detalis on DataCamp courses, with each DataCamp course having its own unique identifier.  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d54a6b-4278-4b40-8323-9662eac0a121",
   "metadata": {},
   "source": [
    "## Pairs of restaurants\n",
    "\n",
    "In the last lesson, you cleaned the restaurants dataset to make it ready for building a restaurants recommendation engine. You have a new DataFrame named \"restaurants_new\" with new restaurants to train your model on, that's been scraped from a new data source.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "You've already cleaned the \"cuisine_type\" and city columns using the techniques learned throughout the course. However you saw duplicates with typos in restaurants names that require record linkage instead of joins with restaurants.\n",
    "\n",
    "In this exercise, you will perform the first step in record linkage and generate possible pairs of rows between restaurants and restaurants_new. Both DataFrames, pandas and recordlinkage are in your environment.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#    Instantiate an indexing object by using the Index() function from recordlinkage.\n",
    "#    Block your pairing on \"cuisine_type\" by using indexer's' \".block()\" method.\n",
    "#    Generate pairs by indexing restaurants and \"restaurants_new\" in that order.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Generate pairs by indexing \"restaurants\" and \"restaurants_new\" in that order.\n",
    "#    Now that you've generated your pairs, you've achieved the first step of record linkage. What are the steps remaining to link both restaurants DataFrames, and in what order?\n",
    "\n",
    "Hint\n",
    "\n",
    "    To generate pairs between restaurants and restaurants_new, use indexer's' .index() method.\n",
    "    This the same example from the lesson's video, just different DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0db9e3f-5fed-41fb-bd6c-63a64fb644c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name                       addr         city  \\\n",
      "0  arnie morton's of chicago   435 s. la cienega blv .   los angeles   \n",
      "1         art's delicatessen       12224 ventura blvd.   studio city   \n",
      "2                  campanile       624 s. la brea ave.   los angeles   \n",
      "3                      fenix    8358 sunset blvd. west     hollywood   \n",
      "4         grill on the alley           9560 dayton way   los angeles   \n",
      "\n",
      "        phone cuisine_type  \n",
      "0  3102461501     american  \n",
      "1  8187621221     american  \n",
      "2  2139381447     american  \n",
      "3  2138486677     american  \n",
      "4  3102760615     american   \n",
      "\n",
      "       name                       addr          city       phone  \\\n",
      "0    kokomo          6333 w. third st.            la  2139330773   \n",
      "1    feenix    8358 sunset blvd. west      hollywood  2138486677   \n",
      "2   parkway       510 s. arroyo pkwy .      pasadena  8187951001   \n",
      "3      r-23           923 e. third st.   los angeles  2136877178   \n",
      "4     gumbo          6333 w. third st.            la  2139330358   \n",
      "\n",
      "    cuisine_type  \n",
      "0       american  \n",
      "1       american  \n",
      "2    californian  \n",
      "3       japanese  \n",
      "4   cajun/creole   \n",
      "\n",
      "['american' 'asian' 'italian']\n",
      "['american' ' american' ' californian' ' japanese' ' cajun/creole'\n",
      " ' hot dogs' ' diners' ' delis' ' hamburgers' ' seafood' ' italian'\n",
      " ' coffee shops' ' russian' ' steakhouses' ' mexican/tex-mex'\n",
      " ' noodle shops' ' mexican' ' middle eastern' ' asian' ' vietnamese'\n",
      " ' health food' ' american ( new )' ' pacific new wave' ' indonesian'\n",
      " ' eclectic' ' chicken' ' fast food' ' southern/soul' ' coffeebar'\n",
      " ' continental' ' french ( new )' ' desserts' ' chinese' ' pizza'] \n",
      "\n",
      " american            19\n",
      " italian              7\n",
      " coffee shops         5\n",
      " californian          5\n",
      " asian                4\n",
      " hot dogs             3\n",
      " diners               3\n",
      " delis                3\n",
      " hamburgers           3\n",
      " mexican              3\n",
      " noodle shops         2\n",
      " pacific new wave     2\n",
      " seafood              2\n",
      " french ( new )       1\n",
      " continental          1\n",
      " coffeebar            1\n",
      " eclectic             1\n",
      " desserts             1\n",
      " chinese              1\n",
      " southern/soul        1\n",
      " fast food            1\n",
      " chicken              1\n",
      "american              1\n",
      " middle eastern       1\n",
      " indonesian           1\n",
      " american ( new )     1\n",
      " health food          1\n",
      " vietnamese           1\n",
      " mexican/tex-mex      1\n",
      " steakhouses          1\n",
      " russian              1\n",
      " cajun/creole         1\n",
      " japanese             1\n",
      " pizza                1\n",
      "Name: cuisine_type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Create a Compare object\\ncompare_col = recordlinkage.Compare()  #*****************************************************************************\\n\\n\\n# Find exact matches for pairs of \"date_of_birth\" and \"state\"     ***************************************************\\ncompare_col.exact(\\'phone\\', \\'phone\\', label=\\'phone\\')  # I think phone number is enough\\ncompare_col.exact(\\'city\\', \\'city\\', label=\\'city\\')\\n\\n\\n# Find similar matches for pairs of surname and address_1 using string similarity   *********************************\\ncompare_col.string(\\'name\\', \\'name\\', threshold=0.8, label=\\'name\\')\\ncompare_col.string(\\'addr\\', \\'addr\\', threshold=0.8, label=\\'addr\\')\\n\\n\\n# Find matches\\npotential_matches = compare_col.compute(pairs, restaurant, restaurant_new)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import recordlinkage\n",
    "\n",
    "\n",
    "print(restaurant.head(), '\\n')\n",
    "\n",
    "restaurant_new = pd.read_csv('restaurants_new.csv', index_col=0)\n",
    "print(restaurant_new.head(), '\\n')\n",
    "\n",
    "\n",
    "print(restaurant['cuisine_type'].unique())\n",
    "print(restaurant_new['cuisine_type'].unique(), '\\n')\n",
    "\n",
    "print(restaurant_new['cuisine_type'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# Create indexing object\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block your pairing on \"cuisine_type\" by using indexer's' .block() method.\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs by indexing restaurants and restaurants_new in that order.\n",
    "pairs = indexer.index(restaurant, restaurant_new)\n",
    "\n",
    "'''\n",
    "# Create a Compare object\n",
    "compare_col = recordlinkage.Compare()  #*****************************************************************************\n",
    "\n",
    "\n",
    "# Find exact matches for pairs of \"date_of_birth\" and \"state\"     ***************************************************\n",
    "compare_col.exact('phone', 'phone', label='phone')  # I think phone number is enough\n",
    "compare_col.exact('city', 'city', label='city')\n",
    "\n",
    "\n",
    "# Find similar matches for pairs of surname and address_1 using string similarity   *********************************\n",
    "compare_col.string('name', 'name', threshold=0.8, label='name')\n",
    "compare_col.string('addr', 'addr', threshold=0.8, label='addr')\n",
    "\n",
    "\n",
    "# Find matches\n",
    "potential_matches = compare_col.compute(pairs, restaurant, restaurant_new)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d7963-0b1b-4576-97bf-caebae057ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845e737-4f09-4bbe-ae90-52a01e4115fa",
   "metadata": {},
   "source": [
    "## Similar restaurants\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In the last exercise, you generated pairs between restaurants and restaurants_new in an effort to cleanly merge both DataFrames using record linkage.\n",
    "\n",
    "When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact matches, string similarities, and more.\n",
    "\n",
    "Now that your pairs have been generated and stored in pairs, you will find exact matches in the city and cuisine_type columns between each pair, and similar strings for each pair in the rest_name column. Both DataFrames, pandas and recordlinkage are in your environment.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "#    Instantiate a comparison object using the \"recordlinkage.Compare()\" function.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DataFrames.\n",
    "#    Use the appropriate comp_cl method to find similar strings with a 0.8 similarity threshold in the rest_name column of both DataFrames.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Compute the comparison of the pairs by using the \".compute()\" method of comp_cl.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "# *******************************************************************************************************************\n",
    "    Print out \"potential_matches\", the columns are the columns being compared, with values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a column. You can find them with potential_matches[potential_matches.sum(axis = 1) >= n]. Where n is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of n be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3c5de3-dcc2-4c0b-9ec0-21946e3dc558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name                       addr         city  \\\n",
      "0  arnie morton's of chicago   435 s. la cienega blv .   los angeles   \n",
      "1         art's delicatessen       12224 ventura blvd.   studio city   \n",
      "2                  campanile       624 s. la brea ave.   los angeles   \n",
      "3                      fenix    8358 sunset blvd. west     hollywood   \n",
      "4         grill on the alley           9560 dayton way   los angeles   \n",
      "\n",
      "        phone cuisine_type  \n",
      "0  3102461501     american  \n",
      "1  8187621221     american  \n",
      "2  2139381447     american  \n",
      "3  2138486677     american  \n",
      "4  3102760615     american   \n",
      "\n",
      "       name                       addr          city       phone  \\\n",
      "0    kokomo          6333 w. third st.            la  2139330773   \n",
      "1    feenix    8358 sunset blvd. west      hollywood  2138486677   \n",
      "2   parkway       510 s. arroyo pkwy .      pasadena  8187951001   \n",
      "3      r-23           923 e. third st.   los angeles  2136877178   \n",
      "4     gumbo          6333 w. third st.            la  2139330358   \n",
      "\n",
      "    cuisine_type  \n",
      "0       american  \n",
      "1       american  \n",
      "2    californian  \n",
      "3       japanese  \n",
      "4   cajun/creole   \n",
      "\n",
      "MultiIndex([(  0, 0),\n",
      "            (  1, 0),\n",
      "            (  2, 0),\n",
      "            (  3, 0),\n",
      "            (  4, 0),\n",
      "            (  8, 0),\n",
      "            (  9, 0),\n",
      "            ( 13, 0),\n",
      "            ( 14, 0),\n",
      "            ( 16, 0),\n",
      "            ...\n",
      "            (315, 0),\n",
      "            (316, 0),\n",
      "            (318, 0),\n",
      "            (321, 0),\n",
      "            (322, 0),\n",
      "            (323, 0),\n",
      "            (324, 0),\n",
      "            (325, 0),\n",
      "            (327, 0),\n",
      "            (334, 0)],\n",
      "           length=153) \n",
      "\n",
      "       city  cuisine_type  name\n",
      "0   0     0             1   0.0\n",
      "192 0     0             1   0.0\n",
      "195 0     0             1   0.0\n",
      "199 0     0             1   0.0\n",
      "200 0     0             1   0.0\n",
      "...     ...           ...   ...\n",
      "96  0     0             1   0.0\n",
      "97  0     0             1   0.0\n",
      "98  0     0             1   0.0\n",
      "85  0     0             1   0.0\n",
      "334 0     0             1   0.0\n",
      "\n",
      "[153 rows x 3 columns]\n",
      "(153, 3)\n",
      "Empty DataFrame\n",
      "Columns: [city, cuisine_type, name]\n",
      "Index: [] \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [name, addr, city, phone, cuisine_type]\n",
      "Index: [] \n",
      "\n",
      "                         name                        addr                city  \\\n",
      "0   arnie morton's of chicago    435 s. la cienega blv .          los angeles   \n",
      "1          art's delicatessen        12224 ventura blvd.          studio city   \n",
      "2                   campanile        624 s. la brea ave.          los angeles   \n",
      "3                       fenix     8358 sunset blvd. west            hollywood   \n",
      "4          grill on the alley            9560 dayton way          los angeles   \n",
      "..                        ...                         ...                 ...   \n",
      "77                      feast         1949 westwood blvd.             west la   \n",
      "78                   mulberry         17040 ventura blvd.              encino   \n",
      "79                 matsuhissa    129 n. la cienega blvd.        beverly hills   \n",
      "80                    jiraffe       502 santa monica blvd        santa monica   \n",
      "81                   martha’s   22nd street grill 25 22nd   st. hermosa beach   \n",
      "\n",
      "         phone  cuisine_type  \n",
      "0   3102461501      american  \n",
      "1   8187621221      american  \n",
      "2   2139381447      american  \n",
      "3   2138486677      american  \n",
      "4   3102760615      american  \n",
      "..         ...           ...  \n",
      "77  3104750400       chinese  \n",
      "78  8189068881         pizza  \n",
      "79  3106599639         asian  \n",
      "80  3109176671   californian  \n",
      "81  3103767786      american  \n",
      "\n",
      "[418 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1987/2819289419.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  full_restaurant = restaurant.append(non_dup)\n"
     ]
    }
   ],
   "source": [
    "print(restaurant.head(), '\\n')\n",
    "print(restaurant_new.head(), '\\n')\n",
    "\n",
    "\n",
    "import recordlinkage\n",
    "\n",
    "'''\n",
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurant, restaurant_new)\n",
    "'''\n",
    "print(pairs, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Create a comparison object\n",
    "comp_col = recordlinkage.Compare()   #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "# Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DFs\n",
    "comp_col.exact('city', 'city', label='city')\n",
    "comp_col.exact('cuisine_type', 'cuisine_type', label='cuisine_type')    # ???????????????????????????????????????????\n",
    "                                                # city and type match meaning same record?  why not phone name\n",
    "\n",
    "\n",
    "# Use the appropriate comp_cl method to find similar strings with a 0.8 threshold in the rest_name column of both DF\n",
    "comp_col.string('name', 'name', threshold=0.8, label='name')\n",
    "\n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_col.compute(pairs, restaurant, restaurant_new)  #*******************************************\n",
    "\n",
    "print(potential_matches.sort_values('city'))\n",
    "\n",
    "print(potential_matches.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "   '''\n",
    "\n",
    "# Isolate potential matches with row sum > 3\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]  #***************************************************\n",
    "\n",
    "print(matches, '\\n')  #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "# Get values of second column index of matches DF\n",
    "matching_index = matches.index.get_level_values(1)  #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "\n",
    "restaurant_dup = restaurant_new[restaurant_new.index.isin(matching_index)]  #++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "print(restaurant_dup, '\\n')\n",
    "\n",
    "\n",
    "non_dup = restaurant_new[~restaurant_new.index.isin(matching_index)]\n",
    "\n",
    "full_restaurant = restaurant.append(non_dup)\n",
    "\n",
    "print(full_restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d666296-3ce5-49b4-b0db-9fc617d3f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "# 336 + 153 = 489\n",
    "\n",
    "restaurant_rows = pd.read_csv('restaurants.csv').shape[0]\n",
    "\n",
    "restaurant_new_rows = pd.read_csv('restaurants_new.csv').shape[0]\n",
    "\n",
    "print(restaurant_rows + restaurant_new_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcad4a-4fa0-4ff9-9ee2-7f9822e1a479",
   "metadata": {},
   "source": [
    "## Linking DataFrames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Awesome work on the first 2 lessons.  You've made it to the last lesson of thei course.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# At this point, you generated you pairs, compared them, and scored them.  Now its time to link your data.  \n",
    "\n",
    "\n",
    "**Remember our census DFs from the previous lessons.  We've alread generated pairs between them, compared four of their columns, two for exact matches and two for string similarity alongside 0.8 threshold, and found potential matches.  Now its time to link both census DFs.  **\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets look closely at our potential matches.  It is a multi index DF, where we have two index columns, rec_id_1, and rec_id_2.  \n",
    "# The first index column (level) stores indices from census_a.  The second index column, stores all possible indices from census_b, for each row index of census_a.  \n",
    "The column of our potential matches are the columns we choose to link both DFs on, where the value is 1 for a match, and 0 otherwise.  The first step in linking DFs, is to islate the potentially matching pairs to the ones we're pretty sure of.  We saw how to do this in the previous lesson, by subsetting the rows where the row sum s above a certain number of columns, in this case 3.  The output is row indices between census_a and census_b that are most likely duplicates.  \n",
    "\n",
    "# Our next step is to extract the one of the index columns and subsetting its associated DF to filter for duplicates. \n",
    "We can choose the second index column, which represents row indices of census_b DF.  We want to extract those indices, and subset cnesus_b on them to remove duplicates with census_a before appending them together.  We can access a DF's index using the \".index\" attribute.  Since this is a multi index DF, it returns a multi-index object containing pairs of row indices from census_a and census_b respectively.  \n",
    "\n",
    "# We want to extract all census_b indices, \n",
    "so we chain it with the \".get_level_values()\" method, which takes in which column index we want to extract its values.  We can either input the index column's name, or its order, which is in this case 1.(so its not always the level=1 or something, but maybe in this case the indces have field names)  \n",
    "\n",
    "To find the duplicates in census_b, we simply subset on all indices of census_b, with the ones found through record linkage.  You can choose to examine them further for similarity with their duplicates in canseu_a, but if youre sure of your analysis, you can go ahead and find the non duplicates by repeating exact same line of code, except by adding  tilde at the begining of your subset.  \n",
    "\n",
    "Now you have your non-duplicates, all you need is a simple \".append()\" (recall the \"pd.concat()\" function), using the DF append method of census_a, and you have your linked DataFrame.  Wala\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To recap, what we did was build on top of our previous work in generating pairs, comparing across columns and finding potential matches.  We then isolated all possible matches, where there are matches across 3 columns or more, ensuring we tightened our search for duplicates across both DFs beofre we link them.  We then extract the row indices of census_b where there are duplicates.  And found rows of sensus_b where they are not duplicated with census_a by using the tilde symbol.  And then we link both DFs for full census results.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "And now you know hoe to link DFs, lets put those skills into action\n",
    "\n",
    "\n",
    "\n",
    "# Import recordlinkage and generate pairs and compare across columns\n",
    "import recordlinkage\n",
    "\n",
    "\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "pairs = \n",
    "\n",
    "\n",
    "# Generate potential matches\n",
    "potential_matches = compare_col.compute(pairs, census_a, census_b)\n",
    "\n",
    "\n",
    "print(potential_matches)\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "              |                | date_of_birth | state | surname   | address_a\n",
    "rec_id_1      | rec_id_2       |               |       |           |\n",
    "rec-1070-org  | rec-561-dup-0  | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-2642-dup-0 | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-608-dup-0  | 0             | 1     | 0.0       | 0.0\n",
    "              ...\n",
    "rec-1631-org  | rec-4070-dup-0 | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-4862-dup-0 | 0             | 1     | 0.0       | 0.0\n",
    "              | rec-629-dup-0  | 0             | 1     | 0.0       | 0.0\n",
    "              \n",
    "              \n",
    "matches = potential_matches[potential_matches.sum(axis=1)>3]\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "              |                | date_of_birth | state | surname   | address_a\n",
    "rec_id_1      | rec_id_2       |               |       |           |\n",
    "rec-2404-org  | rec-2404-dup-0 | 1             | 1     | 1.0       | 1.0\n",
    "rec-4178-org  | rec-4178-dup-0 | 1             | 1     | 1.0       | 1.0\n",
    "rec-1054-org  | rec-1054-dup-0 | 1             | 1     | 1.0       | 1.0\n",
    "...\n",
    "rec-1234-org  | rec-1234-dup-0 | 1             | 1     | 1.0       | 1.0\n",
    "rec-1271-org  | rec-1271-dup-0 | 1             | 1     | 1.0       | 1.0\n",
    "\n",
    "\n",
    "print(matches.index)\n",
    "    MultiIndex(level=[['rec-1007-org', 'rec-1016-org', 'rec-1054-org', 'rec-1066-org', 'rec-1070-org', 'rec-1075-org', 'rec-1080-org', 'rec-110-org', ...]])\n",
    "\n",
    "# Get indices from census_b only\n",
    "duplicate_rows = matches.index.get_level_values(1)  #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    Index(['rec-2404-dup-0', 'rec-4178-dup-0', 'rec-1054-dup-0', 'rec-4663-dup-0', 'rec-485-dup-0', 'rec-2950-dup-0', 'rec-1234-dup-0', ... , 'rec-299-dup-0'])\n",
    "\n",
    "\n",
    "\n",
    "# Finding duplicates in census_b\n",
    "census_b_duplicates = cnesus_b[census_b.index.isin(duplicate_rows)]  #+++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "# Finding new rows in census_b\n",
    "census_b_new = census_b[~census_b.index.isin(duplicate_rows)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b4b72-2a4c-4fab-9725-0ec9f8417703",
   "metadata": {},
   "source": [
    "Python programing, Threading and process, Software engineering, Applo 11 program, Margaret Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded51f21-ad05-4114-a75d-c1bd8b7708bc",
   "metadata": {},
   "source": [
    "## Getting the right index\n",
    "\n",
    "# Here's a DataFrame named matches containing potential matches between two DataFrames, users_1 and users_2. Each DataFrame's row indices is stored in uid_1 and uid_2 respectively.\n",
    "\n",
    "             first_name  address_1  address_2  marriage_status  date_of_birth\n",
    "uid_1 uid_2                                                                  \n",
    "0     3              1          1          1                1              0\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "1     3              1          1          1                1              0\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "     ...            ...         ...        ...              ...            ...\n",
    "\n",
    "How do you extract all values of the uid_1 index column?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "#    matches.index.get_level_values(0)\n",
    "    press\n",
    "    1\n",
    "    matches.index.get_level_values(1)\n",
    "    press\n",
    "    2\n",
    "#    matches.index.get_level_values('uid_1')\n",
    "    press\n",
    "    3\n",
    "    Both 1 and 3 are correct.\n",
    "    press\n",
    "    4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d28c6-58bb-4628-9065-dae87cbdabbc",
   "metadata": {},
   "source": [
    "## Linking them together!\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In the last lesson, you've finished the bulk of the work on your effort to link restaurants and restaurants_new.       **You've generated the different pairs of potentially matching rows, **\n",
    "  **searched for exact matches between the \"cuisine_type\" and \"city\" columns, **\n",
    "  **but compared for similar strings in the rest_name column. **\n",
    "You stored the DataFrame containing the scores in potential_matches.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "Now it's finally time to link both DataFrames. \n",
    "# You will do so by first extracting all row indices of restaurants_new that are matching across the columns mentioned above from potential_matches. \n",
    "# Then you will subset restaurants_new on these indices, then append the non-duplicate values to restaurants. All DataFrames are in your environment, alongside pandas imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.\n",
    "    Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the .get_level_values() method.\n",
    "    Subset restaurants_new for rows that are not in matching_indices.\n",
    "    Append non_dup to restaurants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec006ff-ca65-4a03-b7a3-0e50d69f0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaeabee-e0c6-41a0-b377-b74fe8b1e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate potential matches with row sum >=3\n",
    "matches = ____[____.___(____) >= ____]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.____.____(____)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = ____[~restaurants_new.index.____(____)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.____(____)\n",
    "print(full_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba7482-15f6-44a8-8a8f-95d2a1c4503b",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Fantastic job.  You have now finished the course.  In this course, you learned how to diagnose dirty data, identify the side effectsof dirty data, and learned how to clean data.  \n",
    "\n",
    "In chapter 1, we learned about basic data cleaning problem such as fixing incorrect data types, making sure our data sticks within range, and dropping duplicates.  \n",
    "\n",
    "In chapter 2, we learned about common problems affecting categorical and text data.  \n",
    "\n",
    "In chapter 3, we learned more advanced data problems, such as unifying differently formatted data, cross field validation and completeness.  \n",
    "\n",
    "Finally in chapter 4, we saw how to link datasets where joins don't work, by learning about \"recordlinkage\"\n",
    "\n",
    "\n",
    "With that in mind, there is still so much more to learn on your way to become a data clearning expect.  So make sure to check out DataCamp's content library along the way, wheather taht means courses, tracks, or projects.  \n",
    "\n",
    "Finally, dont forget to apply what you learned in your daily data tasks.  Thank you for taking this journey in clearning data in Python, and see you next time.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013140f-8985-4d92-902a-c2383849979c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999515a-bfd3-4fd6-a31a-2d1a6982f51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860fbe6-7026-4b49-9518-d1fbd6ced516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e4ea8-0183-43cc-86f6-1c9fcc14e4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b487ff0-887d-4c9d-8a4d-c4af744db139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db13c8-52d9-43d5-8963-cd6539510604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d5121-c7ea-4f24-87f8-172bbcc108c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020aad0-e3fa-41e0-a94a-4dcffda750f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a121467-deb3-49d9-bd6e-542174ec69e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a058cae-530f-45e7-8587-bf7b9b2f3a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcc880-cab3-4d1b-b81d-5000d8c6d23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c0733-5df6-40a5-93af-1d2778e3d9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc24ab-5204-4993-b63e-3d4b57590586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055cd25-e46b-47a4-95e7-2b53140307d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7f94a-0af1-436f-8742-350c3b3046e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cada37-4ed1-4754-ac80-658422f980e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326b982-02fb-44af-b33f-1d929149d879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b1a1e-8236-4225-a5ad-e061d1a5c478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e6382-a26a-4913-9fde-3a3bf275773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac01ea-72c2-4816-b086-18db3f58a9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30226e6f-4314-4dd5-8cb1-5b629ee1bcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88e088-53c6-4ac8-a6b0-d0e683629b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51eae4-451e-407a-9189-bc0ca9e6a412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c926df5-24ba-46f6-9c88-25df7a76b16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052eaea-8c07-4458-a688-9d69ac88978d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
