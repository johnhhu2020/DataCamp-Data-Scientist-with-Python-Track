{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdb6c41-92b9-47b2-b660-7e6a4b1e24c2",
   "metadata": {},
   "source": [
    "## Cleaning Data in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055a7c9-8970-4132-829c-2b27d95c641c",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "It's commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions. Data cleaning is an essential task in data science. Without properly cleaned data, the results of any data analysis or machine learning model could be inaccurate. In this course, you will learn how to identify, diagnose, and treat a variety of data cleaning problems in Python, ranging from simple to advanced. You will deal with improper data types, check that your data is in the correct range, handle missing data, perform record linkage, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef6cb7-259a-431e-90c6-1daf990df19d",
   "metadata": {},
   "source": [
    "##  Common data problems\n",
    "Free\n",
    "0%\n",
    "\n",
    "In this chapter, you'll learn how to overcome some of the most common dirty data problems. You'll convert data types, apply range constraints to remove future data points, and remove duplicated data points to avoid double-counting.\n",
    "\n",
    "    Data type constraints    50 xp\n",
    "    Common data types    100 xp\n",
    "    Numeric data or ... ?    100 xp\n",
    "    Summing strings and concatenating numbers    100 xp\n",
    "    Data range constraints    50 xp\n",
    "    Tire size constraints    100 xp\n",
    "    Back to the future    100 xp\n",
    "    Uniqueness constraints    50 xp\n",
    "    How big is your subset?    50 xp\n",
    "    Finding duplicates   100 xp\n",
    "    Treating duplicates    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781858e-0a46-4241-98dd-6547f3c63dc5",
   "metadata": {},
   "source": [
    "##  Text and categorical data problems\n",
    "0%\n",
    "\n",
    "Categorical and text data can often be some of the messiest parts of a dataset due to their unstructured nature. In this chapter, you’ll learn how to fix whitespace and capitalization inconsistencies in category labels, collapse multiple categories into one, and reformat strings for consistency.\n",
    "\n",
    "    Membership constraints    50 xp\n",
    "    Members only    100 xp\n",
    "    Finding consistency    100 xp\n",
    "    Categorical variables    50 xp\n",
    "    Categories of errors    100 xp\n",
    "    Inconsistent categories    100 xp\n",
    "    Remapping categories    100 xp\n",
    "    Cleaning text data    50 xp\n",
    "    Removing titles and taking names    100 xp\n",
    "    Keeping it descriptive    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf42f6f-f930-4e9a-9fd7-7aebf5c266a3",
   "metadata": {},
   "source": [
    "##  Advanced data problems\n",
    "0%\n",
    "\n",
    "In this chapter, you’ll dive into more advanced data cleaning problems, such as ensuring that weights are all written in kilograms instead of pounds. You’ll also gain invaluable skills that will help you verify that values have been added correctly and that missing values don’t negatively impact your analyses.\n",
    "\n",
    "    Uniformity    50 xp\n",
    "    Ambiguous dates    50 xp\n",
    "    Uniform currencies    100 xp\n",
    "    Uniform dates    100 xp\n",
    "    Cross field validation    50 xp\n",
    "    Cross field or no cross field?    100 xp\n",
    "    How's our data integrity?    100 xp\n",
    "    Completeness    50 xp\n",
    "    Is this missing at random?    50 xp\n",
    "    Missing investors    100 xp\n",
    "    Follow the money    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a59e4-6b35-408c-8fb3-1bf98ad897de",
   "metadata": {},
   "source": [
    "##  Record linkage\n",
    "0%\n",
    "\n",
    "Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings. In this chapter, you'll learn how to link records by calculating the similarity between strings—you’ll then use your new skills to join two restaurant review datasets into one clean master dataset.\n",
    "\n",
    "    Comparing strings    50 xp\n",
    "    Minimum edit distance    50 xp\n",
    "    The cutoff point    100 xp\n",
    "    Remapping categories II    100 xp\n",
    "    Generating pairs    50 xp\n",
    "    To link or not to link?    100 xp\n",
    "    Pairs of restaurants    100 xp\n",
    "    Similar restaurants    100 xp\n",
    "    Linking DataFrames    50 xp\n",
    "    Getting the right index    50 xp\n",
    "    Linking them together!    100 xp\n",
    "    Congratulations!    50 xp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094813d4-e269-4a00-8837-ebe1ea5fb63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa37edc2-e0fe-4481-a3d7-c2f29bf3dbee",
   "metadata": {},
   "source": [
    "## Data type constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The institutor's name is Adel, he'll be our host as we learn how to clean data in Python.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# In this course, we're going to understand how to diagnose different problems in our data and how they can come up during out workflow.  We will also understand the side effects of not treating our data correctly.  And various ways to address different types of dirty data.  \n",
    "\n",
    "\n",
    "In this chapter, we're going to discuss the most common data problems you may encounter and how to address them.  \n",
    "\n",
    "# To understand why we need to clean data, lets remind ourselves of the data science workflow.  In a typical data science workflow, we usually access our raw data, explore and process it, develop insights using visualizations or predictive models, and finally report these insights with dashboards or reports.  \n",
    "\n",
    "\n",
    "Access Data --> Explore and Process Data --> Extract Insights --> Report Insights\n",
    "\n",
    "\n",
    "# Dirty data can appear because of duplicate values, miss-spellings, data type parsing errors and legacy systems.  \n",
    "Without making sure that data is properly clearned in the exploration and processing phase, we will surely compromise the insights and reports subsequently generated.  As the old adage says, garbage in garbage out.  \n",
    "\n",
    "\n",
    "When working with data, there are various types that we may encounter along the way.  We could be working with text data, integers, decimals, dates, zip codes, and others.  Luckily, Python has specific data type objects for various data types that you're probably familiar with by now.  This makes it much easier to manipulate these various data types in Python.  \n",
    "    \n",
    "    Text data, Integers, Decimals, Binary, Dates, Categories\n",
    "    str, int, float, bool, datetime, cateory\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# As such, before preparing to analyze and extract insights from our data, we need to make sure our variables have the correct data types, other wise we risk compromising our anaysis.  Lets take a look at the following example.  \n",
    "\n",
    "\n",
    "Belwo is a head od a DF containing revenue generated and quantity of items sold for a sales order.  We want to calculate the total revenue generated by all sales orders.  As you can see, the Revenue dolumn has the dollar sign on the righthand side.  A close inspection of the DF columns' data types using the \".dtypes\" attribute returns object for the Revenue column, which is what Pandas uses to store strings.  We can also check the data types as well as the number of missing values per column in a DataFrame, by using the \".info()\" method.  \n",
    "\n",
    "Since the Revenue column is a string, summing across all sales order returns one large concatenated string containing each row's string.  To fix this, we need to first remove the $ sign from the string so that Pandas is able to convert the strings into numbers without error (how about we define it at the first place? need to do some test).  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To do this with the \"sales['Revenue'].str.strip('$')\" method, while specifying the string we want to strip as an argument which is in this case the dollar sign.  \n",
    "Since our dollar value do not contain decimals, we then convert the Revenue column to an integer by using the \".astype()\" method , specifying the desired data type as argument.  Had our revenue value been decimal, we would have converted the Revenue column to float.  \n",
    "We can make sure that the Revenue column is now an integer by using the \"assert\" statement, which takes in a condition as input, and returns nothing if that condition is met, and ar error is it is not.  \n",
    "You can test almost anything you can image of by using \"assert\", and we'll see more ways to utilize it as we go along the course.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "# A common type of data seems numeric but actually represents cateories with a finite set of possible categories.  This is called categorical data.  \n",
    "We will look more closely at categorical data in Chapter 2, but lets take a look at this example.  Here we have a marriage status column, which is represented by 0 for never married, 1 for married, 2 for separated, and 3 for divorced.  \n",
    "\n",
    "However it will be imported of type integer, which could lead to misleading results when trying to extract some statistical summaries.  We can solve this by using the same \".astype()\" method seen earlier, but this time specifying the \"category\" data type.  When applying the \".describe()\" method again, we see that the summary statistics are much more aligned with that of a categorical variable, discussinng the number of observations, number of unique values, most frequent category instead of mean and standard deviation.  \n",
    "\n",
    "\n",
    "Now that we have solid understanding of data type constrains - lets get to practice.  \n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Remove $ from Revenue column\n",
    "sales['Revenue'] = sales['Revenue'].str.strip('$')\n",
    "sales['Revenue'] = sales['Revenue'].astype('int')\n",
    "\n",
    "# Verify that Revenue is now an integer\n",
    "assert sales['Revenue'].dtypes == 'int'\n",
    "\n",
    "\n",
    "\n",
    "# Convert to categorical\n",
    "df['marriage_status'] = df['marriage_status'].astype('category')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53a2fc-1b2d-48da-9110-a923c4faea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales.csv')\n",
    "\n",
    "print(sales.head())\n",
    "\n",
    "\n",
    "---------------------------------------------\n",
    "  SalesOrderId  | Revenue     | Quantity\n",
    "0        43659  |     23153$  |       12\n",
    "1        43660  |      1457$  |        2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c075397b-8c4e-45ec-b130-394c52927157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23153\n",
      "<class 'str'>\n",
      "23153\n",
      "<class 'int'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4640/1974750029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#assert int(n_price).dtype() == 'int'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "price = '23153$'\n",
    "\n",
    "\n",
    "n_price = price.strip('$')\n",
    "print(n_price)\n",
    "print(type(n_price))\n",
    "\n",
    "print(int(n_price))\n",
    "print(type(int(n_price)))\n",
    "\n",
    "\n",
    "#assert int(n_price).dtype() == 'int'\n",
    "assert type(int(n_price)) == 'int'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9797795-53fc-4193-95bc-156f95347465",
   "metadata": {},
   "source": [
    "## Common data types\n",
    "\n",
    "Manipulating and analyzing data with incorrect data types could lead to compromised analysis as you go along the data science workflow.\n",
    "\n",
    "When working with new data, you should always check the data types of your columns using the .dtypes attribute or the .info() method which you'll see in the next exercise. Often times, you'll run into columns that should be converted to different data types before starting any analysis.\n",
    "\n",
    "In this exercise, you'll first identify different types of data and correctly map them to their respective types.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Assign each card to what type of data you think it is.\n",
    "\n",
    "Numeric data type: Salary earned monthly, \n",
    "                   Number of items bought in a basket, \n",
    "                   Number of points on customer loyalty card\n",
    "\n",
    "Text:              City of residence, \n",
    "                   First name\n",
    "                   Shipping address of a customer\n",
    "\n",
    "Dates:             Birthdates of clients, \n",
    "                   Order date of a product, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0971b53-b4fd-42ea-930a-01603d66afd4",
   "metadata": {},
   "source": [
    "## Numeric data or ... ?\n",
    "\n",
    "In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called \"ride_sharing\". It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.\n",
    "\n",
    "The \"user_type\" column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "\n",
    "    1 for free riders.\n",
    "    2 for pay per ride.\n",
    "    3 for monthly subscribers.\n",
    "\n",
    "In this instance, you will print the information of ride_sharing using .info() and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset. The pandas package is imported as pd.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the information of ride_sharing.\n",
    "#    Use .describe() to print the summary statistics of the user_type column from ride_sharing.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    By looking at the summary statistics - they don't really seem to offer much description on how users are distributed along their purchase type, why do you think that is?\n",
    "Possible Answers\n",
    "    -The user_type column is not of the correct type, it should be converted to str.\n",
    "    -The user_type column has an infinite set of possible values, it should be converted to category.\n",
    "#    -The user_type column has an finite set of possible values that represent groupings of data, it should be converted to category.     Also other 'int64' values, station_A_id, bike_id, user_birth_year, user_gender\n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "    Question 3\n",
    "#    -Convert \"user_type\" into categorical by assigning it the 'category' data type and store it in the \"user_type_cat\" column.\n",
    "#    -Make sure you converted \"user_type_cat\" correctly by using an \"assert\" statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcee990f-9595-4f4e-ab9c-66e9528957f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration           object\n",
      "station_A_id        int64\n",
      "station_A_name     object\n",
      "station_B_id        int64\n",
      "station_B_name     object\n",
      "bike_id             int64\n",
      "user_type           int64\n",
      "user_birth_year     int64\n",
      "user_gender        object\n",
      "dtype: object\n",
      "     duration  station_A_id  \\\n",
      "0  12 minutes            81   \n",
      "1  24 minutes             3   \n",
      "2   8 minutes            67   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "\n",
      "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480          2             1959   \n",
      "1  Eureka Valley Recreation Center     5193          2             1965   \n",
      "2    The Embarcadero at Steuart St     3652          3             1993   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "category\n",
      "user_type describe\n",
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type, dtype: int64\n",
      "12972\n",
      "0        2\n",
      "1        2\n",
      "4        2\n",
      "5        2\n",
      "6        2\n",
      "        ..\n",
      "25752    2\n",
      "25753    2\n",
      "25756    2\n",
      "25757    2\n",
      "25758    2\n",
      "Name: user_type, Length: 12972, dtype: category\n",
      "Categories (3, int64): [1, 2, 3]\n",
      "0        2\n",
      "1        2\n",
      "4        2\n",
      "5        2\n",
      "6        2\n",
      "        ..\n",
      "25752    2\n",
      "25753    2\n",
      "25756    2\n",
      "25757    2\n",
      "25758    2\n",
      "Name: user_type, Length: 12972, dtype: category\n",
      "Categories (3, int64): [1, 2, 3]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  object  \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(4), object(4)\n",
      "memory usage: 1.8+ MB\n",
      "None\n",
      "       station_A_id  station_B_id       bike_id  user_birth_year\n",
      "count  25760.000000  25760.000000  25760.000000     25760.000000\n",
      "mean      31.023602     89.558579   4107.621467      1983.054969\n",
      "std       26.409263    105.144103   1576.315767        10.010992\n",
      "min        3.000000      3.000000     11.000000      1901.000000\n",
      "25%       15.000000     21.000000   3106.000000      1978.000000\n",
      "50%       21.000000     58.000000   4821.000000      1985.000000\n",
      "75%       67.000000     93.000000   5257.000000      1990.000000\n",
      "max       81.000000    383.000000   6638.000000      2001.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(df.head(3))\n",
    "\n",
    "\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "#print(dir(df['user_type']))\n",
    "\n",
    "print(df['user_type'].dtypes)\n",
    "assert df['user_type'].dtype == 'category' # ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('user_type describe')\n",
    "print(df['user_type'].describe())\n",
    "\n",
    "\n",
    "print(len(df[df['user_type']==2]))\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# filt_list = movies_genres.loc[movies_genres['_merge']=='both', 'title'].unique()\n",
    "print(df.loc[df['user_type']==2, 'user_type'])\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "print(df[df['user_type']==2]['user_type'].copy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48055e6c-63c2-47b3-a701-ada0b5216392",
   "metadata": {},
   "source": [
    "## Summing strings and concatenating numbers\n",
    "\n",
    "In the previous exercise, you were able to identify that category is the correct data type for user_type and convert it \n",
    "# in order to extract relevant statistical summaries that shed light on the distribution of user_type.\n",
    "\n",
    "# Another common data type problem is importing what should be numerical values as strings, \n",
    "as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.\n",
    "\n",
    "In this exercise, you'll be converting the string column duration to the type int. Before that however, you will need to make sure to strip \"minutes\" from the column in order to make sure pandas reads it as numerical. The pandas package has been imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Use the \".strip()\" method to strip duration of \"minutes\" and store it in the \"duration_trim\" column.\n",
    "    Convert \"duration_trim\" to int and store it in the \"duration_time\" column.\n",
    "    Write an \"assert\" statement that checks if duration_time's data type is now an int.\n",
    "    Print the average ride duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "738ded22-e604-4f3a-b0ec-f93a3d873df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration           object\n",
      "station_A_id        int64\n",
      "station_A_name     object\n",
      "station_B_id        int64\n",
      "station_B_name     object\n",
      "bike_id             int64\n",
      "user_type           int64\n",
      "user_birth_year     int64\n",
      "user_gender        object\n",
      "dtype: object\n",
      "0    12 minutes\n",
      "1    24 minutes\n",
      "2     8 minutes\n",
      "3     4 minutes\n",
      "4    11 minutes\n",
      "Name: duration, dtype: object\n",
      "0    12\n",
      "1    24\n",
      "2     8\n",
      "3     4\n",
      "4    11\n",
      "Name: duration, dtype: int64\n",
      "\n",
      "\n",
      "<bound method NDFrame._add_numeric_operations.<locals>.mean of 0        12\n",
      "1        24\n",
      "2         8\n",
      "3         4\n",
      "4        11\n",
      "         ..\n",
      "25755    11\n",
      "25756    10\n",
      "25757    14\n",
      "25758    14\n",
      "25759    29\n",
      "Name: duration, Length: 25760, dtype: int64>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  int64   \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(5), object(3)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.dtypes)\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "\n",
    "print(df['duration'].head())\n",
    "\n",
    "\n",
    "df['duration'] = df['duration'].str.strip('minutes')\n",
    "\n",
    "df['duration'] = df['duration'].astype('int')\n",
    "print(df['duration'].head())\n",
    "\n",
    "print('\\n')\n",
    "print(df['duration'].mean)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261ae62-1f89-472f-8ee3-e8315793665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].____.____()\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ____\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['____'].____ == '____'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f7a79-1ec0-460d-8b95-01691e25bbe2",
   "metadata": {},
   "source": [
    "## Data range constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Hi and welcome back.  In this lesson, we're going to discuss data that should fall within a range.  Lets first start off with some motivation.  \n",
    "\n",
    "Imagine we have a dataset of movies with their respective average rating from a streaming service.  The rating can be any integer between 1 and 5.  After creating a histogram with Matplotlib, we see that there are a few movies with an average rating of 6, which is well above the allowable range. \n",
    "# This is most likely an error in data collection or parsing, where a variable is well beyond its range and treating it is essential to have accurate analysis.  \n",
    "\n",
    "Here is another example, where we see subscription dates in the future for a service.  \n",
    "# Inherently this doesn't make any sence, as well cannot sign up for a service in the future, but these errors exist either due to technical or human error.  \n",
    "We use the datetime package's \".date.today()\" function to get today's date, and we filter the dateset by any subscription date higher than today's date.  \n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# We need to pay attention to the range of our data. \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# There's a variety of options to deal with out of range data.  \n",
    "\n",
    "(1) The simplest option is to drop the data.  \n",
    "However, depending on the size of your out of range data, you could be loosing out on essential information.  As a rule of thumb, only drop data when a small proportion of your dataset is affected by out of range values, however you really need to understand your dataset before deciding to drop values.  \n",
    "\n",
    "(2) Another option would be setting custom minimums or maximums to your columns.  \n",
    "\n",
    "(3) We could also set the data to be missing, and impute it, but we'll take a look at how to deal with missing data in Chapter 3.  \n",
    "\n",
    "(4) We could also, dependent on the business assumptions behind our data, assign a custom value for any values of our data that go beyond a certain range.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take a look at the movies example mentioned earlier.  We first isolate the movies with ratings higher than 5.  Now if these values are affect a small set of our data, we can drop them.  We can drop them is 2 ways - we can either create a new filtered movies DF where we only keep values of \"avg_rating\" lower than or equal to 5.  Or drop the values by using the \".drop()\" method.  The \".drop()\" method takes in as argument the row indices of movies for which the \"avg_rating\" is higher than 5.  We set the \"inplace=\" argument to True so that values are dropped in place and we don't have to create a new column.  We can make sure this is set in place using assert statement that checks if the maximumof avg_rating is lower lan or equal to 5.  \n",
    "\n",
    "Depending on the assumptions behind our data, we can also change the out of range values to a hard limit.  For example, here we're setting any value of the avg_rating column to 5 if it goes beyond it.  We can do this using the \".loc[]\" method, which returns all cells that fit a custom row and column index.  It takes as first argument the row index, or here all instaces of avg_rating above 5, and second argument the column index, which is here the avg_rating column.  Aagin, we can make sure that this change was done using an assert statement.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take another look at the date range example mentioned earlier, where we have subscriptions happening in the future.  We first look at the datetypes (.dtype) of the column with the \".dtype\" attribute.  We can confirm that the subscription_date column is an \"object\" and not a \"datetime\" object.  Datetime objects allow much earsier manipulation of date data, so lets convert it to that.  We do so with \"pd.to_datetime()\" function from Pandas, which takes in as argument the column we want to convert.  We can then test the data type conversion by asserting that the subscription date's column is equal to \"datetime64[ns]\", which is how the data type is represented in Pandas.  \n",
    "\n",
    "Now that the column is in datetime, we can treat it in a variety of ways.  We first create a today_date variable using the datetime function \".date.today()\", which allows us to store today's date.  We can then either drop the rows with exceeding dates similar to how we did in the average rating example, or replace exceeding values with todays date.  \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(movies['avg_rating'])\n",
    "plt.title('Average rating of movies (1-5)')\n",
    "\n",
    "\n",
    "\n",
    "# Import date time\n",
    "import datetime\n",
    "\n",
    "today_date = datetime.date.today()\n",
    "user_signups[user_signups['subscription_date'] > today_date]\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# Output Movies with range > 5\n",
    "movies[movies['avg_rating'] > 5]\n",
    "\n",
    "\n",
    "\n",
    "# Drop values using filtering\n",
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "\n",
    "# Drop values using .drop()\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True)  #***********************************************\n",
    "\n",
    "assert movies['avg_rating'].max() == 5\n",
    "\n",
    "\n",
    "\n",
    "# Convert avg_rating > 5 to value 5\n",
    "movies.loc[movies['avg_rating']>5, 'avg_rating'] = 5\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataTime\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date'])\n",
    "\n",
    "# Assert that conversion happened\n",
    "assers user_signups['subscription_date'].dtype == 'datetime64[ns]'  #################################################\n",
    "\n",
    "\n",
    "<1> Drop the data\n",
    "today_date = datetime.date.today()\n",
    "\n",
    "# Drop values using filtering\n",
    "user_signups = user_signups[user_signups['subscription_date'] < today_date]\n",
    "# Drop values using .drop()\n",
    "user_signups.drop(user_signups[user_signups['subscription_date'] < today_date].index, inplace = True)  ##############\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "<2> Hardcode dates with upper limit\n",
    "\n",
    "# Replace values using filtering\n",
    "user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date\n",
    "# Assert is true\n",
    "assert user_signups['subscription_date'].max().date() <= today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc5690b8-861f-4968-a047-36524ae5bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Search': [{'Title': 'Batman: The Killing Joke', 'Year': '2016', 'imdbID': 'tt4853102', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BMTdjZTliODYtNWExMi00NjQ1LWIzN2MtN2Q5NTg5NTk3NzliL2ltYWdlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Mask of the Phantasm', 'Year': '1993', 'imdbID': 'tt0106364', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTRiMWM3MGItNjAxZC00M2E3LThhODgtM2QwOGNmZGU4OWZhXkEyXkFqcGdeQXVyNjExODE1MDc@._V1_SX300.jpg'}, {'Title': 'Batman: The Dark Knight Returns, Part 2', 'Year': '2013', 'imdbID': 'tt2166834', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTEzMmE0ZDYtYWNmYi00ZWM4LWJjOTUtYTE0ZmQyYWM3ZjA0XkEyXkFqcGdeQXVyNTA4NzY1MzY@._V1_SX300.jpg'}, {'Title': 'Batman: Year One', 'Year': '2011', 'imdbID': 'tt1672723', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BNTJjMmVkZjctNjNjMS00ZmI2LTlmYWEtOWNiYmQxYjY0YWVhXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Assault on Arkham', 'Year': '2014', 'imdbID': 'tt3139086', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BZDU1ZGRiY2YtYmZjMi00ZDQwLWJjMWMtNzUwNDMwYjQ4ZTVhXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman', 'Year': '1966', 'imdbID': 'tt0060153', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BMmM1OGIzM2UtNThhZS00ZGNlLWI4NzEtZjlhOTNhNmYxZGQ0XkEyXkFqcGdeQXVyNTkxMzEwMzU@._V1_SX300.jpg'}, {'Title': 'Batman: Gotham Knight', 'Year': '2008', 'imdbID': 'tt1117563', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BM2I0YTFjOTUtMWYzNC00ZTgyLTk2NWEtMmE3N2VlYjEwN2JlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman: Arkham City', 'Year': '2011', 'imdbID': 'tt1568322', 'Type': 'game', 'Poster': 'https://m.media-amazon.com/images/M/MV5BZDE2ZDFhMDAtMDAzZC00ZmY3LThlMTItMGFjMzRlYzExOGE1XkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg'}, {'Title': 'Batman Beyond', 'Year': '1999–2001', 'imdbID': 'tt0147746', 'Type': 'series', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYTBiZjFlZDQtZjc1MS00YzllLWE5ZTQtMmM5OTkyNjZjMWI3XkEyXkFqcGdeQXVyMTA1OTEwNjE@._V1_SX300.jpg'}, {'Title': 'Son of Batman', 'Year': '2014', 'imdbID': 'tt3139072', 'Type': 'movie', 'Poster': 'https://m.media-amazon.com/images/M/MV5BYjdkZWFhNzctYmNhNy00NGM5LTg0Y2YtZWM4NmU2MWQ3ODVkXkEyXkFqcGdeQXVyNTA0OTU0OTQ@._V1_SX300.jpg'}], 'totalResults': '497', 'Response': 'True'}\n",
      "['Batman: The Killing Joke', 'Batman: Mask of the Phantasm', 'Batman: The Dark Knight Returns, Part 2', 'Batman: Year One', 'Batman: Assault on Arkham', 'Batman', 'Batman: Gotham Knight', 'Batman: Arkham City', 'Batman Beyond', 'Son of Batman']\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.omdbapi.com/?t=hackers'\n",
    "url = 'http://www.omdbapi.com/?i=tt3896198&apikey=8c241015'\n",
    "url = 'http://www.omdbapi.com/?s=Batman&page=1&apikey=8c241015'\n",
    "url = 'http://www.omdbapi.com/?s=Batman&page=2&apikey=8c241015'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "r = requests.get(url)\n",
    "j_data = r.json()\n",
    "\n",
    "print(j_data)\n",
    "\n",
    "\n",
    "\n",
    "titles = []\n",
    "with requests.get(url) as r:\n",
    "    j_data = r.json()\n",
    "    content = j_data['Search']\n",
    "    for i in content:\n",
    "        titles.append(i['Title'])\n",
    "    \n",
    "\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33c05df2-cb75-42de-89e9-69dccb9d1d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     duration  station_A_id  \\\n",
      "0  12 minutes            81   \n",
      "1  24 minutes             3   \n",
      "2   8 minutes            67   \n",
      "3   4 minutes            16   \n",
      "4  11 minutes            22   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "3                            Steuart St at Market St            28   \n",
      "4                              Howard St at Beale St           350   \n",
      "\n",
      "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480          2             1959   \n",
      "1  Eureka Valley Recreation Center     5193          2             1965   \n",
      "2    The Embarcadero at Steuart St     3652          3             1993   \n",
      "3     The Embarcadero at Bryant St     1883          1             1979   \n",
      "4             8th St at Brannan St     4626          2             1994   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "3        Male  \n",
      "4        Male  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25760 entries, 0 to 25759\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype   \n",
      "---  ------           --------------  -----   \n",
      " 0   duration         25760 non-null  object  \n",
      " 1   station_A_id     25760 non-null  int64   \n",
      " 2   station_A_name   25760 non-null  object  \n",
      " 3   station_B_id     25760 non-null  int64   \n",
      " 4   station_B_name   25760 non-null  object  \n",
      " 5   bike_id          25760 non-null  int64   \n",
      " 6   user_type        25760 non-null  category\n",
      " 7   user_birth_year  25760 non-null  int64   \n",
      " 8   user_gender      25760 non-null  object  \n",
      "dtypes: category(1), int64(4), object(4)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_A_id</th>\n",
       "      <th>station_B_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25760.000000</td>\n",
       "      <td>25760.000000</td>\n",
       "      <td>25760.000000</td>\n",
       "      <td>25760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.023602</td>\n",
       "      <td>89.558579</td>\n",
       "      <td>4107.621467</td>\n",
       "      <td>1983.054969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.409263</td>\n",
       "      <td>105.144103</td>\n",
       "      <td>1576.315767</td>\n",
       "      <td>10.010992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1901.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3106.000000</td>\n",
       "      <td>1978.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>4821.000000</td>\n",
       "      <td>1985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>67.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>5257.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>6638.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station_A_id  station_B_id       bike_id  user_birth_year\n",
       "count  25760.000000  25760.000000  25760.000000     25760.000000\n",
       "mean      31.023602     89.558579   4107.621467      1983.054969\n",
       "std       26.409263    105.144103   1576.315767        10.010992\n",
       "min        3.000000      3.000000     11.000000      1901.000000\n",
       "25%       15.000000     21.000000   3106.000000      1978.000000\n",
       "50%       21.000000     58.000000   4821.000000      1985.000000\n",
       "75%       67.000000     93.000000   5257.000000      1990.000000\n",
       "max       81.000000    383.000000   6638.000000      2001.000000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('ride_sharing_new.csv', index_col=0)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df['user_type'] = df['user_type'].astype('category')\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "df.describe()\n",
    "\n",
    "#help(df.drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d396a30-5313-49b7-8f24-51dd411433b5",
   "metadata": {},
   "source": [
    "## Tire size constraints\n",
    "\n",
    "In this lesson, you're going to build on top of the work you've been doing with the ride_sharing DataFrame. You'll be working with the \"tire_sizes\" column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.\n",
    "\n",
    "# In this exercise, you will make sure the tire_sizes column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Convert the \"tire_sizes\" column from category to 'int'.\n",
    "    Use \".loc[]\" method to set all values of tire_sizes above 27 to 27.\n",
    "    Reconvert back tire_sizes to 'category' from int.\n",
    "    Print the description of the tire_sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f6c902-1baf-4688-8bd5-21b7cd884a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 duration  station_A_id  \\\n",
      "0           0      12             81   \n",
      "1           1      24              3   \n",
      "2           2       8             67   \n",
      "3           3       4             16   \n",
      "4           4      11             22   \n",
      "\n",
      "                                      station_A_name  station_B_id  \\\n",
      "0                                 Berry St at 4th St           323   \n",
      "1       Powell St BART Station (Market St at 4th St)           118   \n",
      "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
      "3                            Steuart St at Market St            28   \n",
      "4                              Howard St at Beale St           350   \n",
      "\n",
      "                    station_B_name  bike_id user_type  user_birth_year  \\\n",
      "0               Broadway at Kearny     5480         2             1959   \n",
      "1  Eureka Valley Recreation Center     5193         2             1965   \n",
      "2    The Embarcadero at Steuart St     3652         3             1993   \n",
      "3     The Embarcadero at Bryant St     1883         1             1979   \n",
      "4             8th St at Brannan St     4626         2             1994   \n",
      "\n",
      "  user_gender  \n",
      "0        Male  \n",
      "1        Male  \n",
      "2        Male  \n",
      "3        Male  \n",
      "4        Male  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3788/633058815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert tire_sizes to integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mride_sharing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tire_sizes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m____\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'____'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m____\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'____'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Set all values above 27 to 27\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ride_sharing = pd.read_csv('ride_sharing_new.csv')\n",
    "\n",
    "ride_sharing['duration'] = ride_sharing['duration'].str.strip('minutes')\n",
    "ride_sharing['duration'].astype('int')\n",
    "\n",
    "ride_sharing['user_type'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "print(ride_sharing.head())\n",
    "\n",
    "\n",
    "\n",
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_size'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_size'] > 27, 'tire_size'] = 27  # ***********************************************\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')  # ***************************************\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0113d-fa1f-4406-8656-45de4d583eed",
   "metadata": {},
   "source": [
    "## Back to the future\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "A new update to the data pipeline feeding into the \"ride_sharing\" DataFrame has been updated to register each ride's date. This information is stored in the \"ride_date\" column of the type \"object\", which represents strings in pandas.\n",
    "\n",
    "# A bug was discovered which was relaying rides taken today as taken next year (think other conditions, last year?).\n",
    "To fix this, you will find all instances of the \"ride_date\" column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert \"ride_date\" to a datetime object.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "The datetime package has been imported as dt, alongside all the packages you've been using till now.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Convert ride_date to a datetime object and store it in ride_dt column using to_datetime().\n",
    "    Create the variable today, which stores today's date by using the \"dt.date.today()\" function.\n",
    "#    For all instances of \"ride_dt\" in the future, set them to today's date.\n",
    "    Print the maximum date in the ride_dt column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb53826-f7e6-4948-bd7b-d8c71fca039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "today_date = datetime.date.today()\n",
    "\n",
    "ride_sharing['ride_date'] = ride_sharing['ride_date'].astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "ride_sharing['ride_date'] = ride_sharing.loc[ride_sharing['ride_date'] > today_date, 'ride_date'] = today_date\n",
    "ride_sharing['ride_date'] = ride_sharing[ride_sharing['ride_date']>today_date]['ride_date'] = today_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d47f4-a445-4b81-aec4-a470efe94816",
   "metadata": {},
   "source": [
    "# Your thinking makes you smarter, not stareing at the answer, you learn nothing in looking at it, but learn a lot in figuring it out.  Choose the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae977680-7059-4d50-ab84-7e48225d9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to datetime\n",
    "ride_sharing['ride_dt'] = pd.____(____['____'])\n",
    "\n",
    "# Save today's date\n",
    "today = ____\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.____[____['____'] > ____, '____'] = ____\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].____())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5865b6c-ef93-4273-a400-e04e00372d6e",
   "metadata": {},
   "source": [
    "## Uniqueness constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Hi and welcome to the final lesson of this chapter.  Lets discuss another common data clearning problem, duplicate values.  Duplicate values can be diagnosed when we have the same exact information repeated across multiple row, for some or all column of our DF.  \n",
    "\n",
    "In this example DF containing the names, address, height, and weight of individuals, the rows represented have identical values across all columns.  In this one there are duplicate values for all columns except the height column, which leads us to think its more likely a data entry error than an actural other person.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Apart from data entry and human errors alluded to in the previous slide, duplicate data can also arise because of bugs and design errors wheather in business processes or data pipelines.  However, they often most arise form the necessary act of joining and consolidating data from various resources, which could retain duplicate values.  \n",
    "\n",
    "\n",
    "# Lets first see how to find duplicate values.  \"df.duplicated()\" method --> boolean indexing\n",
    "In this example we're working with a bigger version of the height and weight data seen earlier in this video.  We can find duplicates in a DF by using \"df.duplicated()\" method.  It returns a Series of boolean values that are True for duplicated values, and False for non-duplicated values.  \n",
    "We can see exactly which rows are affected by using boolean indexing (df[df.duplicated()]).  However using \"df.duplicated()\" without playing around with the arguments of the method can lead to misleading results, as all of the columns are required to have duplicated values by default, with all duplicated values being marked as True except for the first occurence.  \n",
    "# Here, can I recall how we do the DF merger and concatnate operations?  Think and think\n",
    "This limit our ability to properly diagnose what type of duplication we have, how to effectively treat it.  To properly calibrate how we go about finding duplicates, we will use 2 arguments from the \"df.duplicated()\" method.  The \"subset=\" argument lets us set a list of column names to check for duplication.  For example, it can allows us to find duplicates for the first and last name columns only.  The \"keep=\" argument lets us keep the first occurrence of a duplicate value by setting it to the tring \"first\", \"last\", or keep all occurances of duplicated values by setting it to \"False\".  In below example, we're checking for duplicates across the first name, last name and address columns\n",
    "and we choosing to keep all duplicates.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "We see the following results, to get a better bird's eye view of the duplicates, we sort the duplicated rows using \"df.sort_values(by='first_name')\" method, choosing \"first_name\" to sort by.  We find that there are 4 sets of duplicated rows, the first 2 being complete duplicates of each other across all columns.  The other 2 being incomplete duplicates of each other with discrepancies across height and weight respectively.  \n",
    "\n",
    "# \".drop_duplicates()\" method\n",
    "The complete duplicates can be treated easily.  All that required is keep one of them only and discard the others.  This can be down with the \".drop_duplicates()\" method, which also takes in the same \"subset=\" and \"keep=\" argument asin the \".duplicated()\" method, as well as the \"inplace=\" argument which drops the duplicated values directly inside the height_weight DF.  Here we are droping complete duplicates only, so its not necessary nor adviable to set a \"subset=\", and since the \"keep=\" argument takes in \"first\" as default, we can keep it as such.  Note that we can also set it as \"last\", but not as \"False\" as it would keep all duplicates.  \n",
    "\n",
    "This leaves us with the other 2 sets of duplicates discussed earlier, which are the same for first_name, last_name and address, but contain discrepancies in height and weight.  \n",
    "\n",
    "# Apart from droping rows with really small discrepancies, we can use a statistical measure to combine each set of duplicated values.  \n",
    "For example, we can combine these 2 rows into 1 by computing the average mean between them, or the maximum, or other statistical measures, this is highly dependent on a common sense understanding of our data, and what type of data we have.  We can do this easily using the \".groupby()\" method, which when chained with \".agg()\" method, let you group by a set of common columns and return statistical values for specific columns when the aggregation is being performed. \n",
    "\n",
    "For example here, we created a dictionary called summaries, which instructs groupby to return the maximum of duplicated rows for the height column, and the mean duplicated rows for the weight column.  \n",
    "We then \".groupby()\" height_weight by the column names defined earlier, and chained it with the \"agg()\" method, which takes in the summaries dictionary we created.  \n",
    "We chain this entire line with the \".reset_index()\" method, so that we can have numbered indices in the final output. \n",
    "We can verify that there are no more duplicate values by running the \".duplicated()\" method again, and use brackets to output duplicate rows.  \n",
    "\n",
    "\n",
    "Now that we have a solid grasp of dupliccation, lets practice.  \n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------\n",
    "first_name   | last_name   | address                                    | height   | weight\n",
    "Justin       | Saddlemyer  | Boulevard du Jardin Botainque 3, Bruxelles | 193 cm   | 87 kg\n",
    "Justin       | Saddlemyer  | Boulevard du Jardin Botainque 3, Bruxelles | 194 cm   | 87 kg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Data Entry & Human Error\n",
    "\n",
    "Bugs and design errors\n",
    "\n",
    "Join or merge Errors\n",
    "\n",
    "\n",
    "\n",
    "# Column names to check for duplication\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)  #############################################\n",
    "\n",
    "print(height_weight[duplicates])\n",
    "\n",
    "----------------------------------------------------- Image what will be the output of it\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output duplicate values\n",
    "height_weight[duplicates].sort_values(by='first_name')   # Think why choosing f_name, ll_name, address set duplicate \n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "     first_name  | last_name  | address                              | height  | weight\n",
    " 22        Cole  |    Palmer  |                     8366 At, Street  |    178  | 91\n",
    "102        Cole  |    Palmer  |                     8366 At, Street  |    178  | 91\n",
    " 28     Desirae  |   Shannon  | P.O. Box 643, 5251 Consectetuer, Rd. |    195  | 83\n",
    "103     Desirae  |   Shannon  | P.O. Box 643, 5251 Consectetuer, Rd. |    196  | 83\n",
    "  1        Ivor  |    Pierce  |                   102-3364 Non. Road |    168  | 66\n",
    "101        Ivor  |    Pierce  |                   102-3364 Non. Road |    168  | 88\n",
    " 37        Mary  |     Colon  |                         4674 Ut Rd.  |    179  | 75\n",
    "100        Mary  |     Colon  |                         4674 Ut Rd.  |    179  | 75\n",
    "         \n",
    "         \n",
    "# Drop duplicates\n",
    "height_weight.drop_duplicates(inplace=True)  ########################################################################\n",
    "\n",
    "\n",
    "height_weight.groupby(['first_name', 'last_name', 'address']).agg(np.mean)  ??\n",
    "\n",
    "# Groupby column names and produce statistical summaries\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "summaries = {'height': 'max', 'weight': 'mean'}\n",
    "height_weight = height_weight.groupby(by=column_names).agg(summaries).reset_index()  ################################\n",
    "\n",
    "# Make sure aggregation is done\n",
    "duplicates = height_weight.duplicated(subset=column_names, keep=False)\n",
    "height_weight[duplicates].sort_values(by='first_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c53b81-edc2-42a3-b12c-1ed622b2a063",
   "metadata": {},
   "source": [
    "## How big is your subset?\n",
    "\n",
    "You have the following loans DataFrame which contains loan and credit score data for consumers, and some metadata such as their first and last names. You want to find both complete and incomplete duplicates using \".duplicated()\" method.\n",
    "first_name \tlast_name \t    credit_score \thas_loan\n",
    "Justin \t    Saddlemeyer \t600 \t        1\n",
    "Hadrien \tLacroix \t    450 \t        0\n",
    "\n",
    "Choose the correct usage of \".duplicated()\" below:\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    loans.duplicated()\n",
    "      Because the default method returns both complete and incomplete duplicates.  X Full duplicates in every col\n",
    "    press\n",
    "    1\n",
    "    loans.duplicated(subset = 'first_name')\n",
    "      Because constraining the duplicate rows to the first name lets me find incomplete duplicates as well.  X\n",
    "    press\n",
    "    2\n",
    "    loans.duplicated(subset = ['first_name', 'last_name'], keep = False)\n",
    "      Because subsetting on consumer metadata and not discarding any duplicate returns all duplicated rows.\n",
    "    press   X set \"keep=\" arg to False will keep all duplicates\n",
    "    3\n",
    "#    loans.duplicated(subset = ['first_name', 'last_name'], keep = 'first')\n",
    "      Because this drops all duplicates.\n",
    "    press\n",
    "    4\n",
    "    \n",
    "# The value we set for \"keep=\" arg is for \"mark\" boolean, not keep the records\n",
    "# *******************************************************************************************************************\n",
    "    keep : {'first', 'last', False}, default 'first'\n",
    "        Determines which duplicates (if any) to mark.\n",
    "    \n",
    "        - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n",
    "        - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n",
    "        - False : Mark all duplicates as ``True``.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a93835-bfd1-41c9-b581-403a0c6f59fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function duplicated in module pandas.core.frame:\n",
      "\n",
      "duplicated(self, subset: 'Hashable | Sequence[Hashable] | None' = None, keep: \"Literal['first'] | Literal['last'] | Literal[False]\" = 'first') -> 'Series'\n",
      "    Return boolean Series denoting duplicate rows.\n",
      "    \n",
      "    Considering certain columns is optional.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    subset : column label or sequence of labels, optional\n",
      "        Only consider certain columns for identifying duplicates, by\n",
      "        default use all of the columns.\n",
      "    keep : {'first', 'last', False}, default 'first'\n",
      "        Determines which duplicates (if any) to mark.\n",
      "    \n",
      "        - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n",
      "        - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n",
      "        - False : Mark all duplicates as ``True``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series\n",
      "        Boolean series for each duplicated rows.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Index.duplicated : Equivalent method on index.\n",
      "    Series.duplicated : Equivalent method on Series.\n",
      "    Series.drop_duplicates : Remove duplicate values from Series.\n",
      "    DataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Consider dataset containing ramen rating.\n",
      "    \n",
      "    >>> df = pd.DataFrame({\n",
      "    ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
      "    ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
      "    ...     'rating': [4, 4, 3.5, 15, 5]\n",
      "    ... })\n",
      "    >>> df\n",
      "        brand style  rating\n",
      "    0  Yum Yum   cup     4.0\n",
      "    1  Yum Yum   cup     4.0\n",
      "    2  Indomie   cup     3.5\n",
      "    3  Indomie  pack    15.0\n",
      "    4  Indomie  pack     5.0\n",
      "    \n",
      "    By default, for each set of duplicated values, the first occurrence\n",
      "    is set on False and all others on True.\n",
      "    \n",
      "    >>> df.duplicated()\n",
      "    0    False\n",
      "    1     True\n",
      "    2    False\n",
      "    3    False\n",
      "    4    False\n",
      "    dtype: bool\n",
      "    \n",
      "    By using 'last', the last occurrence of each set of duplicated values\n",
      "    is set on False and all others on True.\n",
      "    \n",
      "    >>> df.duplicated(keep='last')\n",
      "    0     True\n",
      "    1    False\n",
      "    2    False\n",
      "    3    False\n",
      "    4    False\n",
      "    dtype: bool\n",
      "    \n",
      "    By setting ``keep`` on False, all duplicates are True.\n",
      "    \n",
      "    >>> df.duplicated(keep=False)\n",
      "    0     True\n",
      "    1     True\n",
      "    2    False\n",
      "    3    False\n",
      "    4    False\n",
      "    dtype: bool\n",
      "    \n",
      "    To find duplicates on specific column(s), use ``subset``.\n",
      "    \n",
      "    >>> df.duplicated(subset=['brand'])\n",
      "    0    False\n",
      "    1     True\n",
      "    2    False\n",
      "    3     True\n",
      "    4     True\n",
      "    dtype: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.duplicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686092f6-607d-4d0e-926d-8ec9f3ace435",
   "metadata": {},
   "source": [
    "## Finding duplicates\n",
    "\n",
    "# A new update to the data pipeline feeding into \"ride_sharing\" has added the \"ride_id\" column, which represents a unique identifier for each ride.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the \"ride_sharing\" DataFrame.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "In this exercise, you will confirm this suspicion by finding those duplicates. A sample of \"ride_sharing\" is in your environment, as well as all the packages you've been working with thus far.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Find duplicated rows of \"ride_id\"  in the \"ride_sharing\" DataFrame while setting \"keep=\" arg to False.\n",
    "    Subset \"ride_sharing\" on duplicates and sort by \"ride_id\" and assign the results to \"duplicated_rides\".\n",
    "    Print the \"ride_id\", \"duration\" and \"user_birth_year\" columns of \"duplicated_rides\" in that order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de25e1-3624-4ec0-bec2-b5aa1a3005fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)\n",
    "\n",
    "\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "\n",
    "print(duplicated_rides[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a2ecb-7a3a-49f8-ab98-8fd1fa5288ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ____.____(____, ____)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[____].____('____')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['____','____','____']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbab60-bff4-4752-a62a-fe4a09ff1493",
   "metadata": {},
   "source": [
    "## Treating duplicates\n",
    "\n",
    "In the last exercise, you were able to verify that the new update feeding into \"ride_sharing\" contains a bug generating both complete and incomplete duplicated rows for some values of the \"ride_id\" column, with occasional discrepant values for the \"user_birth_year\" and \"duration\" columns.\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average duration, and the minimum user_birth_year for each set of incomplete duplicate rows.\n",
    "# *******************************************************************************************************************\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Drop complete duplicates in \"ride_sharing\" and store the results in \"ride_dup\".\n",
    "#    Create the statistics dictionary which holds minimum aggregation for \"user_birth_year\" and mean aggregation for \"duration\".\n",
    "#    Drop incomplete duplicates by grouping by \"ride_id\" and applying the aggregation in statistics.\n",
    "    Find duplicates again and run the assert statement to verify de-duplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973867c5-89d4-478a-abb8-d88a0e0c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_dup = ride_sharing.drop_duplicates()  # No need set \"inplace=\" arg to True as we passing it top a varianble\n",
    "\n",
    "\n",
    "summaries = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(summaries).reset_index()\n",
    "\n",
    "\n",
    "duplicates = ride_unique.duplicated(subset='ride_id', keep=False)\n",
    "\n",
    "duplicates_rides = ride_unique[duplicates]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daf0a4-c4a3-4adb-8c8c-bdf23d1e1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ____.____()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': ____, 'duration': ____}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.____('____').____(____).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.____(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0  ##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa1f01-de87-460c-99a4-13cff9de7c5b",
   "metadata": {},
   "source": [
    "## Membership constraints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Good work on chapter 1.  We are now equipped to treat more complex, and specific data cleaning problems.  In this chapter, we're going to take a look at common data problems with text and categorical data.  So lets get started.  \n",
    "\n",
    "In this lesson, we'll focus on categorical variables.  As discussed early in chapter 1, categorical data represent variables that represent predefined finite set of categories.  Examples of this range from marriage status, household income categories, loan status and others.  \n",
    "\n",
    "# To run machine learning models on categorical data, they are often coded as numbers.  \n",
    "Since categorical data represent a predefined set of categories, they can't have values that go beyond these predefined categories.  We can have inconsistencies in our categorical data for a variety of reasons.  This could be due to data entry issues with free text vs dropdown fields, data parsing errors and other types of errors.  \n",
    "\n",
    "There's a variety of ways we can treat these, with increasingly specific solutions for different types of inconsistencies.  Most simply, we can drop the rows with incorrect categories.  We can attempt remapping incorrect categories to correct ones, and more.  We'll see a variety of ways of dealing with this throughout the chapter and the course, but for now we'll just focus on dropping data.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets first look at an example. Here is a DataFrame named study_data containing a list of first names, birth dates, and blood types.  Additional, a DataFrame named categories, containing the correct possible categories for the bloodtype column has been created as well.  Notice the inconsistency here?  There's definitely no bloodtype named Z+.  Luckily, the categories DataFrame will help us systematically spot all rows with these inconsistencies.  \n",
    "\n",
    "# Its always good practice to keep a log of all possible values of your categorical data, as it will make dealing with these types of inconsistencies way easier.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Now before moving on to dealing with these inconsistent values, lets have a brief reminder on joins.  The two main types of joins we care about here are anti-joins and inner-joins.  We join DataFrames on common columns between them. \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The anti-joins take in 2 DataFrames A and B, and return data from 1 DataFrame that is not contained in another. \n",
    "Imagine a example we performing a left anti-joins of DF A and B, and are returning the columns of DataFrames A and B for values only found in A of the common column between them being joined on.  \n",
    "\n",
    "Inner-joins returns only the data that is contained DataFrames.  For example, an inner-join of A and B would return columns from both DataFrames for values only found in A and B, of the common column between them being joined on.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "In our example, an left anti-join essentially returns all the data in study data with inconsistent bloodtypes, and an inner-join returns all the rows containing consistent bloodtypes.  Now lets see how to do that in Python.  We first gety all inconsistent categories in the blood_type column of the study_data DataFrame.  We do that by creating a set of the blood_type column which stores its unique values, and use the \".difference()\" method which takes in as argument the blood_type column from the categories DataFrame.  This returns all the categories in blood_type that are not in categories.  We then find the inconsistent rows by finding all the rows of the blood_type columns that are equal to inconsistent categories by using the \".isin()\" method, this returns a series of boolean values that are True for inconsistent rows and False for consistent ones.  We then subset the study_data DF based on the boolean indexing, and viola we have our inconsistent data.  \n",
    "\n",
    "To drop inconsistent rows and keep ones that are only consistent.  We just use the tilde symbol (~) while subsetting which rturns everything except inconsistent rows.  \n",
    "\n",
    "\n",
    "Now thatwe know about treating categorical data, lets practice.  \n",
    "\n",
    "\n",
    "\n",
    "# Read study data and print in\n",
    "study_data = pd.read_csv('study.csv')\n",
    "print(stydy_data)\n",
    "\n",
    "-------------------------------------------------\n",
    "    name       | birthday     | bloodtype\n",
    "1   Beth       | 2019-10-20   | B-\n",
    "2   Ignatius   | 2020-07-08   | A-\n",
    "3   Paul       | 2019-08-12   | O+\n",
    "4   Helen      | 2019-03-17   | O-\n",
    "5   Jennifer   | 2019-12-17   | Z+   <---\n",
    "6   Kennedy    | 2020-04-27   | A+\n",
    "7   Keityh     | 2019-04-19   | AB+\n",
    "\n",
    "\n",
    "# Correct possible blood types\n",
    "print(categories)\n",
    "\n",
    "-----------------------\n",
    "    bloodtype\n",
    "1   O-\n",
    "2   O+\n",
    "3   A-\n",
    "4   A+\n",
    "5   B+\n",
    "6   B-\n",
    "7   AB+\n",
    "8   AB-\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "inconsistent_categories = set(study_data['bloodtype']).difference(categories['bloodtype'])  #########################\n",
    "print(inconsistent_categories)\n",
    "\n",
    "{'z'}\n",
    "\n",
    "# Get and print rows with inconsistent categories\n",
    "inconsistens_rows = study_data['bloodtype'].isin(inconsistent_categories)\n",
    "\n",
    "inconsistene_data = study_data[inconsistent_rows]\n",
    "\n",
    "# Drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4afd28-8a3b-41ab-8105-1aa1a61934b8",
   "metadata": {},
   "source": [
    "## Members only\n",
    "\n",
    "Throughout the course so far, you've been exposed to some common problems that you may encounter with your data, from data type constraints, data range constrains, uniqueness constraints, and now membership constraints for categorical values.\n",
    "\n",
    "In this exercise, you will map hypothetical problems to their respective categories.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map the data problem observed with the correct type of data problem.\n",
    "\n",
    "Hint\n",
    "\n",
    "#    Remember, a membership constraint is when a categorical column has values that are not in the predefined set of categories of your column.\n",
    "\n",
    "Other Constraint: \n",
    "     A \"revenue\" column represented as a string\n",
    "     A \"birthdate\" column with value in the future\n",
    "     An \"age\" column with value above \"130\"\n",
    "\n",
    "\n",
    "Membership Constraint:\n",
    "     A \"month\" column with the value \"14\"\n",
    "     A \"day_of_week\" column with the value \"Suntermonday\"\n",
    "     A \"GPA\" column containing a \"z-\" grade\n",
    "     A \"has_loan\" column with the value \"12\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28556e-d74e-41e3-8c6b-62f9d05f4478",
   "metadata": {},
   "source": [
    "## Finding consistency\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this exercise and throughout this chapter, you'll be working with the airlines DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named categories was created, containing all correct possible values for the survey columns.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise. The pandas package has been imported as pd, and the airlines and categories DataFrames are in your environment.\n",
    "Instructions 1/4\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "#    Print the categories DataFrame and take a close look at all possible correct categories of the survey columns.\n",
    "#    Print the unique values of the survey columns in airlines using the \".unique()\" method.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Print the unique values of the survey columns in airlines using the .unique() method.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Find rows of airlines with a cleanliness value not in categories and print the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d166ab-1eb1-4535-a087-7e5489cbf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_val_lis = {'col_key1': ['val_11', 'val_12', 'val_13', 'val_14'],\n",
    "                'col_key2': ['val_21', 'val_22', 'val_23', 'val_24'],\n",
    "                'col_key3': ['val_31', 'val_32', 'val_33', 'val_34']}\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "list_dict = [{'country': 'Brazil', 'capital': 'Brasilia', 'area': 8.516, 'population': 200.4}, \n",
    "             {'country':'Russia', 'capital':'Moscow', 'area':17.10, 'population':143.5}, \n",
    "             {'country':'India', 'capital':'New Delhi', 'area':3.286, 'population':1252}, \n",
    "             {'country':'China', 'capital':'Beijing', 'area':9.597, 'population':1357}, \n",
    "             {'country':'South Africa','capital':'Pretoria','area':1.221, 'population':52.98}]\n",
    "\n",
    "import pandas as pd\n",
    "putcome = pd.DataFrame(list_dict)\n",
    "print(putcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7e8e25d-7700-4e54-b309-99f3250c0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        East US       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  \n",
      "0      Very satisfied  \n",
      "1      Very satisfied  \n",
      "2             Neutral  \n",
      "3  Somewhat satsified  \n",
      "4  Somewhat satsified   \n",
      "\n",
      "          id       day        airline destination dest_region dest_size  \\\n",
      "2805  2222.0  Thursday      SOUTHWEST     PHOENIX     West US       Hub   \n",
      "2806  2684.0    Friday         UNITED     ORLANDO     East US       Hub   \n",
      "2807  2549.0   Tuesday        JETBLUE  LONG BEACH     West US     Small   \n",
      "2808  2162.0  Saturday  CHINA EASTERN     QINGDAO        Asia     Large   \n",
      "0        NaN       NaN            NaN         NaN         NaN       NaN   \n",
      "\n",
      "     boarding_area   dept_time  wait_min cleanliness         safety  \\\n",
      "2805   Gates 20-39  2018-12-31     165.0       Clean      Very safe   \n",
      "2806   Gates 70-90  2018-12-31      92.0       Clean      Very safe   \n",
      "2807    Gates 1-12  2018-12-31      95.0       Clean  Somewhat safe   \n",
      "2808    Gates 1-12  2018-12-31     220.0       Clean      Very safe   \n",
      "0              NaN         NaN       NaN        cool   good quality   \n",
      "\n",
      "            satisfaction  \n",
      "2805      Very satisfied  \n",
      "2806      Very satisfied  \n",
      "2807      Very satisfied  \n",
      "2808  Somewhat satsified  \n",
      "0              best ever   \n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty' 'cool']\n",
      "['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'\n",
      " 'good quality']\n",
      "['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied' 'best ever'] \n",
      "\n",
      "      cleanliness           safety          satisfaction\n",
      "0           Clean        Very safe        Very satisfied\n",
      "1  Somewhat clean    Somewhat safe    Somewhat satsified\n",
      "2         Average          Neutral               Neutral\n",
      "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
      "4           Dirty  Somewhat unsafe      Very unsatisfied \n",
      "\n",
      "{'cool'}\n",
      "2804    False\n",
      "2805    False\n",
      "2806    False\n",
      "2807    False\n",
      "2808    False\n",
      "0        True\n",
      "Name: cleanliness, dtype: bool \n",
      "\n",
      "          id       day        airline   destination dest_region dest_size  \\\n",
      "2804  1475.0   Tuesday         ALASKA  NEW YORK-JFK     East US       Hub   \n",
      "2805  2222.0  Thursday      SOUTHWEST       PHOENIX     West US       Hub   \n",
      "2806  2684.0    Friday         UNITED       ORLANDO     East US       Hub   \n",
      "2807  2549.0   Tuesday        JETBLUE    LONG BEACH     West US     Small   \n",
      "2808  2162.0  Saturday  CHINA EASTERN       QINGDAO        Asia     Large   \n",
      "\n",
      "     boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "2804   Gates 50-59  2018-12-31     280.0  Somewhat clean        Neutral   \n",
      "2805   Gates 20-39  2018-12-31     165.0           Clean      Very safe   \n",
      "2806   Gates 70-90  2018-12-31      92.0           Clean      Very safe   \n",
      "2807    Gates 1-12  2018-12-31      95.0           Clean  Somewhat safe   \n",
      "2808    Gates 1-12  2018-12-31     220.0           Clean      Very safe   \n",
      "\n",
      "            satisfaction  \n",
      "2804  Somewhat satsified  \n",
      "2805      Very satisfied  \n",
      "2806      Very satisfied  \n",
      "2807      Very satisfied  \n",
      "2808  Somewhat satsified  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv', index_col=0)\n",
    "print(airlines.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "dirty_airlines = pd.DataFrame({'cleanliness': ['cool'], 'safety': ['good quality'], 'satisfaction': ['best ever']})\n",
    "\n",
    "airlines = pd.concat([airlines, dirty_airlines])#, ignore_index=True)\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "print(airlines.tail(), \"\\n\")\n",
    "\n",
    "\n",
    "print(type(airlines['cleanliness'].unique()))\n",
    "print(airlines['cleanliness'].unique())\n",
    "print(airlines['safety'].unique())\n",
    "print(airlines['satisfaction'].unique(), \"\\n\")\n",
    "\n",
    "\n",
    "categories = pd.DataFrame({'cleanliness': ['Clean', 'Somewhat clean', 'Average', 'Somewhat dirty', 'Dirty'], \n",
    "                          'safety': ['Very safe', 'Somewhat safe', 'Neutral', 'Very unsafe', 'Somewhat unsafe'], \n",
    "                          'satisfaction': ['Very satisfied', 'Somewhat satsified', 'Neutral', 'Somewhat unsatisfied', 'Very unsatisfied']})\n",
    "\n",
    "print(categories, '\\n')\n",
    "\n",
    "#print(airlines[['cleanliness', 'safety', 'satisfaction']].unique())\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# So lets image we'll drop every row when a column has inconsistent value\n",
    "# *******************************************************************************************************************\n",
    "bad_cleanliness = set(airlines['cleanliness']).difference(categories['cleanliness'])  ###############################\n",
    "print(bad_cleanliness)\n",
    "\n",
    "inconsistens_cleanliness_rows = airlines['cleanliness'].isin(bad_cleanliness)\n",
    "\n",
    "print(inconsistens_cleanliness_rows[-6:], '\\n')\n",
    "\n",
    "good_cleanliness = airlines[~inconsistens_cleanliness_rows]\n",
    "print(good_cleanliness.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8799349-f287-42eb-9709-6419ba8ff593",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Find the cleanliness category in airlines not in categories\n",
    "    cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "    # Find rows with that category\n",
    "    cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "    # Print rows with inconsistent category\n",
    "    print(airlines[cat_clean_rows])\n",
    "\n",
    "    -> airlines의 cleanliness 컬럼의 중복을 제거한 리스트로 만들어, 차집합을 통해 겹치지 않는 값을\n",
    "    cat_clean에 저장한다. 이때 겹치는 값을 제외한 Unacceptable만 저장된다.\n",
    "\n",
    "    -> 그리고 cleanliness 컬럼의 값이 cat_clean에 포함되어있는지를 불리언의 형태로\n",
    "    cat_clean_rows에 저장하였다.\n",
    "\n",
    "    -> 마지막으로 이 값들이 포함되어있는 행들을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f99ad-2e03-44ee-9acf-22e63ee6f779",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Awesome work on the last lesson.  Now lets discuss other types of problems that could affect categorical variables.  In the last lesson, we saw how categorical data has a value membership constraint, where columns need to have a predefined set of values.  However this is not the only set of problems we may encounter.  \n",
    "\n",
    "When cleaning categorical data, some of the problems we may encounter include value inconsistency, the presence of too many categories that could be collapsed into one, and making sure data is of the right type.  Lets start with making sure our categorical data is consistent.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# A common categorical data problem is having values that slightly differ because of capitalizations.  \n",
    "Not treating this could lead tyo misleading results when we dicide to analyze the data, for example, lets assume we're working with a demographics dataset, and we have a marriage status column with inconsistent capitalization.  Below is what counting the number of married people in the marriage_status Series would look like.  \n",
    "\n",
    "Note that \".value_counts()\" method works on Series only.  For a DF, we can \"df.groupby()\" the column and use the \".count()\" method.  \n",
    "\n",
    "To deal with this, we can either capitalize or lowercase the marriage_status column.  This can be done with the \"str.upper()\" or \"str.lower()\" function respectively.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Another common problem with categorical values are leading or trailing spaces.  \n",
    "For example, imagine the same demographics DF containing values with leading spaces.  Below is what the counts of married vs unmarried people would look like.  Note that there is a married category with a trailing space on the right, which makes it hard to sport on the output, as opposed to unmarried.  \n",
    "\n",
    "To remove leading and trailing spaces, we can use the \"str.strip()\" method, which when giving no input, strips all leading and trailing white spaces.  \n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Sometimes, we may want to create categories out of our data, such as creating household income groups from income data.  \n",
    "To create categories out of data, lets use the example of creating an income group column in the demographics DataFrame.  (do you remember how we did this using \"pd.cut()\" & \"pd.qcut()\")  \n",
    "\n",
    "# \"pd.qcut(df['column'], q=, labels=)\"\n",
    "We can do this in 2 ways.  The first method utilizes the \"pd.qcut()\" function from Pandas, which automatically divides our data based on its distribution into the number of categories we set in the \"q=\" argument, we created the category names in the group_names list and fed it to \"labels=\" argument, returning the following.  \n",
    "\n",
    "Notice the first row actually misrepresents the actual income of the income group, as we didn't instruct qcut where our ranges actually lie.  \n",
    "\n",
    "# \"pd.cut(df['column'], bins=, labels=)\"\n",
    "We can do this with \"pd.cut()\" function instead, which lets us define category cutoff ranges with the bins argument.  It takes in a list of cutoff points for each category, with the final one being infinity represented with \"np.inf()\" from NumPy.  From the output, we can we can see that this is much more correct.  \n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Sometimes, we may want to reduce the amount of categories we have in our data.  Lets move on to mapping categories to fewer ones.  \n",
    "\n",
    "For example, assume we have a column containing the operating system of different devices, and contains these unique values: Microsoft, MacOS, IOS, Android, Linux.  Say we want to collapse these categories into 2, DesktopOS, and MobileOS.  \n",
    "\n",
    "# We can do this by using the \".replace()\" method.  \n",
    "It takes in a dictionary that maps each existing category to the category name you desire.  In this case, this is a mapping dictionary.  A quick print of the unique values of operating system shows the mapping has been complete.  \n",
    "\n",
    "\n",
    "Now that we know about treating categorical data, lets practice.  \n",
    "\n",
    "\n",
    "\n",
    " (1) Value inconsistency\n",
    "     Inconsistent fields: 'married', 'Maried', 'UNMARRIED', 'not married' ...\n",
    "     Trailing white spaces: 'married', ' married '\n",
    "(11) Collapsing too many categories to few\n",
    "     Creating new groups: \"0-20k\", \"20-40k\" categories from continuous household income data\n",
    "     Mapping groups to new ones: Mapping household income categories to \"rich\", \"poor\"\n",
    "\n",
    "\n",
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "\n",
    "marriage_status.value_counts()\n",
    "\n",
    "unmarried   352\n",
    "married     268\n",
    "MARRIED     204\n",
    "UNMARRIED   176\n",
    "dtype: int64\n",
    "\n",
    "\n",
    " unmarried   352\n",
    "married      268\n",
    "MARRIED      204\n",
    "UNMARRIED    176\n",
    "dtype: int64\n",
    "\n",
    "\n",
    "\n",
    "# Using  \"pd.qcut()\"\n",
    "group_names = ['0-200k', '200-500k', '500k+']\n",
    "\n",
    "demographics[\"income_group\"] = pd.qcut(demographics['household_income'], q=3, labels=group_names)\n",
    "\n",
    "# Print income_group column\n",
    "demographics[['income_group', 'household_income']]\n",
    "\n",
    "-----------------------------------------\n",
    "    income_group  | household_income\n",
    "    200-500k      | 189243\n",
    "    500k+         | 778533\n",
    "    \n",
    "\n",
    "\n",
    "# Using \"pd.cut()\" - creat category ranges and names\n",
    "ranges = [0, 200000, 500000, np.inf]  ###############################################################################\n",
    "\n",
    "group_names = ['0-200k', '200-500k', '500k+']\n",
    "\n",
    "# Create income group column\n",
    "demographics['income_group'] = pd.cut(demographics['household_income'], bins=ranges, labels=group_names)\n",
    "\n",
    "-----------------------------------------\n",
    "    income_group  | household_income\n",
    "    0-200k        | 189243\n",
    "    500k+         | 778533\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a mapping dictionary and replace\n",
    "mapping = {\"Microsoft\": \"DesktopOS\", \"MacOS\": \"DesktopOS\", \"Linux\": \"DesktopOS\", \"IOS\": \"MobileOS\", \"Android\": \"MobileOS\"}\n",
    "\n",
    "devices[\"operating_system\"] = devices[\"operating_system\"].replace(mapping) ##########################################\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "devices[\"operating_system\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bcecf-9e21-4829-94ff-14e12af6a85c",
   "metadata": {},
   "source": [
    "## Categories of errors\n",
    "\n",
    "In the video exercise, you saw how to address common problems affecting categorical variables in your data, including white spaces and inconsistencies in your categories, and the problem of creating new categories and mapping existing ones to new ones.\n",
    "\n",
    "To get a better idea of the toolkit at your disposal, you will be mapping functions and methods from pandas and Python used to address each type of problem.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map each function/method to the categorical data problem it solves.\n",
    "\n",
    "White spaces and inconsistency:\n",
    "     .str.upper()\n",
    "     .str.lower()\n",
    "     .str.strip()\n",
    "\n",
    "Creating or remapping categories:\n",
    "     pd.cut()\n",
    "     .replace()\n",
    "     pd.qcut()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee7ece-abbd-48cb-a60f-c28674df7348",
   "metadata": {},
   "source": [
    "##  Inconsistent categories\n",
    "\n",
    "In this exercise, you'll be revisiting the airlines DataFrame from the previous lesson.\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport.\n",
    "\n",
    "In this exercise, you will examine two categorical columns from this DataFrame, dest_region and dest_size respectively, assess how to address them and make sure that they are cleaned and ready for analysis. The pandas package has been imported as pd, and the airlines DataFrame is in your environment.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the unique values in \"dest_region\" and \"dest_size\" respectively.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    From looking at the output, what do you think is the problem with these columns?\n",
    "    Possible Answers\n",
    "    The \"dest_region\" column has only inconsistent values due to capitalization.\n",
    "#    The \"dest_region\" column has inconsistent values due to capitalization and has one value that needs to be remapped.    yes, the 'euro' should be mapped to 'Europe'\n",
    "    The dest_size column has only inconsistent values due to leading and trailing spaces.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Change the capitalization of all values of \"dest_region\" to lowercase.\n",
    "#    Replace the 'eur' with 'europe' in dest_region using the \".replace()\" method.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "#    Strip white spaces from the \"dest_size\" column using the \".strip()\" method.\n",
    "#    Verify that the changes have been into effect by printing the unique values of the columns using .unique() .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1a7758-e6a6-4ce1-8f43-63364abd6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    id        day      airline        destination    dest_region  \\\n",
      "0           0  1351    Tuesday  UNITED INTL             KANSAI           Asia   \n",
      "1           1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
      "2           2  2820   Thursday        DELTA        LOS ANGELES        West US   \n",
      "3           3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US   \n",
      "4           4  2992  Wednesday     AMERICAN              MIAMI        East US   \n",
      "\n",
      "  dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
      "0       Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
      "1     Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
      "2       Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
      "3       Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
      "4       Hub   Gates 50-59  2018-12-31     559.0  Somewhat clean   \n",
      "\n",
      "          safety        satisfaction  \n",
      "0        Neutral      Very satisfied  \n",
      "1      Very safe      Very satisfied  \n",
      "2  Somewhat safe             Neutral  \n",
      "3      Very safe  Somewhat satsified  \n",
      "4      Very safe  Somewhat satsified   \n",
      "\n",
      "['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
      " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
      " 'Australia/New Zealand' 'middle east'] \n",
      "\n",
      "['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
      " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     '] \n",
      "\n",
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand'] \n",
      "\n",
      "['Hub', 'Small', 'Medium', 'Large']\n",
      "Categories (4, object): ['Hub', 'Large', 'Medium', 'Small'] \n",
      "\n",
      "category\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "airlines = pd.read_csv('airlines_final.csv')\n",
    "print(airlines.head(), '\\n')\n",
    "\n",
    "\n",
    "print(airlines['dest_region'].unique(), '\\n')\n",
    "\n",
    "print(airlines['dest_size'].unique(), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "\n",
    "\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur': 'europe'})  #######################################\n",
    "\n",
    "\n",
    "print(airlines['dest_region'].unique(), '\\n')\n",
    "\n",
    "\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()  ##########################################################\n",
    "\n",
    "airlines['dest_size'] = airlines['dest_size'].astype('category')  ###################################################\n",
    "\n",
    "print(airlines['dest_size'].unique(), '\\n')\n",
    "print(airlines['dest_size'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24f943-27c0-40ef-aed3-12d366c1f90f",
   "metadata": {},
   "source": [
    "## Remapping categories\n",
    "\n",
    "To better understand survey respondents from airlines, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The airlines DataFrame contains the \"day\" and \"wait_min\" columns, which are categorical and numerical respectively. The day column contains the exact day a flight took place, and wait_min contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "\n",
    "    wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+\n",
    "    day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend.\n",
    "\n",
    "The pandas and numpy packages have been imported as pd and np. Let's create some new categorical data!\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create the ranges and labels for the \"wait_type\" column mentioned in the description.\n",
    "#    Create the \"wait_type\" column by from wait_min by using \"pd.cut()\", while inputting \"label_ranges\" and \"label_names\" in the correct arguments.\n",
    "#    Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.\n",
    "#    Create the day_week column by using .replace().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a3b33e9-e9d2-4109-bc9f-096205e7e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    id        day      airline        destination    dest_region  \\\n",
      "0           0  1351    Tuesday  UNITED INTL             KANSAI           asia   \n",
      "1           1   373     Friday       ALASKA  SAN JOSE DEL CABO  canada/mexico   \n",
      "2           2  2820   Thursday        DELTA        LOS ANGELES        west us   \n",
      "3           3  1157    Tuesday    SOUTHWEST        LOS ANGELES        west us   \n",
      "4           4  2992  Wednesday     AMERICAN              MIAMI        east us   \n",
      "\n",
      "  dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
      "0       Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
      "1     Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
      "2       Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
      "3       Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
      "4       Hub   Gates 50-59  2018-12-31     559.0  Somewhat clean   \n",
      "\n",
      "          safety        satisfaction wait_type day_week  \n",
      "0        Neutral      Very satisfied    medium  weekday  \n",
      "1      Very safe      Very satisfied    medium  weekday  \n",
      "2  Somewhat safe             Neutral    medium  weekday  \n",
      "3      Very safe  Somewhat satsified      long  weekday  \n",
      "4      Very safe  Somewhat satsified      long  weekday   \n",
      "\n",
      "category \n",
      "\n",
      "['Tuesday', 'Friday', 'Thursday', 'Wednesday', 'Saturday', 'Sunday', 'Monday']\n",
      "Categories (7, object): ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday'] \n",
      "\n",
      "category \n",
      "\n",
      "['weekday', 'weekend']\n",
      "Categories (2, object): ['weekend', 'weekday'] \n",
      "\n",
      "['medium', 'long', 'short']\n",
      "Categories (3, object): ['short' < 'medium' < 'long'] \n",
      "\n",
      "      Unnamed: 0    id        day              airline     destination  \\\n",
      "1223        1399  2307   Thursday             AMERICAN  CHICAGO-O'HARE   \n",
      "1126        1286  2145  Wednesday           AIR CANADA       VANCOUVER   \n",
      "1637        1867  3069     Sunday  PHILIPPINE AIRLINES          MANILA   \n",
      "1833        2093  2778  Wednesday               UNITED          AUSTIN   \n",
      "559          643  2151   Thursday       CATHAY PACIFIC       HONG KONG   \n",
      "104          117  1563  Wednesday                DELTA         ATLANTA   \n",
      "554          638  2540   Saturday              JETBLUE      LONG BEACH   \n",
      "\n",
      "        dest_region dest_size boarding_area   dept_time  wait_min  \\\n",
      "1223     midwest us       Hub   Gates 50-59  2018-12-31     150.0   \n",
      "1126  canada/mexico    Medium  Gates 91-102  2018-12-31     125.0   \n",
      "1637           asia    Medium    Gates 1-12  2018-01-01     305.0   \n",
      "1833     midwest us    Medium   Gates 70-90  2018-12-31     150.0   \n",
      "559            asia       Hub    Gates 1-12  2018-12-31     155.0   \n",
      "104         east us       Hub   Gates 40-48  2018-12-31     135.0   \n",
      "554         west us     Small    Gates 1-12  2018-12-31      63.0   \n",
      "\n",
      "         cleanliness         safety        satisfaction wait_type day_week  \n",
      "1223  Somewhat clean  Somewhat safe  Somewhat satsified    medium  weekday  \n",
      "1126           Clean      Very safe  Somewhat satsified    medium  weekday  \n",
      "1637  Somewhat clean  Somewhat safe  Somewhat satsified      long  weekend  \n",
      "1833  Somewhat clean        Neutral  Somewhat satsified    medium  weekday  \n",
      "559            Clean      Very safe      Very satisfied    medium  weekday  \n",
      "104   Somewhat clean  Somewhat safe             Neutral    medium  weekday  \n",
      "554   Somewhat clean      Very safe  Somewhat satsified    medium  weekend  \n"
     ]
    }
   ],
   "source": [
    "print(airlines.head(), '\\n')\n",
    "\n",
    "\n",
    "print(airlines['day'].dtypes, '\\n')\n",
    "\n",
    "print(airlines['day'].unique(), '\\n')\n",
    "\n",
    "airlines['day'] = airlines['day'].astype('category')\n",
    "print(airlines['day'].dtypes, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "wait_ranges = [0, 60, 180, np.inf]\n",
    "wait_labels = ['short', 'medium', 'long']\n",
    "\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins=wait_ranges, labels=wait_labels)\n",
    "\n",
    "\n",
    "day_mapping = {'Tuesday':'weekday', 'Friday':'weekday', 'Thursday':'weekday',  'Wednesday':'weekday',\n",
    "               'Saturday':'weekend', 'Sunday':'weekend', 'Monday':'weekday'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(day_mapping)  ########################################################\n",
    "\n",
    "\n",
    "print(airlines['day_week'].unique(), '\\n')\n",
    "print(airlines['wait_type'].unique(), '\\n')\n",
    "\n",
    "print(airlines.sample(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2a496-58d1-4a3e-97b4-70c8635ab3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, ____, np.inf]\n",
    "label_names = ['short', ____, ____]\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.____(____, bins = ____, \n",
    "                                labels = ____)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'____', 'Wednesday': '____', \n",
    "            'Thursday': '____', '____': '____', \n",
    "            'Saturday': 'weekend', '____': '____'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].____(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d526058-b3e2-4c3c-8a9e-0baa214a71ee",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Good job on the previous lesson.  In the final lesson of this chapter, we'll talk about text data and regular expressions.  Text data is on of the most common types of data types.  Examples of it range from names, phone numbers, addresses, emails and more.  \n",
    "\n",
    "Common text data problems include handling inconsistencies, making sure text data is of a certain length, typos and others.  Lets take a look at the following example.  Below is a DF named phones containing the full name and phone numbers of individuals.  Both are string columns.  \n",
    "\n",
    "Notice the phone number column.  We can see that there are phone number values, that begin with 00 or +.  We also see  that there are one of entry where the phone number is 4 digits, which is non-existent.  Furthermore, we can see that there are dashes across the phone number column.  \n",
    "\n",
    "If we wanted to feed these phone numbers into an automated call system, or create a report discussing the distribution of users by area code, we couldn't really do so without uniform phone numbers.  \n",
    "\n",
    "Ideally, we'd want to the phone number column s such.  Where all phone numbers are aligned to begin with 00, where any number below the 10 digit value is replaced with NaN to represent a missing value, and where all dashes have been removed.  Lets see how thats done.  \n",
    "\n",
    "# \".str.replace()\" on string data and \".str.len()\" to subset & indexi damaged data to fill as NaN\n",
    "# *******************************************************************************************************************\n",
    "Lets first begin by replacing the plus sign with 00, to do this, we use the \".str.replace()\" method with takes in 2 values, the string being replaced, which is in this case the plus sign and the string to replace it with which is is this case 00.  We can see that the column has been updated.  We use the same exact technique to dashes, by replacing the dash symbol with an empty string.  Now finally, we're going to replace all phone numbers below 10 digits to NaN.  We can do this by chaining the Phone number column with the \".str.len()\", which returns the string length of each row in the column.  We can he use the \".loc[]\" method, to index rows where digits is below 10, and replace the value of Phone number with NumPy's nan object.  \n",
    "\n",
    "We can also write assert statement top test whether the Phone number column has a specific length, and whether it contains the symbols we removed.  The first assert statement tests that the minimum length of the strings in the Phone number column, found through str.len(), is bigger than or equal to 10.  In the second assert statement, we use the \"str.contains()\" methpd to test wheather the Phone number column contains a specific pattern.  It returns a series of booleans that are Ture for matches and False for non-matches.  We set the pattern \"+|-\", the bar (|) pipe here is basically an or statement, so we're trying to find matches for either symbols.  We chain it with the \"any()\" method which returns True if any element in the output of our \".str.contains()\" is True, and test whether it returns False.  \n",
    "\n",
    "# Regular Expressions gives us the ability to search for any pattern in text data\n",
    "But what about more complicated examples?  How can we clean a Phone number column that looks like below fopr example?  Where Phone numbers can contain a range of symbols from plus signs, dashes, parenthesis and maybe more.  This is where Regular Expressions come in.  Regular Expressions gives us the ability to search for any pattern in text data, like only digits for example.  They are likely control + find in your browser, but way more dynamic and robust.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lets take a look at this example.  Here we are attempting to only extract digits from the Phone number column.  To do this, we use the \".str.replace()\" method with the pattern we want to replace with an empty string.  Notice the pattern fed into the method.  This is essentially us telling Pandas top replace anything that is not digt with nothing.  \n",
    "\n",
    "We won't get into the specifics of Regular Expressions, and how to construct them, but they are immensely useful for difficult string cleaning tasks, so make sure to check out DataCamp's course library on Regular Expressions.  \n",
    "\n",
    "Now that we know how to clean text data, lets get to practice.  \n",
    "\n",
    "\n",
    "\n",
    " (1) Data inconsistency\n",
    "     \"+96171679912\" or \"0096171679912\"\n",
    " (2) Fixed length violations:\n",
    "     Passwords needs to be at least 8 characters\n",
    " (3) Typos:\n",
    "     \"+961.71.679912\"\n",
    "\n",
    "\n",
    "phones = pd.read_csv('phones.csv')\n",
    "print(phones)\n",
    "\n",
    "--------------------------------------------\n",
    "            Full name  |       Phone number\n",
    "      Noelani A. Gray  |   001-702-397-5143\n",
    "        Myles Z. Gomez |   001-329-485-0540\n",
    "          Gil B. Silva |   001-195-492-2338\n",
    "    Prescott D. Hardin |    +1-297-996-4904\n",
    "    Benedict G. Valdaz |   001-969-820-3536\n",
    "      Reece M. Andrews |               4138\n",
    "        Harfa E. Keith |   001-536-175-8444\n",
    "\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('+', '00')  #############################################\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('-', '')\n",
    "\n",
    "phones['Phone number'] = phones.loc[len(phones['Phone number'])<10, 'Phone number'] = 'Nan'  ##### Take a try and see\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "phones.loc[phones['Phone number'].str.len()<10, 'Phone number'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "assert phones['Phone number'].str.len.min() < 10\n",
    "\n",
    "\n",
    "# Assert all numbers do not have \"+\" or \"-\" symbol\n",
    "assert phpnes['Phone number'].str.contains('+|-').any() == False  ###################################################\n",
    "\n",
    "--------------------------------------------\n",
    "            Full name  |       Phone number\n",
    "      Noelani A. Gray  |      0017023975143\n",
    "        Myles Z. Gomez |      0013294850540\n",
    "          Gil B. Silva |      0011954922338\n",
    "    Prescott D. Hardin |      0012979964904\n",
    "    Benedict G. Valdaz |      0019698203536\n",
    "      Reece M. Andrews |                NaN\n",
    "        Harfa E. Keith |      0015361758444\n",
    "        \n",
    "\n",
    "--------------------------------------------\n",
    "            Full name  |       Phone number\n",
    "      Noelani A. Gray  |     +(01706)-25891\n",
    "        Myles Z. Gomez |       +0500-571437\n",
    "          Gil B. Silva |         +0800-1111\n",
    "    Prescott D. Hardin |      +07058-879063\n",
    "    Benedict G. Valdaz |     +(016799)-8424\n",
    "\n",
    "\n",
    "# Replace letters with nothing\n",
    "phones['Phone number'] = phones['Phone number'].str.replace(r'\\D+', '')  ############################################\n",
    "\n",
    "--------------------------------------------\n",
    "            Full name  |       Phone number\n",
    "      Noelani A. Gray  |         0170625891\n",
    "        Myles Z. Gomez |         0500571437\n",
    "          Gil B. Silva |           08001111\n",
    "    Prescott D. Hardin |        07058879063\n",
    "    Benedict G. Valdaz |         0167998424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75e7cd2b-ca7b-423d-9d52-e42c741e8e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Full name      Phone number\n",
      "0     Noelani A. Gray  001-702-397-5143\n",
      "1      Myles Z. Gomez  001-329-485-0540\n",
      "2        Gil B. Silva  001-195-492-2338\n",
      "3  Prescott D. Hardin   +1-297-996-4904\n",
      "4  Benedict G. Valdaz  001-969-820-3536 \n",
      "\n",
      "object \n",
      "\n",
      "            Full name   Phone number  Phone\n",
      "0     Noelani A. Gray  0017023975143  False\n",
      "1      Myles Z. Gomez  0013294850540  False\n",
      "2        Gil B. Silva  0011954922338  False\n",
      "3  Prescott D. Hardin    12979964904  False\n",
      "4  Benedict G. Valdaz  0019698203536  False\n",
      "5    Reece M. Andrews           4138  False\n",
      "6      Harfa E. Keith  0015361758444  False \n",
      "\n",
      "object\n",
      "            Full name   Phone number  Phone\n",
      "0     Noelani A. Gray  0017023975143  False\n",
      "1      Myles Z. Gomez  0013294850540  False\n",
      "2        Gil B. Silva  0011954922338  False\n",
      "3  Prescott D. Hardin    12979964904  False\n",
      "4  Benedict G. Valdaz  0019698203536  False\n",
      "5    Reece M. Andrews            NaN  False\n",
      "6      Harfa E. Keith  0015361758444  False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "phones = pd.read_csv('phones.csv')\n",
    "print(phones.head(), '\\n')\n",
    "\n",
    "print(phones['Phone number'].dtypes, '\\n')\n",
    "\n",
    "\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('-', '', regex=True)\n",
    "phones['Phone number'] = phones['Phone number'].str.replace('+', '', regex=True)\n",
    "# FutureWarning: The default value of regex will change from True to False in a future version. \n",
    "# In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
    "\n",
    "\n",
    "phones['Phone'] = phones['Phone number'].str.contains('+', regex=False)#.any()  #----------------------------------\n",
    "print(phones, '\\n')\n",
    "# Return boolean Series or Index based on whether a given pattern or regex is\n",
    "# contained within a string of a Series or Index.\n",
    "\n",
    "print(phones['Phone number'].dtypes)\n",
    "\n",
    "\n",
    "phones.loc[phones['Phone number'].str.len()<10, 'Phone number'] = np.nan  #------------------------------------------\n",
    "print(phones, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b68014c2-68eb-4660-b479-9d36855a0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__frozen', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_data', '_doc_args', '_freeze', '_get_series_list', '_index', '_inferred_dtype', '_is_categorical', '_is_string', '_name', '_orig', '_parent', '_validate', '_wrap_result', 'capitalize', 'casefold', 'cat', 'center', 'contains', 'count', 'decode', 'encode', 'endswith', 'extract', 'extractall', 'find', 'findall', 'fullmatch', 'get', 'get_dummies', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'islower', 'isnumeric', 'isspace', 'istitle', 'isupper', 'join', 'len', 'ljust', 'lower', 'lstrip', 'match', 'normalize', 'pad', 'partition', 'removeprefix', 'removesuffix', 'repeat', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'slice', 'slice_replace', 'split', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'wrap', 'zfill']\n",
      "Help on method contains in module pandas.core.strings.accessor:\n",
      "\n",
      "contains(pat, case=True, flags=0, na=None, regex=True) method of pandas.core.strings.accessor.StringMethods instance\n",
      "    Test if pattern or regex is contained within a string of a Series or Index.\n",
      "    \n",
      "    Return boolean Series or Index based on whether a given pattern or regex is\n",
      "    contained within a string of a Series or Index.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    pat : str\n",
      "        Character sequence or regular expression.\n",
      "    case : bool, default True\n",
      "        If True, case sensitive.\n",
      "    flags : int, default 0 (no flags)\n",
      "        Flags to pass through to the re module, e.g. re.IGNORECASE.\n",
      "    na : scalar, optional\n",
      "        Fill value for missing values. The default depends on dtype of the\n",
      "        array. For object-dtype, ``numpy.nan`` is used. For ``StringDtype``,\n",
      "        ``pandas.NA`` is used.\n",
      "    regex : bool, default True\n",
      "        If True, assumes the pat is a regular expression.\n",
      "    \n",
      "        If False, treats the pat as a literal string.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series or Index of boolean values\n",
      "        A Series or Index of boolean values indicating whether the\n",
      "        given pattern is contained within the string of each element\n",
      "        of the Series or Index.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    match : Analogous, but stricter, relying on re.match instead of re.search.\n",
      "    Series.str.startswith : Test if the start of each string element matches a\n",
      "        pattern.\n",
      "    Series.str.endswith : Same as startswith, but tests the end of string.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Returning a Series of booleans using only a literal pattern.\n",
      "    \n",
      "    >>> s1 = pd.Series(['Mouse', 'dog', 'house and parrot', '23', np.NaN])\n",
      "    >>> s1.str.contains('og', regex=False)\n",
      "    0    False\n",
      "    1     True\n",
      "    2    False\n",
      "    3    False\n",
      "    4      NaN\n",
      "    dtype: object\n",
      "    \n",
      "    Returning an Index of booleans using only a literal pattern.\n",
      "    \n",
      "    >>> ind = pd.Index(['Mouse', 'dog', 'house and parrot', '23.0', np.NaN])\n",
      "    >>> ind.str.contains('23', regex=False)\n",
      "    Index([False, False, False, True, nan], dtype='object')\n",
      "    \n",
      "    Specifying case sensitivity using `case`.\n",
      "    \n",
      "    >>> s1.str.contains('oG', case=True, regex=True)\n",
      "    0    False\n",
      "    1    False\n",
      "    2    False\n",
      "    3    False\n",
      "    4      NaN\n",
      "    dtype: object\n",
      "    \n",
      "    Specifying `na` to be `False` instead of `NaN` replaces NaN values\n",
      "    with `False`. If Series or Index does not contain NaN values\n",
      "    the resultant dtype will be `bool`, otherwise, an `object` dtype.\n",
      "    \n",
      "    >>> s1.str.contains('og', na=False, regex=True)\n",
      "    0    False\n",
      "    1     True\n",
      "    2    False\n",
      "    3    False\n",
      "    4    False\n",
      "    dtype: bool\n",
      "    \n",
      "    Returning 'house' or 'dog' when either expression occurs in a string.\n",
      "    \n",
      "    >>> s1.str.contains('house|dog', regex=True)\n",
      "    0    False\n",
      "    1     True\n",
      "    2     True\n",
      "    3    False\n",
      "    4      NaN\n",
      "    dtype: object\n",
      "    \n",
      "    Ignoring case sensitivity using `flags` with regex.\n",
      "    \n",
      "    >>> import re\n",
      "    >>> s1.str.contains('PARROT', flags=re.IGNORECASE, regex=True)\n",
      "    0    False\n",
      "    1    False\n",
      "    2     True\n",
      "    3    False\n",
      "    4      NaN\n",
      "    dtype: object\n",
      "    \n",
      "    Returning any digit using regular expression.\n",
      "    \n",
      "    >>> s1.str.contains('\\\\d', regex=True)\n",
      "    0    False\n",
      "    1    False\n",
      "    2    False\n",
      "    3     True\n",
      "    4      NaN\n",
      "    dtype: object\n",
      "    \n",
      "    Ensure `pat` is a not a literal pattern when `regex` is set to True.\n",
      "    Note in the following example one might expect only `s2[1]` and `s2[3]` to\n",
      "    return `True`. However, '.0' as a regex matches any character\n",
      "    followed by a 0.\n",
      "    \n",
      "    >>> s2 = pd.Series(['40', '40.0', '41', '41.0', '35'])\n",
      "    >>> s2.str.contains('.0', regex=True)\n",
      "    0     True\n",
      "    1     True\n",
      "    2    False\n",
      "    3     True\n",
      "    4    False\n",
      "    dtype: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dir(phones['Phone number'].str))\n",
    "\n",
    "help(phones['Phone number'].str.contains)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b265bb-c750-480f-962a-32eb6d7ef031",
   "metadata": {},
   "source": [
    "## Removing titles and taking names\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "While collecting survey respondent metadata in the airlines DataFrame, the full name of respondents was saved in the \"full_name\" column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as \"Dr.\", \"Mr.\", \"Ms.\" and \"Miss\".\n",
    "\n",
    "Your ultimate objective is to create two new columns named first_name and last_name, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "The airlines DataFrame is in your environment, alongside pandas as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Remove \"Dr.\", \"Mr.\", \"Miss\" and \"Ms.\" from full_name by replacing them with an empty string \"\" in that order.\n",
    "    Run the assert statement using .str.contains() that tests whether full_name still contains any of the honorifics.\n",
    "\n",
    "Hint\n",
    "\n",
    "    The .str.replace() method takes in the pattern to find, and the pattern to replace it by.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a518b0ac-37a2-4e74-a90c-e5c81b1303e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id        day      airline        destination    dest_region dest_size  \\\n",
      "0  1351    Tuesday  UNITED INTL             KANSAI           Asia       Hub   \n",
      "1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico     Small   \n",
      "2  2820   Thursday        DELTA        LOS ANGELES        West US       Hub   \n",
      "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US       Hub   \n",
      "4  2992  Wednesday     AMERICAN              MIAMI        East US       Hub   \n",
      "\n",
      "  boarding_area   dept_time  wait_min     cleanliness         safety  \\\n",
      "0  Gates 91-102  2018-12-31     115.0           Clean        Neutral   \n",
      "1   Gates 50-59  2018-12-31     135.0           Clean      Very safe   \n",
      "2   Gates 40-48  2018-12-31      70.0         Average  Somewhat safe   \n",
      "3   Gates 20-39  2018-12-31     190.0           Clean      Very safe   \n",
      "4   Gates 50-59  2018-12-31     559.0  Somewhat clean      Very safe   \n",
      "\n",
      "         satisfaction  \n",
      "0      Very satisfied  \n",
      "1      Very satisfied  \n",
      "2             Neutral  \n",
      "3  Somewhat satsified  \n",
      "4  Somewhat satsified   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines = pd.read_csv('airlines_final.csv', index_col=0)\n",
    "print(airlines.head(), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3daf1b8-4b85-4545-a07d-8876d6f25dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")  #-----------------------------------------------\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = ____\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "____\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "____\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False  #++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c25e0-1b97-4d29-af26-7f8b9dc209cc",
   "metadata": {},
   "source": [
    "## Keeping it descriptive\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the \"survey_response\" column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than 40 , and make sure your new DataFrame contains responses with 40 characters or more using an assert statement.\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "The airlines DataFrame is in your environment, and pandas is imported as pd.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Using the \"airlines\" DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len().\n",
    "#    Isolate the rows of airlines with resp_length higher than 40.\n",
    "    Assert that the smallest survey_response length in airlines_survey is now bigger than 40.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a84a05-ee49-4a27-a737-bc3636168454",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.loc[airlines['survey_response'].str.len<40, 'survey_response'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094aabf3-dbb9-4891-bc84-215be59c11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = ____\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[____ > ____]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert ____.str.len().____ > _____   ################################################################################\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bf200-a450-421c-888e-2fe940c1da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85a61272-f84d-45b7-8df3-0d3fa27a2d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 5, 7, 9}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "def find_z(n):\n",
    "    \n",
    "    z = []\n",
    "    for item in n:\n",
    "        if item == 1:\n",
    "            z.append(item)\n",
    "        if item == 2:\n",
    "            z.append(item)\n",
    "            \n",
    "        for i in range(2, item):\n",
    "            for j in range (2, i):\n",
    "                if i % j == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    z.append(i)\n",
    "            \n",
    "    return set(z)\n",
    "\n",
    "find_z(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8b76d-567b-403e-943c-c6798a518e00",
   "metadata": {},
   "source": [
    "## Uniformity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Stellar work on chapter 2.  \n",
    "# You're now an expert at handling categorical and text variables.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this chapter, we are looking at more advanced data clearning problems, such as uniformity, cross fields validation and dealing with missing data.  \n",
    "\n",
    "In chapter 1, we saw how out of range values are a common problem when clearning data, and that when left untouched, can skew your analysis.  In this lesson, we're going to tackle a problem that could similarly skew our data, which is unit uniformity.  For example, we can have temperature data that have values in both Fahrenheit and Celsius, weight data in Kilograms and in stones, dates in multiple formats, and so on.  Verifying uniformity is imperative to having accurate analysis.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "Here is a dataset with average temperature data throughout the month of March in New York City.  The dataset was collected from different sources with temperature data in Celsius and Fahrenheit merged together.  We can see that unless a major climate event occurred, the value \"62.6\" is most likely Fahrenheit not Celsius.  \n",
    "\n",
    "Lets confirm the presence of these values visually.  We can do so by plotting a scatter plot of our data.  We can do this using Matplotlib.pyplot imported as plt.  Use the \"plt.scatter()\" function, which takes in what to plot on the x axis, the y axis, and which data source to use.  We set the title, axis labels with the helper function below, then show the plot.  Notice the outer data points?  They all must be Fahrenheit.  A simple Google search returns the formula for converting Fahrenheit to Celsius.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To convert our temperature data, we isolate all rows of temperature column where it is above 40 using the \".loc[]\" method.  We choose 40 because its a common sense maximum for Celsius temperature in New York City.  We then convert these values to Celsius using the formula and resign them to their respective Fahrenheit values in temperatures.  We can make sure that our conversion was correct with assert statement, by making sure the maximum value of temperatrure is less tha 40.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "\n",
    "Here is another common uniformity problem with date data.  This is a DataFrame called birthdays containing birth dates for a variety of individuals.  It has been collected from a variety of sources and merged into one.  Notice the format of each oeservation in Birthday column, and even contains error data.  We'll learn how to deal with those.  \n",
    "\n",
    "#  \"pd.to_datetime()\" function accepts different formats, but\n",
    "We already discussed datetime objects.  Without getting too much into details, datetime accepts different formats that help you format your dates as pleased.  The Pandas \"pd.to_datetime()\" function automatically accepts most date formats, but could raise errors when certain formats are unrecognizable.  You don't have to memorize these formats, just know that they exist amnd are easily searchable.  \n",
    "\n",
    "# this isn't enough and will most likely return an error in real-world multiple formats circumstance\n",
    "You can treat these date inconsistencies easily by converting your date column to datetime.  We can do this in Pandas use the \"pd.to_datetime()\" function mentioned above.  However this isn't enough and will most likely return an error, since we have dates in multiple formats, especially the weird day/day/year format - the error one, which triggers an error with months.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# Instead we set the \"infer_datetime_format=\" arg equal to True and set the \"errors=\" arg equal top \"coerce\".  \n",
    "This will infer the format and return missing value for dates that couldn't be identified and converted instead of a value error.  This returns the birthday column with alligned formats, with the initial ambiguous format of day-day-year, being set to NaT, which represents missing values in Pandas for datetime objects.  \n",
    "\n",
    "# We can also convert the format of a datetime column using the \".dt.strftime()\" method, \n",
    "Which accetps a datetime format of your choice.  For example, here we converthe Birthday colum to day-month-year, instead of year-month-day.  However a common problem is having ambiguous dates with vague formats.  For example is this \"2019-03-08\" date value set in March or August?  Unfortunately there is no clear cut way to soprt this inconsistency or to treat it.  Depending on the size of the dataset and suspected ambiguities, we can either convert these dates to NAs ad deal with them accordingly.  Or if you have additional context on the source of your data, you can probably infer the format.  If the majority of subsequent or previous data is of one format, you can probably infer the format as well.  All in all, it is essential to properly understand where your data coes from, before trying to treat it, as it will make making these decisions much easier.  \n",
    "\n",
    "New lets make our data uniform.  \n",
    "\n",
    "\n",
    "\n",
    "temperatures = pd.read_csv(temperature.csv)\n",
    "temperature.head()\n",
    "\n",
    "-------------------------------------------\n",
    "   Date      | Temperature\n",
    "   03.03.19  | 14.0\n",
    "   04.03.19  | 15.0\n",
    "   05.03.19  | 18.0\n",
    "   06.03.19  | 16.0\n",
    "   07.03.19  | 62.6\n",
    "   \n",
    "   \n",
    "plt.scatter(data=temperatures, x='Date', y='Temperature')\n",
    "plt.title()\n",
    "plt.xlable()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "temp_fah = temperature.loc[temperature['Temperature']>40, 'Temperaature']   #++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "temperature.loc[temperature['Temperature']>40, 'Temperaature'] = (temp_fah - 32) * (5/9)  #++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------\n",
    "   Birthday         | First name   | Last name\n",
    "   27/27/19         | Rowan        | Nunex\n",
    "   03-29-19         | Brynn        | Yang\n",
    "   March 3rd, 2019  | Sophia       | Reilly\n",
    "   24-03-19         | Deacon       | Prince\n",
    "   06-03-19         | Griffith     | Neal\n",
    "   \n",
    "\n",
    "-----------------------------------------\n",
    "Date                | datetime format\n",
    "25-12-2019          | %d-%m-%Y\n",
    "December 25th 2019  | %c\n",
    "12-25-2019          | %m-%d-%Y\n",
    "...                 | ...\n",
    "\n",
    "pd.to_datetime() can recognize most formats automatically\n",
    "\n",
    "# Converts to datetime - but won't work cause the error data day-day-year\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'])\n",
    "  ValueError: month must be in 1..12\n",
    "  \n",
    "# Will work\n",
    "birthday['Birthday'] = pd.to_datetime(birthday['Birthday'], \n",
    "                                      # Attempt to infer format of each date\n",
    "                                      infer_datetime_format=True,   #++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "                                      # Return NA fpr rows where conversion failed\n",
    "                                      errors='coerce')   #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01799846-2006-4145-9beb-7d1bfd87e068",
   "metadata": {},
   "source": [
    "## Ambiguous dates\n",
    "\n",
    "You have a DataFrame containing a \"subscription_date\" column that was collected from various sources with different Date formats such as YYYY-mm-dd and YYYY-dd-mm. What is the best way to unify the formats for ambiguous values such as 2019-04-07?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    Set them to NA and drop them.\n",
    "    press   # If we wish take the easy way\n",
    "    1\n",
    "    Infer the format of the data in question by checking the format of subsequent and previous values.\n",
    "    press   # Could be typo too\n",
    "    2\n",
    "    Infer the format from the original data source.\n",
    "    press   # If its provided\n",
    "    3\n",
    "#    All of the above are possible, as long as we investigate where our data comes from, and understand the dynamics affecting it before cleaning it.\n",
    "    press\n",
    "    4\n",
    "    \n",
    "Hint\n",
    "\n",
    "    Ambiguous date formats represent a data cleaning challenge that requires a solid understanding of where your data comes from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7139047-fad7-450c-849a-b01b1c924723",
   "metadata": {},
   "source": [
    "## Uniform currencies\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the banking DataFrame. The dataset contains data on the amount of money stored in accounts (acct_amount), their currency (acct_cur), amount invested (inv_amount), account opening date (account_opened), and last transaction date (last_transaction) that were consolidated from American and European branches.\n",
    "\n",
    "# You are tasked with understanding the average account size and how investments vary by the size of account, \n",
    "however in order to produce this analysis accurately, you first need to unify the currency amount into dollars. The pandas package has been imported as pd, and the banking DataFrame is in your environment.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Find the rows of \"acct_cur\" in banking that are equal to 'euro' and store them in the variable \"acct_eu\".\n",
    "#    Find all the rows of \"acct_amount\" in banking that fit the \"acct_eu\" condition, and convert them to USD by multiplying them with 1.1.\n",
    "    Find all the rows of \"acct_cur\" in banking that fit the \"acct_eu\" condition, set them to 'dollar'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135d297b-8ea3-4c11-b672-fbcb3f4aebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cust_id  birth_date  Age   acct_amount  inv_amount   fund_A   fund_B  \\\n",
      "52  A631984D  1978-02-27   42  7.779933e+04       46410   2188.0  35508.0   \n",
      "63  0B44C3F8  1975-02-20   45  3.398487e+04       31393  10373.0   5583.0   \n",
      "6   6B094617  1977-08-26   43  8.985598e+04       34549   1796.0    312.0   \n",
      "61  45F31C81  1975-01-12   49  1.206753e+08       94608  15416.0  18845.0   \n",
      "0   870A9281  1962-06-09   58  6.352331e+04       51295  30105.0   4138.0   \n",
      "40  777A7F2C  1973-08-12   47  5.268417e+04       20968    380.0   2402.0   \n",
      "15  3C5CBBD7  1971-05-20   49  5.967801e+04       35937   4133.0   8540.0   \n",
      "94  A731C34E  1961-06-03   59  9.535202e+04       84065  12061.0  15742.0   \n",
      "18  C9FB0E86  1965-10-04   55  8.868234e+04       26164   5504.0   4063.0   \n",
      "\n",
      "     fund_C   fund_D account_opened last_transaction  \n",
      "52    331.0   8383.0       26-01-18         06-10-19  \n",
      "63   1669.0  13768.0       10-04-18         28-09-19  \n",
      "6   20610.0  11831.0       06-02-18         14-02-19  \n",
      "61  20325.0  40022.0       05-11-18         25-12-19  \n",
      "0    1420.0  15632.0       02-09-18         22-02-19  \n",
      "40  14612.0   3574.0       27-10-18         25-05-19  \n",
      "15   9595.0  13669.0       03-01-19         02-10-18  \n",
      "94  35725.0  20537.0       13-11-17         13-01-19  \n",
      "18   5602.0  10995.0       19-05-18         06-08-19  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "banking = pd.read_csv('banking_dirty.csv', index_col=0)\n",
    "print(banking.sample(9))\n",
    "\n",
    "\n",
    "\n",
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1   #++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'  #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de27f63-c182-4936-bdee-898542b25921",
   "metadata": {},
   "source": [
    "## Uniform dates\n",
    "\n",
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The account_opened column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a datetime object, while making sure that the format is inferred and potentially incorrect formats are set to missing. The banking DataFrame is in your environment and pandas was imported as pd.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the header of \"account_opened\" from the \"banking\" DataFrame and take a look at the different results.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Question:\n",
    "--------\n",
    "Take a look at the output. You tried converting the values to datetime  using  the  default \n",
    "\"to_datetime()\" function without changing any argument, however received the following error:\n",
    "ValueError: month must be in 1..12\n",
    "Why do you think that is?\n",
    "Possible Answers\n",
    "- The to_datetime() function needs to be explicitly told which date format each row is in.[X]\n",
    "- The to_datetime() function can only be applied on YY-mm-dd date formats.[X]\n",
    "# - The 21-14-17 entry is erroneous and leads to an error.[Correct]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Convert the account_opened column to datetime, while making sure the date format is inferred\n",
    "  and that erroneous formats that raise error return a missing value.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Extract the year from the amended \"account_opened\" column and assign it to the \"acct_year\" column.\n",
    "- Print the newly created \"acct_year\" column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf0a26-1290-4e3d-ac7b-205450c6d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banking['account_opened'].unique())\n",
    "\n",
    "print(banking)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'], \n",
    "                                           infer_datetime_format=True, \n",
    "                                           errors='coerce')\n",
    "\n",
    "\n",
    "banking['acct_year'] = banking['account_opened'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77add242-2b79-46ff-aa5e-27e9a989f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
      "12  EEBD980F  1990-11-20   34     57838.49       50812  18314.0   1477.0   \n",
      "41  2EC1B555  1974-03-17   46     55976.78       51477   8303.0  24112.0   \n",
      "78  F7FC8F78  1974-05-07   46     88049.82       84430   4590.0  24786.0   \n",
      "74  904A19DD  1987-02-18   33     31981.36       13188   9599.0    858.0   \n",
      "33  B5D367B5  1981-02-12   39     44226.86       36571   1280.0   8191.0   \n",
      "64  5321D380  1982-04-30   38     59700.08        8143    117.0   1198.0   \n",
      "60  5AEA5AB8  1972-10-24   48    100266.99       89341     41.0  13870.0   \n",
      "2   BFC13E88  1990-09-12   34     59863.77       24567  10323.0   4590.0   \n",
      "35  078C654F  1993-10-17   27     87312.64       66529   3684.0  17635.0   \n",
      "\n",
      "      fund_C   fund_D account_opened last_transaction  \n",
      "12  29049.48   5539.0       08-12-18         04-01-20  \n",
      "41  15776.00   3286.0       05-12-17         21-10-19  \n",
      "78   3346.00  51708.0       28-02-18         30-04-18  \n",
      "74   1083.00   1648.0       28-01-19         23-06-19  \n",
      "33   3462.00  23638.0       16-09-17         03-04-19  \n",
      "64    409.00   6419.0       09-10-18         04-02-19  \n",
      "60  60112.00  15318.0       09-06-18         03-07-19  \n",
      "2    8469.00   1185.0       25-04-18         02-04-18  \n",
      "35  11717.00  33493.0       14-04-17         05-08-18  \n",
      "    cust_id  birth_date  Age  acct_amount  inv_amount   fund_A   fund_B  \\\n",
      "0  870A9281  1962-06-09   58     63523.31       51295  30105.0   4138.0   \n",
      "1  166B05B0  1962-12-16   58     38175.46       15050   4995.0    938.0   \n",
      "2  BFC13E88  1990-09-12   34     59863.77       24567  10323.0   4590.0   \n",
      "3  F2158F66  1985-11-03   35     84132.10       23712   3908.0    492.0   \n",
      "4  7A73F334  1990-05-17   30    120512.00       93230  12158.4  51281.0   \n",
      "\n",
      "    fund_C   fund_D account_opened last_transaction  year  \n",
      "0   1420.0  15632.0     2018-02-09         22-02-19  2018  \n",
      "1   6696.0   2421.0     2019-02-28         31-10-18  2019  \n",
      "2   8469.0   1185.0     2018-04-25         02-04-18  2018  \n",
      "3   6482.0  12830.0     2017-07-11         08-11-18  2017  \n",
      "4  13434.0  18383.0     2018-05-14         19-07-18  2018  \n",
      "datetime64[ns]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "banking = pd.read_csv('banking_dirty.csv', index_col=0)\n",
    "print(banking.sample(9))\n",
    "\n",
    "\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'], \n",
    "                                           infer_datetime_format=True, \n",
    "                                           errors='coerce')\n",
    "\n",
    "banking['year'] = banking['account_opened'].dt.year  #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "print(banking.head())\n",
    "\n",
    "print(banking['account_opened'].dtypes)\n",
    "print(banking['year'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca095c-134e-4bce-99af-809fd645e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthday['Birthday'] = pd.to_datetime(birthday['Birthday'], \n",
    "                                      # Attempt to infer format of each date\n",
    "                                      infer_datetime_format=True,   #++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "                                      # Return NA fpr rows where conversion failed\n",
    "                                      errors='coerce')   #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898df08-2d3b-410f-a746-2cfbd1075eb0",
   "metadata": {},
   "source": [
    "## Cross field validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Here is the second lesson of this chapter.  In this lesson, we'll talk about cross field validation for diagnosing dirty data.  Let take a look at the following dataset.  \n",
    "\n",
    "It contains flight statistics on the total number of passengers in economy, business and first class as well as the total passengers for each flight.  \n",
    "# *******************************************************************************************************************\n",
    "We know that these columns have been collected and merged from different data sources, and a common challenge when merging data from different sources is data integrity, or more broadly making sure that our data is correct.  This is where \"cross field validation\" comes in.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "\"Cross Field Validation\" is the use of multiple fields in your dataset to sanity check the integrity of your data.  For example in our flights dataset, this could be summing economy, business and first class values and making sure they are equal to the total passengers on the plane.  This could be easily done in Pandas, by first subsetting on the columns to sum, then use the \".sum()\" method with the axis argument set to 1 to indicate row wise summing.  We then find the instances where the total passengers column is equal to the sum of the classes.  And find and filter out instances of inconsistent passenger amounts by subsetting on the equality we created with brackets and the tilde symbol.  \n",
    "# *******************************************************************************************************************\n",
    "\n",
    "Here is another example containing user IDs, birthdays and age values for a set of users.  We can for example make sure that the age and birthday columns are correct by subtracting the number of years between today's date and each birthday.  We can do this by first making sure the Birthday column is converted to \"datetime64[ns]\" with the Pandas \"pd.to_datetime()\" function.  We then create an object to storing today's date using the datetime package's \"date.today()\" function.  We then calculate the difference in years between today's date's year and the year of each birthday by using the \".dt.year\" attribute of the users Birthdat column (for datetime object we have .year attribute).  We then find instances where the calculated ages are equal to the actual age column in the users DF.  We then find and filter out the instances where we have inconsistences using subsetting with brackets and tilde symbol on the equality we created.  \n",
    "\n",
    "\n",
    "# So what should be the course of action in case we spot inconsistencies with \"Cross Field Validation\"?  \n",
    "Just like other data cleaning problems, there is no one size fits all solution, as often the best solution requires an in depth understanding of our datasets.  \n",
    "\n",
    "We can decide either drop inconsistent data, set is to missing and impute it, or apply some rules due to domain knowledge.  All these routes and assumptions can be decided upon only when you have a good understanding of where your dataset comes from different sources feeding into it.  \n",
    "\n",
    "\n",
    "Now that you know \"Cross Field Validation\", lets get to practice.  \n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "   flight_number | economy_class | business_class | first_class | total_passengers\n",
    "           DL140 |           100 |             60 |          40 |              200\n",
    "           BA248 |           130 |            100 |          70 |              300\n",
    "          MEA124 |           100 |             50 |          50 |              200\n",
    "          AFR939 |           140 |             70 |          90 |              300\n",
    "           DL140 |           130 |            100 |          20 |              250\n",
    "\n",
    "sum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis=1)\n",
    "passenger_equ = sum_classes == flights['total_passengers']\n",
    "\n",
    "# Find and filter out rows with inconsistent passenger totals\n",
    "inconsistent_pass = flights[~passenger_equ]\n",
    "consistsnce_pass = flights[passenger_equ]\n",
    "\n",
    "\n",
    "import datetime\n",
    "# Convert to datetime and get today's date\n",
    "users['Birthday'] = pd.to_datetime(users['Birthday'])  # If we assume all observation in one format, no concatinate\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "# For each row in the Birthday column, calculate year difference\n",
    "age_manual = today.year - user['Birthday'].dt.year\n",
    "\n",
    "# Find instances where ages match\n",
    "age_equ = age_manual == users['age']\n",
    "\n",
    "# Find and filter out rows with inconsistent age\n",
    "inconsistent_age = users[~age_equ]\n",
    "consistent_age = age[age_equ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777b8f0-d110-41bd-a000-d1d30f2e89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross field or no cross field?\n",
    "\n",
    "Throughout this course, you've been immersed in a variety of data cleaning problems from range constraints, data type constraints, uniformity and more.\n",
    "\n",
    "In this lesson, you were introduced to cross field validation as a means to sanity check your data and making sure you have strong data integrity.\n",
    "\n",
    "Now, you will map different applicable concepts and techniques to their respective categories.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Map different applicable concepts and techniques to their respective categories.\n",
    "\n",
    "\n",
    "Cross Field Validation:\n",
    "    Comfirming the Age provided by users by cross checking their birthdays\n",
    "    Row wise operations such as \".sum(axis=1)\"\n",
    "    \n",
    "Not Cross Field Validation:\n",
    "    Making sure that a \"revenue\" column is a numeric column\n",
    "    Making sure a \"subscription_date\" column has no values set in the future\n",
    "    The use the the \".astype()\" method.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f5846-61ca-49fb-bb6a-2c6e0b10bce0",
   "metadata": {},
   "source": [
    "## How's our data integrity?\n",
    "\n",
    "New data has been merged into the banking DataFrame that contains details on how investments in the inv_amount column are allocated across four different funds A, B, C and D.\n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the age and birth_date columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of inv_amount and age against the amount invested in different funds and customers' birthdays. Both pandas and datetime have been imported as pd and dt respectively.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "        Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.\n",
    "        Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.\n",
    "\n",
    "    Question 2\n",
    "\n",
    "        Store today's date into today, and manually calculate customers' ages and store them in ages_manual.\n",
    "        Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628da8c-6fac-47f3-9316-290b59e27721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9b3ad-1738-4f84-b990-87d2629c6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[____].____(____) == ____\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = ____[____]\n",
    "inconsistent_inv = ____[____]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a03198-ec16-49c0-b67e-9be584dac693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f182d-c4b1-4d68-b49c-0bae6f75baa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b968c2-3eac-40c4-85f3-cb85469c0acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca2fd7-2c2c-4d10-8985-a4926c586488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1a7ea-fb05-4f0a-b4e2-3bae2be84b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e3110-ff97-473a-bc08-341156ed22c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119e39-5965-407b-8278-b4d4ccb017de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f894985-7dab-4dcd-b945-c6085eea9630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c8bd6-1e7c-4e97-851d-f20f5a6cd4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c1f7f-14a8-400f-859e-0a1d7c00062f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212483f-04a3-4a41-b09a-2ebf8e83bf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73bd84-821f-42cf-a953-1c8d5663676b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0c4de-ef37-4e8c-ac52-331ecf20d20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abc35c-15f2-4b53-8d30-95a365f3d4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163c4f9-0c73-4f02-b2a3-72f4c0d5a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b925f9a-51e1-4339-90c5-15f7276f75a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d50508-2483-41fd-95aa-7c51dcddb852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf82d1-7be0-4ec1-bd7a-e4c022d767c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6853a29-2b7c-4033-980f-440b9116db18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d33e7-9a26-45a5-928a-4f9137977e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051bff-4379-4701-98d0-0f3dbf613040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a6391-c35c-430f-9203-6caec9024446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b43593-9a0b-4fcc-8b05-8c1eb1892b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b26f54-248b-453a-aa99-c5804e119c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db9e3f-5fed-41fb-bd6c-63a64fb644c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d7963-0b1b-4576-97bf-caebae057ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72ec58-1544-434e-8c7c-e88e40615894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c5de3-dcc2-4c0b-9ec0-21946e3dc558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3e77f-98af-459f-b2ed-ab5c3bddf34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db7f19-7f56-4cbd-97be-8597992bd68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777d141-90c8-459c-ba5b-f6539ff59088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaeabee-e0c6-41a0-b377-b74fe8b1e7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff3cd0-37ac-426d-8a45-562daa94bdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013140f-8985-4d92-902a-c2383849979c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999515a-bfd3-4fd6-a31a-2d1a6982f51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860fbe6-7026-4b49-9518-d1fbd6ced516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e4ea8-0183-43cc-86f6-1c9fcc14e4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b487ff0-887d-4c9d-8a4d-c4af744db139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db13c8-52d9-43d5-8963-cd6539510604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d5121-c7ea-4f24-87f8-172bbcc108c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020aad0-e3fa-41e0-a94a-4dcffda750f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a121467-deb3-49d9-bd6e-542174ec69e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a058cae-530f-45e7-8587-bf7b9b2f3a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcc880-cab3-4d1b-b81d-5000d8c6d23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c0733-5df6-40a5-93af-1d2778e3d9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc24ab-5204-4993-b63e-3d4b57590586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055cd25-e46b-47a4-95e7-2b53140307d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7f94a-0af1-436f-8742-350c3b3046e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cada37-4ed1-4754-ac80-658422f980e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326b982-02fb-44af-b33f-1d929149d879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b1a1e-8236-4225-a5ad-e061d1a5c478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e6382-a26a-4913-9fde-3a3bf275773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac01ea-72c2-4816-b086-18db3f58a9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30226e6f-4314-4dd5-8cb1-5b629ee1bcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88e088-53c6-4ac8-a6b0-d0e683629b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51eae4-451e-407a-9189-bc0ca9e6a412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c926df5-24ba-46f6-9c88-25df7a76b16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052eaea-8c07-4458-a688-9d69ac88978d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
